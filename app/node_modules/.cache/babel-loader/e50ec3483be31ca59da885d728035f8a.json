{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Executor: Evaluates SymbolicTensor based on feeds.\n */\nimport { cast, dispose, memory, util } from '@tensorflow/tfjs-core';\nimport { ValueError } from '../errors';\nimport { toList } from '../utils/generic_utils';\nimport { InputLayer } from './input_layer';\nimport { SymbolicTensor } from './topology';\n/**\n * Helper function to check the dtype and shape compatibility of a feed value.\n */\nfunction assertFeedCompatibility(key, val) {\n  // Check dtype compatibility.\n  if (key.dtype == null || key.dtype === val.dtype) {\n    //  a.  If types match, return val tensor as is.\n    return val;\n  }\n  try {\n    //  b. Attempt to convert to expected type.\n    return cast(val, key.dtype);\n  } catch (err) {\n    //  c. If conversion fails, return helpful error.\n    throw new ValueError(`The dtype of the feed (${val.dtype}) can not be cast to the dtype ` + `of the key '${key.name}' (${key.dtype}).`);\n  }\n}\n/**\n * FeedDict: A mapping from unique SymbolicTensors to feed values for them.\n * A feed value is a concrete value represented as an `Tensor`.\n */\nexport class FeedDict {\n  /**\n   * Constructor, optionally does copy-construction.\n   * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case\n   *   copy-construction will be performed.\n   */\n  constructor(feeds) {\n    this.id2Value = {};\n    this.id2Mask = {};\n    this.name2Id = {};\n    if (feeds instanceof FeedDict) {\n      for (const id in feeds.id2Value) {\n        this.id2Value[id] = feeds.id2Value[id];\n        if (id in feeds.id2Mask) {\n          this.id2Mask[id] = feeds.id2Mask[id];\n        }\n      }\n    } else {\n      if (feeds == null) {\n        return;\n      }\n      for (const feed of feeds) {\n        this.add(feed.key, feed.value);\n      }\n    }\n  }\n  /**\n   * Add a key-value pair to the FeedDict.\n   *\n   * @param key The key of the feed.\n   * @param value The value of the tensor feed.\n   * @param mask The value of the mask feed (optional).\n   * @returns This `FeedDict`.\n   * @throws ValueError: If the key `SymbolicTensor` already exists in the\n   *   `FeedDict`.\n   */\n  add(key, value, mask) {\n    if (this.id2Value[key.id] == null) {\n      this.id2Value[key.id] = assertFeedCompatibility(key, value);\n      this.name2Id[key.name] = key.id;\n      if (mask != null) {\n        this.id2Mask[key.id] = mask;\n      }\n    } else {\n      throw new ValueError(`Duplicate key: name=${key.name}, id=${key.id}`);\n    }\n    return this;\n  }\n  /**\n   * Add a Feed to the FeedDict.\n   * @param feed The new `Feed` to add.\n   * @returns This `FeedDict`.\n   */\n  addFeed(feed) {\n    this.add(feed.key, feed.value);\n  }\n  /**\n   * Probe whether a key already exists in the FeedDict.\n   * @param key\n   */\n  hasKey(key) {\n    return this.id2Value[key.id] != null;\n  }\n  /**\n   * Get all the SymbolicTensor available in this FeedDict.\n   */\n  names() {\n    return Object.keys(this.name2Id);\n  }\n  /**\n   * Get the feed value for given key.\n   * @param key The SymbolicTensor, or its name (as a string), of which the\n   *     value is sought.\n   * @returns If `key` exists, the corresponding feed value.\n   * @throws ValueError: If `key` does not exist in this `FeedDict`.\n   */\n  getValue(key) {\n    if (key instanceof SymbolicTensor) {\n      if (this.id2Value[key.id] == null) {\n        throw new ValueError(`Nonexistent key: ${key.name}`);\n      } else {\n        return this.id2Value[key.id];\n      }\n    } else {\n      const id = this.name2Id[key];\n      if (id == null) {\n        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n      }\n      return this.id2Value[id];\n    }\n  }\n  /**\n   * Get the feed mask for given key.\n   * @param key The SymbolicTensor, or its name (as a string), of which the\n   *     value is sought.\n   * @returns If `key` exists, the corresponding feed mask.\n   * @throws ValueError: If `key` does not exist in this `FeedDict`.\n   */\n  getMask(key) {\n    if (key instanceof SymbolicTensor) {\n      if (this.id2Value[key.id] == null) {\n        throw new ValueError(`Nonexistent key: ${key.name}`);\n      } else {\n        return this.id2Mask[key.id];\n      }\n    } else {\n      const id = this.name2Id[key];\n      if (id == null) {\n        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n      }\n      return this.id2Mask[id];\n    }\n  }\n  /** Dispose all mask Tensors held by this object. */\n  disposeMasks() {\n    if (this.id2Mask != null) {\n      dispose(this.id2Mask);\n    }\n  }\n}\n// Cache for topologically sorted SymbolicTensors for given execution\n// targets (i.e., fetches).\nconst cachedSorted = {};\n// Cache for recipient count maps for given execution targets (i.e., fetches).\nconst cachedRecipientCounts = {};\n/**\n * Execute a SymbolicTensor by using concrete feed values.\n *\n * A `SymbolicTensor` object is a node in a computation graph of TF.js\n * Layers. The object is backed by a source layer and input\n * `SymbolicTensor`s to the source layer. This method evaluates\n * the `call()` method of the source layer, using concrete values of the\n * inputs obtained from either\n * * `feedDict`, if the input key exists in `feedDict`, or else,\n * * a recursive call to `execute()` itself.\n *\n * @param x: The `SymbolicTensor` to execute.\n * @param feedDict: The feed values, as base condition of the recursion.\n *   execution.\n * @param kwargs: Optional keyword arguments.\n * @param probe: A probe object (of interface `ExecutionProbe`) used for\n *   testing memory footprint of `execute` calls.\n * @returns Result of the execution.\n * @throws ValueError: If any `SymbolicTensor`s from `InputLayer`s\n *   encountered during the execution lacks a feed value in `feedDict`.\n */\nexport function execute(fetches, feedDict, kwargs, probe) {\n  const training = kwargs == null ? false : kwargs['training'];\n  const arrayFetches = Array.isArray(fetches);\n  const fetchArray = arrayFetches ? fetches : [fetches];\n  const outputNames = fetchArray.map(t => t.name);\n  const finalOutputs = [];\n  const feedNames = feedDict.names();\n  for (const outputName of outputNames) {\n    if (feedNames.indexOf(outputName) !== -1) {\n      finalOutputs.push(feedDict.getValue(outputName));\n    } else {\n      finalOutputs.push(null);\n    }\n  }\n  if (probe != null) {\n    // For optional probing of memory footprint during execution.\n    probe.maxNumTensors = -Infinity;\n    probe.minNumTensors = Infinity;\n  }\n  // Check cache.\n  const fetchAndFeedKey = outputNames.join(',') + '|' + feedDict.names().join(',');\n  let sorted;\n  let recipientCounts;\n  if (cachedSorted[fetchAndFeedKey] == null) {\n    // Cache doesn't contain the desired combination of fetches. Compute\n    // topological sort for the combination for the first time.\n    const out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);\n    sorted = out.sorted;\n    recipientCounts = out.recipientCounts;\n    // Store results in cache for future use.\n    cachedSorted[fetchAndFeedKey] = sorted;\n    cachedRecipientCounts[fetchAndFeedKey] = recipientCounts;\n  }\n  sorted = cachedSorted[fetchAndFeedKey];\n  recipientCounts = {};\n  if (!training) {\n    Object.assign(recipientCounts, cachedRecipientCounts[fetchAndFeedKey]);\n  }\n  const internalFeedDict = new FeedDict(feedDict);\n  // Start iterative execution on the topologically-sorted SymbolicTensors.\n  for (let i = 0; i < sorted.length; ++i) {\n    if (probe != null) {\n      // For optional probing of memory usage during execution.\n      const numTensors = memory().numTensors;\n      if (numTensors > probe.maxNumTensors) {\n        probe.maxNumTensors = numTensors;\n      }\n      if (numTensors < probe.minNumTensors) {\n        probe.minNumTensors = numTensors;\n      }\n    }\n    const symbolic = sorted[i];\n    const srcLayer = symbolic.sourceLayer;\n    if (srcLayer instanceof InputLayer) {\n      continue;\n    }\n    const inputValues = [];\n    const inputMasks = [];\n    const tensorsToDispose = [];\n    let maskExists = false;\n    for (const input of symbolic.inputs) {\n      const value = internalFeedDict.getValue(input);\n      const mask = internalFeedDict.getMask(input);\n      inputValues.push(value);\n      inputMasks.push(mask);\n      if (mask != null) {\n        maskExists = true;\n      }\n      if (!training) {\n        recipientCounts[input.name]--;\n        if (recipientCounts[input.name] === 0 && !feedDict.hasKey(input) && outputNames.indexOf(input.name) === -1 && !value.isDisposed && input.sourceLayer.stateful !== true) {\n          tensorsToDispose.push(value);\n        }\n      }\n    }\n    if (maskExists) {\n      kwargs = kwargs || {};\n      kwargs['mask'] = inputMasks[0];\n    }\n    const outputTensors = toList(srcLayer.apply(inputValues, kwargs));\n    let outputMask = null;\n    if (srcLayer.supportsMasking) {\n      outputMask = srcLayer.computeMask(inputValues, inputMasks);\n    }\n    const layerOutputs = getNodeOutputs(symbolic);\n    const outputSymbolicTensors = Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];\n    for (let i = 0; i < outputSymbolicTensors.length; ++i) {\n      if (!internalFeedDict.hasKey(outputSymbolicTensors[i])) {\n        internalFeedDict.add(outputSymbolicTensors[i], outputTensors[i], Array.isArray(outputMask) ? outputMask[0] : outputMask);\n      }\n      const index = outputNames.indexOf(outputSymbolicTensors[i].name);\n      if (index !== -1) {\n        finalOutputs[index] = outputTensors[i];\n      }\n    }\n    if (!training) {\n      // Clean up Tensors that are no longer needed.\n      dispose(tensorsToDispose);\n    }\n  }\n  // NOTE(cais): Unlike intermediate tensors, we don't discard mask\n  // tensors as we go, because these tensors are sometimes passed over a\n  // series of mutliple layers, i.e., not obeying the immediate input\n  // relations in the graph. If this becomes a memory-usage concern,\n  // we can improve this in the future.\n  internalFeedDict.disposeMasks();\n  return arrayFetches ? finalOutputs : finalOutputs[0];\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for an array of fetches.\n *\n * This function calls getTopologicalSortAndRecipientCountsForOneFetch and\n * merges their results.\n *\n * @param fetch The array of fetches requested. Must be a non-empty array.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientCounts: Recipient counts for all SymbolicTensors in `sorted`.\n */\nfunction getTopologicalSortAndRecipientCounts(fetches, feedDict) {\n  util.assert(fetches != null && fetches.length > 0, () => `Expected at least one fetch, got none`);\n  let finalSorted = [];\n  let finalRecipientMap = {};\n  if (fetches.length === 1) {\n    // Special-casing 1 fetch for efficiency.\n    const out = getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);\n    finalSorted = out.sorted;\n    finalRecipientMap = out.recipientMap;\n  } else {\n    const visited = new Set();\n    for (const fetch of fetches) {\n      const {\n        sorted,\n        recipientMap\n      } = getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict);\n      // Merge sorted SymbolicTensor Arrays.\n      for (const symbolicTensor of sorted) {\n        if (!visited.has(symbolicTensor.name)) {\n          finalSorted.push(symbolicTensor);\n          visited.add(symbolicTensor.name);\n        }\n      }\n      // Merge recipient maps.\n      for (const name in recipientMap) {\n        if (finalRecipientMap[name] == null) {\n          finalRecipientMap[name] = new Set();\n        }\n        recipientMap[name].forEach(recipient => finalRecipientMap[name].add(recipient));\n      }\n    }\n  }\n  return {\n    sorted: finalSorted,\n    recipientCounts: recipientMap2Counts(finalRecipientMap)\n  };\n}\nfunction recipientMap2Counts(recipientMap) {\n  const recipientCounts = {};\n  for (const name in recipientMap) {\n    recipientCounts[name] = recipientMap[name].size;\n  }\n  return recipientCounts;\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for a single fetch.\n *\n * This helper function processes the upstream SymbolicTensors of a single\n * fetch.\n *\n * @param fetch The single fetch requested.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientMap: Recipient names for all SymbolicTensors in `sorted`.\n */\nexport function getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict) {\n  const visited = new Set();\n  const sorted = [];\n  const recipientMap = {};\n  // Put keys of the feedDict into visited first, so they don't have to be\n  // walked. This is needed in case where there are feeds for intermediate\n  // SymbolicTensors of the graph.\n  for (const key of feedDict.names()) {\n    visited.add(key);\n  }\n  const stack = [];\n  const marks = [];\n  // Initial population of stack and marks.\n  stack.push(fetch);\n  while (stack.length > 0) {\n    const top = stack[stack.length - 1];\n    if (visited.has(top.name)) {\n      stack.pop();\n      continue;\n    }\n    const topIsMarked = marks[marks.length - 1] === stack.length - 1;\n    if (top.inputs.length === 0 || topIsMarked) {\n      // Input SymbolicTensor or all children have been visited.\n      stack.pop();\n      sorted.push(top);\n      visited.add(top.name);\n      if (topIsMarked) {\n        marks.pop();\n      }\n    } else {\n      // A non-input SymbolicTensor whose upstream SymbolicTensors haven't\n      // been visited yet. Push them onto the stack.\n      marks.push(stack.length - 1);\n      for (const input of top.inputs) {\n        // Increment the recipient count. Note that this needs to happen\n        // regardless of whether the SymbolicTensor has been visited before.\n        if (recipientMap[input.name] == null) {\n          recipientMap[input.name] = new Set();\n        }\n        recipientMap[input.name].add(top.name);\n        if (visited.has(input.name)) {\n          continue; // Avoid repeated visits to the same SymbolicTensor.\n        }\n\n        stack.push(input);\n      }\n    }\n  }\n  return {\n    sorted,\n    recipientMap\n  };\n}\n/**\n * Get the symbolic output tensors of the node to which a given fetch belongs.\n * @param fetch The fetched symbolic tensor.\n * @returns The Array of symbolic tensors output by the node to which `fetch`\n *   belongs.\n */\nfunction getNodeOutputs(fetch) {\n  let layerOutputs;\n  if (fetch.sourceLayer.inboundNodes.length === 1) {\n    layerOutputs = fetch.sourceLayer.output;\n  } else {\n    let nodeIndex = null;\n    for (let i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {\n      for (const outputTensor of fetch.sourceLayer.inboundNodes[i].outputTensors) {\n        if (outputTensor.id === fetch.id) {\n          nodeIndex = i;\n          break;\n        }\n      }\n    }\n    layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);\n  }\n  return layerOutputs;\n}","map":{"version":3,"sources":["../../src/engine/executor.ts"],"names":[],"mappings":"AAAA;;;;;;;;AAQG;AAEH;;AAEG;AAEH,SAAQ,IAAI,EAAE,OAAO,EAAE,MAAM,EAAU,IAAI,QAAO,uBAAuB;AAEzE,SAAQ,UAAU,QAAO,WAAW;AAEpC,SAAQ,MAAM,QAAO,wBAAwB;AAE7C,SAAQ,UAAU,QAAO,eAAe;AACxC,SAAQ,cAAc,QAAO,YAAY;AAEzC;;AAEG;AACH,SAAS,uBAAuB,CAAC,GAAmB,EAAE,GAAW,EAAA;EAC/D;EACA,IAAI,GAAG,CAAC,KAAK,IAAI,IAAI,IAAI,GAAG,CAAC,KAAK,KAAK,GAAG,CAAC,KAAK,EAAE;IAChD;IACA,OAAO,GAAG;EACX;EACD,IAAI;IACF;IACA,OAAO,IAAI,CAAC,GAAG,EAAE,GAAG,CAAC,KAAK,CAAC;GAC5B,CAAC,OAAO,GAAG,EAAE;IACZ;IACA,MAAM,IAAI,UAAU,CAChB,0BAA0B,GAAG,CAAC,KAAK,iCAAiC,GACpE,eAAe,GAAG,CAAC,IAAI,MAAM,GAAG,CAAC,KAAK,IAAI,CAAC;EAChD;AACH;AAUA;;;AAGG;AACH,OAAM,MAAO,QAAQ,CAAA;EAKnB;;;;AAIG;EACH,WAAA,CAAY,KAAuB,EAAA;IAT3B,IAAA,CAAA,QAAQ,GAA2B,CAAA,CAAE;IACrC,IAAA,CAAA,OAAO,GAA2B,CAAA,CAAE;IACpC,IAAA,CAAA,OAAO,GAA6B,CAAA,CAAE;IAQ5C,IAAI,KAAK,YAAY,QAAQ,EAAE;MAC7B,KAAK,MAAM,EAAE,IAAI,KAAK,CAAC,QAAQ,EAAE;QAC/B,IAAI,CAAC,QAAQ,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,QAAQ,CAAC,EAAE,CAAC;QACtC,IAAI,EAAE,IAAI,KAAK,CAAC,OAAO,EAAE;UACvB,IAAI,CAAC,OAAO,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,OAAO,CAAC,EAAE,CAAC;QACrC;MACF;KACF,MAAM;MACL,IAAI,KAAK,IAAI,IAAI,EAAE;QACjB;MACD;MACD,KAAK,MAAM,IAAI,IAAI,KAAK,EAAE;QACxB,IAAI,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,EAAE,IAAI,CAAC,KAAK,CAAC;MAC/B;IACF;EACH;EAEA;;;;;;;;;AASG;EACH,GAAG,CAAC,GAAmB,EAAE,KAAa,EAAE,IAAa,EAAA;IACnD,IAAI,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,EAAE,CAAC,IAAI,IAAI,EAAE;MACjC,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,uBAAuB,CAAC,GAAG,EAAE,KAAK,CAAC;MAC3D,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,GAAG,CAAC,EAAE;MAC/B,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI;MAC5B;KACF,MAAM;MACL,MAAM,IAAI,UAAU,CAAC,uBAAuB,GAAG,CAAC,IAAI,QAAQ,GAAG,CAAC,EAAE,EAAE,CAAC;IACtE;IACD,OAAO,IAAI;EACb;EAEA;;;;AAIG;EACH,OAAO,CAAC,IAAU,EAAA;IAChB,IAAI,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,EAAE,IAAI,CAAC,KAAK,CAAC;EAChC;EAEA;;;AAGG;EACH,MAAM,CAAC,GAAmB,EAAA;IACxB,OAAO,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,EAAE,CAAC,IAAI,IAAI;EACtC;EAEA;;AAEG;EACH,KAAK,GAAA;IACH,OAAO,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,OAAO,CAAC;EAClC;EAEA;;;;;;AAMG;EACH,QAAQ,CAAC,GAA0B,EAAA;IACjC,IAAI,GAAG,YAAY,cAAc,EAAE;MACjC,IAAI,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,EAAE,CAAC,IAAI,IAAI,EAAE;QACjC,MAAM,IAAI,UAAU,CAAC,oBAAoB,GAAG,CAAC,IAAI,EAAE,CAAC;OACrD,MAAM;QACL,OAAO,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,EAAE,CAAC;MAC7B;KACF,MAAM;MACL,MAAM,EAAE,GAAG,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC;MAC5B,IAAI,EAAE,IAAI,IAAI,EAAE;QACd,MAAM,IAAI,UAAU,CAAC,yCAAyC,GAAG,EAAE,CAAC;MACrE;MACD,OAAO,IAAI,CAAC,QAAQ,CAAC,EAAE,CAAC;IACzB;EACH;EAEA;;;;;;AAMG;EACH,OAAO,CAAC,GAA0B,EAAA;IAChC,IAAI,GAAG,YAAY,cAAc,EAAE;MACjC,IAAI,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,EAAE,CAAC,IAAI,IAAI,EAAE;QACjC,MAAM,IAAI,UAAU,CAAC,oBAAoB,GAAG,CAAC,IAAI,EAAE,CAAC;OACrD,MAAM;QACL,OAAO,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,EAAE,CAAC;MAC5B;KACF,MAAM;MACL,MAAM,EAAE,GAAG,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC;MAC5B,IAAI,EAAE,IAAI,IAAI,EAAE;QACd,MAAM,IAAI,UAAU,CAAC,yCAAyC,GAAG,EAAE,CAAC;MACrE;MACD,OAAO,IAAI,CAAC,OAAO,CAAC,EAAE,CAAC;IACxB;EACH;EAEA;EACA,YAAY,GAAA;IACV,IAAI,IAAI,CAAC,OAAO,IAAI,IAAI,EAAE;MACxB,OAAO,CAAC,IAAI,CAAC,OAAO,CAAC;IACtB;EACH;AACD;AAED;AACA;AACA,MAAM,YAAY,GAAmD,CAAA,CAAE;AAEvE;AACA,MAAM,qBAAqB,GACuC,CAAA,CAAE;AAsBpE;;;;;;;;;;;;;;;;;;;;AAoBG;AACH,OAAM,SAAU,OAAO,CACnB,OAAwC,EAAE,QAAkB,EAC5D,MAAe,EAAE,KAAsB,EAAA;EAEzC,MAAM,QAAQ,GAAY,MAAM,IAAI,IAAI,GAAG,KAAK,GAAG,MAAM,CAAC,UAAU,CAAC;EAErE,MAAM,YAAY,GAAG,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC;EAC3C,MAAM,UAAU,GACZ,YAAY,GAAG,OAA2B,GAAG,CAAC,OAAyB,CAAC;EAE5E,MAAM,WAAW,GAAG,UAAU,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC;EAC/C,MAAM,YAAY,GAAa,EAAE;EACjC,MAAM,SAAS,GAAG,QAAQ,CAAC,KAAK,EAAE;EAClC,KAAK,MAAM,UAAU,IAAI,WAAW,EAAE;IACpC,IAAI,SAAS,CAAC,OAAO,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE;MACxC,YAAY,CAAC,IAAI,CAAC,QAAQ,CAAC,QAAQ,CAAC,UAAU,CAAC,CAAC;KACjD,MAAM;MACL,YAAY,CAAC,IAAI,CAAC,IAAI,CAAC;IACxB;EACF;EAED,IAAI,KAAK,IAAI,IAAI,EAAE;IACjB;IACA,KAAK,CAAC,aAAa,GAAG,CAAC,QAAQ;IAC/B,KAAK,CAAC,aAAa,GAAG,QAAQ;EAC/B;EAED;EACA,MAAM,eAAe,GACjB,WAAW,CAAC,IAAI,CAAC,GAAG,CAAC,GAAG,GAAG,GAAG,QAAQ,CAAC,KAAK,EAAE,CAAC,IAAI,CAAC,GAAG,CAAC;EAC5D,IAAI,MAAwB;EAC5B,IAAI,eAA8C;EAClD,IAAI,YAAY,CAAC,eAAe,CAAC,IAAI,IAAI,EAAE;IACzC;IACA;IACA,MAAM,GAAG,GAAG,oCAAoC,CAAC,UAAU,EAAE,QAAQ,CAAC;IACtE,MAAM,GAAG,GAAG,CAAC,MAAM;IACnB,eAAe,GAAG,GAAG,CAAC,eAAe;IAErC;IACA,YAAY,CAAC,eAAe,CAAC,GAAG,MAAM;IACtC,qBAAqB,CAAC,eAAe,CAAC,GAAG,eAAe;EACzD;EACD,MAAM,GAAG,YAAY,CAAC,eAAe,CAAC;EACtC,eAAe,GAAG,CAAA,CAAE;EACpB,IAAI,CAAC,QAAQ,EAAE;IACb,MAAM,CAAC,MAAM,CAAC,eAAe,EAAE,qBAAqB,CAAC,eAAe,CAAC,CAAC;EACvE;EAED,MAAM,gBAAgB,GAAG,IAAI,QAAQ,CAAC,QAAQ,CAAC;EAE/C;EACA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;IACtC,IAAI,KAAK,IAAI,IAAI,EAAE;MACjB;MACA,MAAM,UAAU,GAAG,MAAM,EAAE,CAAC,UAAU;MACtC,IAAI,UAAU,GAAG,KAAK,CAAC,aAAa,EAAE;QACpC,KAAK,CAAC,aAAa,GAAG,UAAU;MACjC;MACD,IAAI,UAAU,GAAG,KAAK,CAAC,aAAa,EAAE;QACpC,KAAK,CAAC,aAAa,GAAG,UAAU;MACjC;IACF;IAED,MAAM,QAAQ,GAAG,MAAM,CAAC,CAAC,CAAC;IAC1B,MAAM,QAAQ,GAAG,QAAQ,CAAC,WAAW;IACrC,IAAI,QAAQ,YAAY,UAAU,EAAE;MAClC;IACD;IACD,MAAM,WAAW,GAAa,EAAE;IAChC,MAAM,UAAU,GAAa,EAAE;IAC/B,MAAM,gBAAgB,GAAa,EAAE;IAErC,IAAI,UAAU,GAAG,KAAK;IACtB,KAAK,MAAM,KAAK,IAAI,QAAQ,CAAC,MAAM,EAAE;MACnC,MAAM,KAAK,GAAG,gBAAgB,CAAC,QAAQ,CAAC,KAAK,CAAC;MAC9C,MAAM,IAAI,GAAG,gBAAgB,CAAC,OAAO,CAAC,KAAK,CAAC;MAC5C,WAAW,CAAC,IAAI,CAAC,KAAK,CAAC;MACvB,UAAU,CAAC,IAAI,CAAC,IAAI,CAAC;MACrB,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,UAAU,GAAG,IAAI;MAClB;MACD,IAAI,CAAC,QAAQ,EAAE;QACb,eAAe,CAAC,KAAK,CAAC,IAAI,CAAC,EAAE;QAC7B,IAAI,eAAe,CAAC,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,QAAQ,CAAC,MAAM,CAAC,KAAK,CAAC,IAC5D,WAAW,CAAC,OAAO,CAAC,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,UAAU,IAC3D,KAAK,CAAC,WAAW,CAAC,QAAQ,KAAK,IAAI,EAAE;UACvC,gBAAgB,CAAC,IAAI,CAAC,KAAK,CAAC;QAC7B;MACF;IACF;IAED,IAAI,UAAU,EAAE;MACd,MAAM,GAAG,MAAM,IAAI,CAAA,CAAE;MACrB,MAAM,CAAC,MAAM,CAAC,GAAG,UAAU,CAAC,CAAC,CAAC;IAC/B;IACD,MAAM,aAAa,GACf,MAAM,CAAC,QAAQ,CAAC,KAAK,CAAC,WAAW,EAAE,MAAM,CAAC,CAAa;IAC3D,IAAI,UAAU,GAAoB,IAAI;IACtC,IAAI,QAAQ,CAAC,eAAe,EAAE;MAC5B,UAAU,GAAG,QAAQ,CAAC,WAAW,CAAC,WAAW,EAAE,UAAU,CAAC;IAC3D;IACD,MAAM,YAAY,GAAG,cAAc,CAAC,QAAQ,CAAC;IAC7C,MAAM,qBAAqB,GACvB,KAAK,CAAC,OAAO,CAAC,YAAY,CAAC,GAAG,YAAY,GAAG,CAAC,YAAY,CAAC;IAC/D,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,qBAAqB,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;MACrD,IAAI,CAAC,gBAAgB,CAAC,MAAM,CAAC,qBAAqB,CAAC,CAAC,CAAC,CAAC,EAAE;QACtD,gBAAgB,CAAC,GAAG,CAChB,qBAAqB,CAAC,CAAC,CAAC,EAAE,aAAa,CAAC,CAAC,CAAC,EAC1C,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,GAAG,UAAU,CAAC,CAAC,CAAC,GAAG,UAAU,CAAC;MAC5D;MACD,MAAM,KAAK,GAAG,WAAW,CAAC,OAAO,CAAC,qBAAqB,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC;MAChE,IAAI,KAAK,KAAK,CAAC,CAAC,EAAE;QAChB,YAAY,CAAC,KAAK,CAAC,GAAG,aAAa,CAAC,CAAC,CAAC;MACvC;IACF;IAED,IAAI,CAAC,QAAQ,EAAE;MACb;MACA,OAAO,CAAC,gBAAgB,CAAC;IAC1B;EACF;EACD;EACA;EACA;EACA;EACA;EACA,gBAAgB,CAAC,YAAY,EAAE;EAE/B,OAAO,YAAY,GAAG,YAAY,GAAG,YAAY,CAAC,CAAC,CAAC;AACtD;AAUA;;;;;;;;;;AAUG;AACH,SAAS,oCAAoC,CACzC,OAAyB,EAAE,QAAkB,EAAA;EAE/C,IAAI,CAAC,MAAM,CACP,OAAO,IAAI,IAAI,IAAI,OAAO,CAAC,MAAM,GAAG,CAAC,EACrC,MAAM,uCAAuC,CAAC;EAElD,IAAI,WAAW,GAAqB,EAAE;EACtC,IAAI,iBAAiB,GAAiB,CAAA,CAAE;EACxC,IAAI,OAAO,CAAC,MAAM,KAAK,CAAC,EAAE;IACxB;IACA,MAAM,GAAG,GACL,+CAA+C,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,QAAQ,CAAC;IACzE,WAAW,GAAG,GAAG,CAAC,MAAM;IACxB,iBAAiB,GAAG,GAAG,CAAC,YAAY;GACrC,MAAM;IACL,MAAM,OAAO,GAAG,IAAI,GAAG,EAAU;IACjC,KAAK,MAAM,KAAK,IAAI,OAAO,EAAE;MAC3B,MAAM;QAAC,MAAM;QAAE;MAAY,CAAC,GACxB,+CAA+C,CAAC,KAAK,EAAE,QAAQ,CAAC;MAEpE;MACA,KAAK,MAAM,cAAc,IAAI,MAAM,EAAE;QACnC,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,cAAc,CAAC,IAAI,CAAC,EAAE;UACrC,WAAW,CAAC,IAAI,CAAC,cAAc,CAAC;UAChC,OAAO,CAAC,GAAG,CAAC,cAAc,CAAC,IAAI,CAAC;QACjC;MACF;MAED;MACA,KAAK,MAAM,IAAI,IAAI,YAAY,EAAE;QAC/B,IAAI,iBAAiB,CAAC,IAAI,CAAC,IAAI,IAAI,EAAE;UACnC,iBAAiB,CAAC,IAAI,CAAC,GAAG,IAAI,GAAG,EAAU;QAC5C;QACD,YAAY,CAAC,IAAI,CAAC,CAAC,OAAO,CACtB,SAAS,IAAI,iBAAiB,CAAC,IAAI,CAAC,CAAC,GAAG,CAAC,SAAS,CAAC,CAAC;MACzD;IACF;EACF;EACD,OAAO;IACL,MAAM,EAAE,WAAW;IACnB,eAAe,EAAE,mBAAmB,CAAC,iBAAiB;GACvD;AACH;AAEA,SAAS,mBAAmB,CAAC,YAA0B,EAAA;EACrD,MAAM,eAAe,GAAoB,CAAA,CAAE;EAC3C,KAAK,MAAM,IAAI,IAAI,YAAY,EAAE;IAC/B,eAAe,CAAC,IAAI,CAAC,GAAG,YAAY,CAAC,IAAI,CAAC,CAAC,IAAI;EAChD;EACD,OAAO,eAAe;AACxB;AAEA;;;;;;;;;;AAUG;AACH,OAAM,SAAU,+CAA+C,CAC3D,KAAqB,EAAE,QAAkB,EAAA;EAE3C,MAAM,OAAO,GAAG,IAAI,GAAG,EAAU;EACjC,MAAM,MAAM,GAAqB,EAAE;EACnC,MAAM,YAAY,GAAiB,CAAA,CAAE;EAErC;EACA;EACA;EACA,KAAK,MAAM,GAAG,IAAI,QAAQ,CAAC,KAAK,EAAE,EAAE;IAClC,OAAO,CAAC,GAAG,CAAC,GAAG,CAAC;EACjB;EAED,MAAM,KAAK,GAAqB,EAAE;EAClC,MAAM,KAAK,GAAa,EAAE;EAE1B;EACA,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC;EAEjB,OAAO,KAAK,CAAC,MAAM,GAAG,CAAC,EAAE;IACvB,MAAM,GAAG,GAAG,KAAK,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC;IACnC,IAAI,OAAO,CAAC,GAAG,CAAC,GAAG,CAAC,IAAI,CAAC,EAAE;MACzB,KAAK,CAAC,GAAG,EAAE;MACX;IACD;IACD,MAAM,WAAW,GAAG,KAAK,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,KAAK,KAAK,CAAC,MAAM,GAAG,CAAC;IAChE,IAAI,GAAG,CAAC,MAAM,CAAC,MAAM,KAAK,CAAC,IAAI,WAAW,EAAE;MAC1C;MACA,KAAK,CAAC,GAAG,EAAE;MACX,MAAM,CAAC,IAAI,CAAC,GAAG,CAAC;MAChB,OAAO,CAAC,GAAG,CAAC,GAAG,CAAC,IAAI,CAAC;MACrB,IAAI,WAAW,EAAE;QACf,KAAK,CAAC,GAAG,EAAE;MACZ;KACF,MAAM;MACL;MACA;MACA,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC;MAC5B,KAAK,MAAM,KAAK,IAAI,GAAG,CAAC,MAAM,EAAE;QAC9B;QACA;QACA,IAAI,YAAY,CAAC,KAAK,CAAC,IAAI,CAAC,IAAI,IAAI,EAAE;UACpC,YAAY,CAAC,KAAK,CAAC,IAAI,CAAC,GAAG,IAAI,GAAG,EAAU;QAC7C;QACD,YAAY,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,GAAG,CAAC,GAAG,CAAC,IAAI,CAAC;QAEtC,IAAI,OAAO,CAAC,GAAG,CAAC,KAAK,CAAC,IAAI,CAAC,EAAE;UAC3B,SAAS,CAAE;QACZ;;QACD,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC;MAClB;IACF;EACF;EACD,OAAO;IAAC,MAAM;IAAE;EAAY,CAAC;AAC/B;AAEA;;;;;AAKG;AACH,SAAS,cAAc,CAAC,KAAqB,EAAA;EAE3C,IAAI,YAA6C;EACjD,IAAI,KAAK,CAAC,WAAW,CAAC,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;IAC/C,YAAY,GAAG,KAAK,CAAC,WAAW,CAAC,MAAM;GACxC,MAAM;IACL,IAAI,SAAS,GAAW,IAAI;IAC5B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,WAAW,CAAC,YAAY,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;MAC9D,KAAK,MAAM,YAAY,IAAI,KAAK,CAAC,WAAW,CAAC,YAAY,CAAC,CAAC,CAAC,CAClD,aAAa,EAAE;QACvB,IAAI,YAAY,CAAC,EAAE,KAAK,KAAK,CAAC,EAAE,EAAE;UAChC,SAAS,GAAG,CAAC;UACb;QACD;MACF;IACF;IACD,YAAY,GAAG,KAAK,CAAC,WAAW,CAAC,WAAW,CAAC,SAAS,CAAC;EACxD;EACD,OAAO,YAAY;AACrB","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Executor: Evaluates SymbolicTensor based on feeds.\n */\nimport { cast, dispose, memory, util } from '@tensorflow/tfjs-core';\nimport { ValueError } from '../errors';\nimport { toList } from '../utils/generic_utils';\nimport { InputLayer } from './input_layer';\nimport { SymbolicTensor } from './topology';\n/**\n * Helper function to check the dtype and shape compatibility of a feed value.\n */\nfunction assertFeedCompatibility(key, val) {\n    // Check dtype compatibility.\n    if (key.dtype == null || key.dtype === val.dtype) {\n        //  a.  If types match, return val tensor as is.\n        return val;\n    }\n    try {\n        //  b. Attempt to convert to expected type.\n        return cast(val, key.dtype);\n    }\n    catch (err) {\n        //  c. If conversion fails, return helpful error.\n        throw new ValueError(`The dtype of the feed (${val.dtype}) can not be cast to the dtype ` +\n            `of the key '${key.name}' (${key.dtype}).`);\n    }\n}\n/**\n * FeedDict: A mapping from unique SymbolicTensors to feed values for them.\n * A feed value is a concrete value represented as an `Tensor`.\n */\nexport class FeedDict {\n    /**\n     * Constructor, optionally does copy-construction.\n     * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case\n     *   copy-construction will be performed.\n     */\n    constructor(feeds) {\n        this.id2Value = {};\n        this.id2Mask = {};\n        this.name2Id = {};\n        if (feeds instanceof FeedDict) {\n            for (const id in feeds.id2Value) {\n                this.id2Value[id] = feeds.id2Value[id];\n                if (id in feeds.id2Mask) {\n                    this.id2Mask[id] = feeds.id2Mask[id];\n                }\n            }\n        }\n        else {\n            if (feeds == null) {\n                return;\n            }\n            for (const feed of feeds) {\n                this.add(feed.key, feed.value);\n            }\n        }\n    }\n    /**\n     * Add a key-value pair to the FeedDict.\n     *\n     * @param key The key of the feed.\n     * @param value The value of the tensor feed.\n     * @param mask The value of the mask feed (optional).\n     * @returns This `FeedDict`.\n     * @throws ValueError: If the key `SymbolicTensor` already exists in the\n     *   `FeedDict`.\n     */\n    add(key, value, mask) {\n        if (this.id2Value[key.id] == null) {\n            this.id2Value[key.id] = assertFeedCompatibility(key, value);\n            this.name2Id[key.name] = key.id;\n            if (mask != null) {\n                this.id2Mask[key.id] = mask;\n            }\n        }\n        else {\n            throw new ValueError(`Duplicate key: name=${key.name}, id=${key.id}`);\n        }\n        return this;\n    }\n    /**\n     * Add a Feed to the FeedDict.\n     * @param feed The new `Feed` to add.\n     * @returns This `FeedDict`.\n     */\n    addFeed(feed) {\n        this.add(feed.key, feed.value);\n    }\n    /**\n     * Probe whether a key already exists in the FeedDict.\n     * @param key\n     */\n    hasKey(key) {\n        return this.id2Value[key.id] != null;\n    }\n    /**\n     * Get all the SymbolicTensor available in this FeedDict.\n     */\n    names() {\n        return Object.keys(this.name2Id);\n    }\n    /**\n     * Get the feed value for given key.\n     * @param key The SymbolicTensor, or its name (as a string), of which the\n     *     value is sought.\n     * @returns If `key` exists, the corresponding feed value.\n     * @throws ValueError: If `key` does not exist in this `FeedDict`.\n     */\n    getValue(key) {\n        if (key instanceof SymbolicTensor) {\n            if (this.id2Value[key.id] == null) {\n                throw new ValueError(`Nonexistent key: ${key.name}`);\n            }\n            else {\n                return this.id2Value[key.id];\n            }\n        }\n        else {\n            const id = this.name2Id[key];\n            if (id == null) {\n                throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n            }\n            return this.id2Value[id];\n        }\n    }\n    /**\n     * Get the feed mask for given key.\n     * @param key The SymbolicTensor, or its name (as a string), of which the\n     *     value is sought.\n     * @returns If `key` exists, the corresponding feed mask.\n     * @throws ValueError: If `key` does not exist in this `FeedDict`.\n     */\n    getMask(key) {\n        if (key instanceof SymbolicTensor) {\n            if (this.id2Value[key.id] == null) {\n                throw new ValueError(`Nonexistent key: ${key.name}`);\n            }\n            else {\n                return this.id2Mask[key.id];\n            }\n        }\n        else {\n            const id = this.name2Id[key];\n            if (id == null) {\n                throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n            }\n            return this.id2Mask[id];\n        }\n    }\n    /** Dispose all mask Tensors held by this object. */\n    disposeMasks() {\n        if (this.id2Mask != null) {\n            dispose(this.id2Mask);\n        }\n    }\n}\n// Cache for topologically sorted SymbolicTensors for given execution\n// targets (i.e., fetches).\nconst cachedSorted = {};\n// Cache for recipient count maps for given execution targets (i.e., fetches).\nconst cachedRecipientCounts = {};\n/**\n * Execute a SymbolicTensor by using concrete feed values.\n *\n * A `SymbolicTensor` object is a node in a computation graph of TF.js\n * Layers. The object is backed by a source layer and input\n * `SymbolicTensor`s to the source layer. This method evaluates\n * the `call()` method of the source layer, using concrete values of the\n * inputs obtained from either\n * * `feedDict`, if the input key exists in `feedDict`, or else,\n * * a recursive call to `execute()` itself.\n *\n * @param x: The `SymbolicTensor` to execute.\n * @param feedDict: The feed values, as base condition of the recursion.\n *   execution.\n * @param kwargs: Optional keyword arguments.\n * @param probe: A probe object (of interface `ExecutionProbe`) used for\n *   testing memory footprint of `execute` calls.\n * @returns Result of the execution.\n * @throws ValueError: If any `SymbolicTensor`s from `InputLayer`s\n *   encountered during the execution lacks a feed value in `feedDict`.\n */\nexport function execute(fetches, feedDict, kwargs, probe) {\n    const training = kwargs == null ? false : kwargs['training'];\n    const arrayFetches = Array.isArray(fetches);\n    const fetchArray = arrayFetches ? fetches : [fetches];\n    const outputNames = fetchArray.map(t => t.name);\n    const finalOutputs = [];\n    const feedNames = feedDict.names();\n    for (const outputName of outputNames) {\n        if (feedNames.indexOf(outputName) !== -1) {\n            finalOutputs.push(feedDict.getValue(outputName));\n        }\n        else {\n            finalOutputs.push(null);\n        }\n    }\n    if (probe != null) {\n        // For optional probing of memory footprint during execution.\n        probe.maxNumTensors = -Infinity;\n        probe.minNumTensors = Infinity;\n    }\n    // Check cache.\n    const fetchAndFeedKey = outputNames.join(',') + '|' + feedDict.names().join(',');\n    let sorted;\n    let recipientCounts;\n    if (cachedSorted[fetchAndFeedKey] == null) {\n        // Cache doesn't contain the desired combination of fetches. Compute\n        // topological sort for the combination for the first time.\n        const out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);\n        sorted = out.sorted;\n        recipientCounts = out.recipientCounts;\n        // Store results in cache for future use.\n        cachedSorted[fetchAndFeedKey] = sorted;\n        cachedRecipientCounts[fetchAndFeedKey] = recipientCounts;\n    }\n    sorted = cachedSorted[fetchAndFeedKey];\n    recipientCounts = {};\n    if (!training) {\n        Object.assign(recipientCounts, cachedRecipientCounts[fetchAndFeedKey]);\n    }\n    const internalFeedDict = new FeedDict(feedDict);\n    // Start iterative execution on the topologically-sorted SymbolicTensors.\n    for (let i = 0; i < sorted.length; ++i) {\n        if (probe != null) {\n            // For optional probing of memory usage during execution.\n            const numTensors = memory().numTensors;\n            if (numTensors > probe.maxNumTensors) {\n                probe.maxNumTensors = numTensors;\n            }\n            if (numTensors < probe.minNumTensors) {\n                probe.minNumTensors = numTensors;\n            }\n        }\n        const symbolic = sorted[i];\n        const srcLayer = symbolic.sourceLayer;\n        if (srcLayer instanceof InputLayer) {\n            continue;\n        }\n        const inputValues = [];\n        const inputMasks = [];\n        const tensorsToDispose = [];\n        let maskExists = false;\n        for (const input of symbolic.inputs) {\n            const value = internalFeedDict.getValue(input);\n            const mask = internalFeedDict.getMask(input);\n            inputValues.push(value);\n            inputMasks.push(mask);\n            if (mask != null) {\n                maskExists = true;\n            }\n            if (!training) {\n                recipientCounts[input.name]--;\n                if (recipientCounts[input.name] === 0 && !feedDict.hasKey(input) &&\n                    outputNames.indexOf(input.name) === -1 && !value.isDisposed &&\n                    input.sourceLayer.stateful !== true) {\n                    tensorsToDispose.push(value);\n                }\n            }\n        }\n        if (maskExists) {\n            kwargs = kwargs || {};\n            kwargs['mask'] = inputMasks[0];\n        }\n        const outputTensors = toList(srcLayer.apply(inputValues, kwargs));\n        let outputMask = null;\n        if (srcLayer.supportsMasking) {\n            outputMask = srcLayer.computeMask(inputValues, inputMasks);\n        }\n        const layerOutputs = getNodeOutputs(symbolic);\n        const outputSymbolicTensors = Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];\n        for (let i = 0; i < outputSymbolicTensors.length; ++i) {\n            if (!internalFeedDict.hasKey(outputSymbolicTensors[i])) {\n                internalFeedDict.add(outputSymbolicTensors[i], outputTensors[i], Array.isArray(outputMask) ? outputMask[0] : outputMask);\n            }\n            const index = outputNames.indexOf(outputSymbolicTensors[i].name);\n            if (index !== -1) {\n                finalOutputs[index] = outputTensors[i];\n            }\n        }\n        if (!training) {\n            // Clean up Tensors that are no longer needed.\n            dispose(tensorsToDispose);\n        }\n    }\n    // NOTE(cais): Unlike intermediate tensors, we don't discard mask\n    // tensors as we go, because these tensors are sometimes passed over a\n    // series of mutliple layers, i.e., not obeying the immediate input\n    // relations in the graph. If this becomes a memory-usage concern,\n    // we can improve this in the future.\n    internalFeedDict.disposeMasks();\n    return arrayFetches ? finalOutputs : finalOutputs[0];\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for an array of fetches.\n *\n * This function calls getTopologicalSortAndRecipientCountsForOneFetch and\n * merges their results.\n *\n * @param fetch The array of fetches requested. Must be a non-empty array.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientCounts: Recipient counts for all SymbolicTensors in `sorted`.\n */\nfunction getTopologicalSortAndRecipientCounts(fetches, feedDict) {\n    util.assert(fetches != null && fetches.length > 0, () => `Expected at least one fetch, got none`);\n    let finalSorted = [];\n    let finalRecipientMap = {};\n    if (fetches.length === 1) {\n        // Special-casing 1 fetch for efficiency.\n        const out = getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);\n        finalSorted = out.sorted;\n        finalRecipientMap = out.recipientMap;\n    }\n    else {\n        const visited = new Set();\n        for (const fetch of fetches) {\n            const { sorted, recipientMap } = getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict);\n            // Merge sorted SymbolicTensor Arrays.\n            for (const symbolicTensor of sorted) {\n                if (!visited.has(symbolicTensor.name)) {\n                    finalSorted.push(symbolicTensor);\n                    visited.add(symbolicTensor.name);\n                }\n            }\n            // Merge recipient maps.\n            for (const name in recipientMap) {\n                if (finalRecipientMap[name] == null) {\n                    finalRecipientMap[name] = new Set();\n                }\n                recipientMap[name].forEach(recipient => finalRecipientMap[name].add(recipient));\n            }\n        }\n    }\n    return {\n        sorted: finalSorted,\n        recipientCounts: recipientMap2Counts(finalRecipientMap)\n    };\n}\nfunction recipientMap2Counts(recipientMap) {\n    const recipientCounts = {};\n    for (const name in recipientMap) {\n        recipientCounts[name] = recipientMap[name].size;\n    }\n    return recipientCounts;\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for a single fetch.\n *\n * This helper function processes the upstream SymbolicTensors of a single\n * fetch.\n *\n * @param fetch The single fetch requested.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientMap: Recipient names for all SymbolicTensors in `sorted`.\n */\nexport function getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict) {\n    const visited = new Set();\n    const sorted = [];\n    const recipientMap = {};\n    // Put keys of the feedDict into visited first, so they don't have to be\n    // walked. This is needed in case where there are feeds for intermediate\n    // SymbolicTensors of the graph.\n    for (const key of feedDict.names()) {\n        visited.add(key);\n    }\n    const stack = [];\n    const marks = [];\n    // Initial population of stack and marks.\n    stack.push(fetch);\n    while (stack.length > 0) {\n        const top = stack[stack.length - 1];\n        if (visited.has(top.name)) {\n            stack.pop();\n            continue;\n        }\n        const topIsMarked = marks[marks.length - 1] === stack.length - 1;\n        if (top.inputs.length === 0 || topIsMarked) {\n            // Input SymbolicTensor or all children have been visited.\n            stack.pop();\n            sorted.push(top);\n            visited.add(top.name);\n            if (topIsMarked) {\n                marks.pop();\n            }\n        }\n        else {\n            // A non-input SymbolicTensor whose upstream SymbolicTensors haven't\n            // been visited yet. Push them onto the stack.\n            marks.push(stack.length - 1);\n            for (const input of top.inputs) {\n                // Increment the recipient count. Note that this needs to happen\n                // regardless of whether the SymbolicTensor has been visited before.\n                if (recipientMap[input.name] == null) {\n                    recipientMap[input.name] = new Set();\n                }\n                recipientMap[input.name].add(top.name);\n                if (visited.has(input.name)) {\n                    continue; // Avoid repeated visits to the same SymbolicTensor.\n                }\n                stack.push(input);\n            }\n        }\n    }\n    return { sorted, recipientMap };\n}\n/**\n * Get the symbolic output tensors of the node to which a given fetch belongs.\n * @param fetch The fetched symbolic tensor.\n * @returns The Array of symbolic tensors output by the node to which `fetch`\n *   belongs.\n */\nfunction getNodeOutputs(fetch) {\n    let layerOutputs;\n    if (fetch.sourceLayer.inboundNodes.length === 1) {\n        layerOutputs = fetch.sourceLayer.output;\n    }\n    else {\n        let nodeIndex = null;\n        for (let i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {\n            for (const outputTensor of fetch.sourceLayer.inboundNodes[i]\n                .outputTensors) {\n                if (outputTensor.id === fetch.id) {\n                    nodeIndex = i;\n                    break;\n                }\n            }\n        }\n        layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);\n    }\n    return layerOutputs;\n}\n//# sourceMappingURL=executor.js.map"]},"metadata":{},"sourceType":"module"}
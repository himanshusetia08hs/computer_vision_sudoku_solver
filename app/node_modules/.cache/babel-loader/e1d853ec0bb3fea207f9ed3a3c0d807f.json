{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Recurrent Neural Network Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getActivation, serializeActivation } from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, SymbolicTensor } from '../engine/topology';\nimport { Layer } from '../engine/topology';\nimport { AttributeError, NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, Initializer, Ones, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { assertPositiveInteger } from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor, isArrayOfShapes } from '../utils/types_utils';\nimport { batchGetValue, batchSetValue } from '../variables';\nimport { deserialize } from './serialization';\n/**\n * Standardize `apply()` args to a single list of tensor inputs.\n *\n * When running a model loaded from file, the input tensors `initialState` and\n * `constants` are passed to `RNN.apply()` as part of `inputs` instead of the\n * dedicated kwargs fields. `inputs` consists of\n * `[inputs, initialState0, initialState1, ..., constant0, constant1]` in this\n * case.\n * This method makes sure that arguments are\n * separated and that `initialState` and `constants` are `Array`s of tensors\n * (or None).\n *\n * @param inputs Tensor or `Array` of  tensors.\n * @param initialState Tensor or `Array` of tensors or `null`/`undefined`.\n * @param constants Tensor or `Array` of tensors or `null`/`undefined`.\n * @returns An object consisting of\n *   inputs: A tensor.\n *   initialState: `Array` of tensors or `null`.\n *   constants: `Array` of tensors or `null`.\n * @throws ValueError, if `inputs` is an `Array` but either `initialState` or\n *   `constants` is provided.\n */\nexport function standardizeArgs(inputs, initialState, constants, numConstants) {\n  if (Array.isArray(inputs)) {\n    if (initialState != null || constants != null) {\n      throw new ValueError('When inputs is an array, neither initialState or constants ' + 'should be provided');\n    }\n    if (numConstants != null) {\n      constants = inputs.slice(inputs.length - numConstants, inputs.length);\n      inputs = inputs.slice(0, inputs.length - numConstants);\n    }\n    if (inputs.length > 1) {\n      initialState = inputs.slice(1, inputs.length);\n    }\n    inputs = inputs[0];\n  }\n  function toListOrNull(x) {\n    if (x == null || Array.isArray(x)) {\n      return x;\n    } else {\n      return [x];\n    }\n  }\n  initialState = toListOrNull(initialState);\n  constants = toListOrNull(constants);\n  return {\n    inputs,\n    initialState,\n    constants\n  };\n}\n/**\n * Iterates over the time dimension of a tensor.\n *\n * @param stepFunction RNN step function.\n *   Parameters:\n *     inputs: tensor with shape `[samples, ...]` (no time dimension),\n *       representing input for the batch of samples at a certain time step.\n *     states: an Array of tensors.\n *   Returns:\n *     outputs: tensor with shape `[samples, outputDim]` (no time dimension).\n *     newStates: list of tensors, same length and shapes as `states`. The first\n *       state in the list must be the output tensor at the previous timestep.\n * @param inputs Tensor of temporal data of shape `[samples, time, ...]` (at\n *   least 3D).\n * @param initialStates Tensor with shape `[samples, outputDim]` (no time\n *   dimension), containing the initial values of the states used in the step\n *   function.\n * @param goBackwards If `true`, do the iteration over the time dimension in\n *   reverse order and return the reversed sequence.\n * @param mask Binary tensor with shape `[sample, time, 1]`, with a zero for\n *   every element that is masked.\n * @param constants An Array of constant values passed at each step.\n * @param unroll Whether to unroll the RNN or to use a symbolic loop. *Not*\n *   applicable to this imperative deeplearn.js backend. Its value is ignored.\n * @param needPerStepOutputs Whether the per-step outputs are to be\n *   concatenated into a single tensor and returned (as the second return\n *   value). Default: `false`. This arg is included so that the relatively\n *   expensive concatenation of the stepwise outputs can be omitted unless\n *   the stepwise outputs need to be kept (e.g., for an LSTM layer of which\n *   `returnSequence` is `true`.)\n * @returns An Array: `[lastOutput, outputs, newStates]`.\n *   lastOutput: the lastest output of the RNN, of shape `[samples, ...]`.\n *   outputs: tensor with shape `[samples, time, ...]` where each entry\n *     `output[s, t]` is the output of the step function at time `t` for sample\n *     `s`. This return value is provided if and only if the\n *     `needPerStepOutputs` is set as `true`. If it is set as `false`, this\n *     return value will be `undefined`.\n *   newStates: Array of tensors, latest states returned by the step function,\n *      of shape `(samples, ...)`.\n * @throws ValueError If input dimension is less than 3.\n *\n * TODO(nielsene): This needs to be tidy-ed.\n */\nexport function rnn(stepFunction, inputs, initialStates) {\n  let goBackwards = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : false;\n  let mask = arguments.length > 4 ? arguments[4] : undefined;\n  let constants = arguments.length > 5 ? arguments[5] : undefined;\n  let unroll = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : false;\n  let needPerStepOutputs = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : false;\n  return tfc.tidy(() => {\n    const ndim = inputs.shape.length;\n    if (ndim < 3) {\n      throw new ValueError(`Input should be at least 3D, but is ${ndim}D.`);\n    }\n    // Transpose to time-major, i.e., from [batch, time, ...] to [time, batch,\n    // ...].\n    const axes = [1, 0].concat(math_utils.range(2, ndim));\n    inputs = tfc.transpose(inputs, axes);\n    if (constants != null) {\n      throw new NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' + 'constants yet.');\n    }\n    // Porting Note: the unroll option is ignored by the imperative backend.\n    if (unroll) {\n      console.warn('Backend rnn(): the unroll = true option is not applicable to the ' + 'imperative deeplearn.js backend.');\n    }\n    if (mask != null) {\n      mask = mask.asType('bool').asType('float32');\n      if (mask.rank === ndim - 1) {\n        mask = tfc.expandDims(mask, -1);\n      }\n      mask = tfc.transpose(mask, axes);\n    }\n    if (goBackwards) {\n      inputs = tfc.reverse(inputs, 0);\n      if (mask != null) {\n        mask = tfc.reverse(mask, 0);\n      }\n    }\n    // Porting Note: PyKeras with TensorFlow backend uses a symbolic loop\n    //   (tf.while_loop). But for the imperative deeplearn.js backend, we just\n    //   use the usual TypeScript control flow to iterate over the time steps in\n    //   the inputs.\n    // Porting Note: PyKeras patches a \"_use_learning_phase\" attribute to\n    // outputs.\n    //   This is not idiomatic in TypeScript. The info regarding whether we are\n    //   in a learning (i.e., training) phase for RNN is passed in a different\n    //   way.\n    const perStepOutputs = [];\n    let lastOutput;\n    let states = initialStates;\n    const timeSteps = inputs.shape[0];\n    const perStepInputs = tfc.unstack(inputs);\n    let perStepMasks;\n    if (mask != null) {\n      perStepMasks = tfc.unstack(mask);\n    }\n    for (let t = 0; t < timeSteps; ++t) {\n      const currentInput = perStepInputs[t];\n      const stepOutputs = tfc.tidy(() => stepFunction(currentInput, states));\n      if (mask == null) {\n        lastOutput = stepOutputs[0];\n        states = stepOutputs[1];\n      } else {\n        const maskedOutputs = tfc.tidy(() => {\n          const stepMask = perStepMasks[t];\n          const negStepMask = tfc.onesLike(stepMask).sub(stepMask);\n          // TODO(cais): Would tfc.where() be better for performance?\n          const output = stepOutputs[0].mul(stepMask).add(states[0].mul(negStepMask));\n          const newStates = states.map((state, i) => {\n            return stepOutputs[1][i].mul(stepMask).add(state.mul(negStepMask));\n          });\n          return {\n            output,\n            newStates\n          };\n        });\n        lastOutput = maskedOutputs.output;\n        states = maskedOutputs.newStates;\n      }\n      if (needPerStepOutputs) {\n        perStepOutputs.push(lastOutput);\n      }\n    }\n    let outputs;\n    if (needPerStepOutputs) {\n      const axis = 1;\n      outputs = tfc.stack(perStepOutputs, axis);\n    }\n    return [lastOutput, outputs, states];\n  });\n}\nexport class RNN extends Layer {\n  constructor(args) {\n    super(args);\n    let cell;\n    if (args.cell == null) {\n      throw new ValueError('cell property is missing for the constructor of RNN.');\n    } else if (Array.isArray(args.cell)) {\n      cell = new StackedRNNCells({\n        cells: args.cell\n      });\n    } else {\n      cell = args.cell;\n    }\n    if (cell.stateSize == null) {\n      throw new ValueError('The RNN cell should have an attribute `stateSize` (tuple of ' + 'integers, one integer per RNN state).');\n    }\n    this.cell = cell;\n    this.returnSequences = args.returnSequences == null ? false : args.returnSequences;\n    this.returnState = args.returnState == null ? false : args.returnState;\n    this.goBackwards = args.goBackwards == null ? false : args.goBackwards;\n    this._stateful = args.stateful == null ? false : args.stateful;\n    this.unroll = args.unroll == null ? false : args.unroll;\n    this.supportsMasking = true;\n    this.inputSpec = [new InputSpec({\n      ndim: 3\n    })];\n    this.stateSpec = null;\n    this.states_ = null;\n    // TODO(cais): Add constantsSpec and numConstants.\n    this.numConstants = null;\n    // TODO(cais): Look into the use of initial_state in the kwargs of the\n    //   constructor.\n    this.keptStates = [];\n  }\n  // Porting Note: This is the equivalent of `RNN.states` property getter in\n  //   PyKeras.\n  getStates() {\n    if (this.states_ == null) {\n      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      return math_utils.range(0, numStates).map(x => null);\n    } else {\n      return this.states_;\n    }\n  }\n  // Porting Note: This is the equivalent of the `RNN.states` property setter in\n  //   PyKeras.\n  setStates(states) {\n    this.states_ = states;\n  }\n  computeOutputShape(inputShape) {\n    if (isArrayOfShapes(inputShape)) {\n      inputShape = inputShape[0];\n    }\n    inputShape = inputShape;\n    // TODO(cais): Remove the casting once stacked RNN cells become supported.\n    let stateSize = this.cell.stateSize;\n    if (!Array.isArray(stateSize)) {\n      stateSize = [stateSize];\n    }\n    const outputDim = stateSize[0];\n    let outputShape;\n    if (this.returnSequences) {\n      outputShape = [inputShape[0], inputShape[1], outputDim];\n    } else {\n      outputShape = [inputShape[0], outputDim];\n    }\n    if (this.returnState) {\n      const stateShape = [];\n      for (const dim of stateSize) {\n        stateShape.push([inputShape[0], dim]);\n      }\n      return [outputShape].concat(stateShape);\n    } else {\n      return outputShape;\n    }\n  }\n  computeMask(inputs, mask) {\n    return tfc.tidy(() => {\n      if (Array.isArray(mask)) {\n        mask = mask[0];\n      }\n      const outputMask = this.returnSequences ? mask : null;\n      if (this.returnState) {\n        const stateMask = this.states.map(s => null);\n        return [outputMask].concat(stateMask);\n      } else {\n        return outputMask;\n      }\n    });\n  }\n  /**\n   * Get the current state tensors of the RNN.\n   *\n   * If the state hasn't been set, return an array of `null`s of the correct\n   * length.\n   */\n  get states() {\n    if (this.states_ == null) {\n      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      const output = [];\n      for (let i = 0; i < numStates; ++i) {\n        output.push(null);\n      }\n      return output;\n    } else {\n      return this.states_;\n    }\n  }\n  set states(s) {\n    this.states_ = s;\n  }\n  build(inputShape) {\n    // Note inputShape will be an Array of Shapes of initial states and\n    // constants if these are passed in apply().\n    const constantShape = null;\n    if (this.numConstants != null) {\n      throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n    }\n    if (isArrayOfShapes(inputShape)) {\n      inputShape = inputShape[0];\n    }\n    inputShape = inputShape;\n    const batchSize = this.stateful ? inputShape[0] : null;\n    const inputDim = inputShape.slice(2);\n    this.inputSpec[0] = new InputSpec({\n      shape: [batchSize, null, ...inputDim]\n    });\n    // Allow cell (if RNNCell Layer) to build before we set or validate\n    // stateSpec.\n    const stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    if (constantShape != null) {\n      throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n    } else {\n      this.cell.build(stepInputShape);\n    }\n    // Set or validate stateSpec.\n    let stateSize;\n    if (Array.isArray(this.cell.stateSize)) {\n      stateSize = this.cell.stateSize;\n    } else {\n      stateSize = [this.cell.stateSize];\n    }\n    if (this.stateSpec != null) {\n      if (!util.arraysEqual(this.stateSpec.map(spec => spec.shape[spec.shape.length - 1]), stateSize)) {\n        throw new ValueError(`An initialState was passed that is not compatible with ` + `cell.stateSize. Received stateSpec=${this.stateSpec}; ` + `However cell.stateSize is ${this.cell.stateSize}`);\n      }\n    } else {\n      this.stateSpec = stateSize.map(dim => new InputSpec({\n        shape: [null, dim]\n      }));\n    }\n    if (this.stateful) {\n      this.resetStates();\n    }\n  }\n  /**\n   * Reset the state tensors of the RNN.\n   *\n   * If the `states` argument is `undefined` or `null`, will set the\n   * state tensor(s) of the RNN to all-zero tensors of the appropriate\n   * shape(s).\n   *\n   * If `states` is provided, will set the state tensors of the RNN to its\n   * value.\n   *\n   * @param states Optional externally-provided initial states.\n   * @param training Whether this call is done during training. For stateful\n   *   RNNs, this affects whether the old states are kept or discarded. In\n   *   particular, if `training` is `true`, the old states will be kept so\n   *   that subsequent backpropgataion through time (BPTT) may work properly.\n   *   Else, the old states will be discarded.\n   */\n  resetStates(states) {\n    let training = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;\n    tidy(() => {\n      if (!this.stateful) {\n        throw new AttributeError('Cannot call resetStates() on an RNN Layer that is not stateful.');\n      }\n      const batchSize = this.inputSpec[0].shape[0];\n      if (batchSize == null) {\n        throw new ValueError('If an RNN is stateful, it needs to know its batch size. Specify ' + 'the batch size of your input tensors: \\n' + '- If using a Sequential model, specify the batch size by ' + 'passing a `batchInputShape` option to your first layer.\\n' + '- If using the functional API, specify the batch size by ' + 'passing a `batchShape` option to your Input layer.');\n      }\n      // Initialize state if null.\n      if (this.states_ == null) {\n        if (Array.isArray(this.cell.stateSize)) {\n          this.states_ = this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n        } else {\n          this.states_ = [tfc.zeros([batchSize, this.cell.stateSize])];\n        }\n      } else if (states == null) {\n        // Dispose old state tensors.\n        tfc.dispose(this.states_);\n        // For stateful RNNs, fully dispose kept old states.\n        if (this.keptStates != null) {\n          tfc.dispose(this.keptStates);\n          this.keptStates = [];\n        }\n        if (Array.isArray(this.cell.stateSize)) {\n          this.states_ = this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n        } else {\n          this.states_[0] = tfc.zeros([batchSize, this.cell.stateSize]);\n        }\n      } else {\n        if (!Array.isArray(states)) {\n          states = [states];\n        }\n        if (states.length !== this.states_.length) {\n          throw new ValueError(`Layer ${this.name} expects ${this.states_.length} state(s), ` + `but it received ${states.length} state value(s). Input ` + `received: ${states}`);\n        }\n        if (training === true) {\n          // Store old state tensors for complete disposal later, i.e., during\n          // the next no-arg call to this method. We do not dispose the old\n          // states immediately because that BPTT (among other things) require\n          // them.\n          this.keptStates.push(this.states_.slice());\n        } else {\n          tfc.dispose(this.states_);\n        }\n        for (let index = 0; index < this.states_.length; ++index) {\n          const value = states[index];\n          const dim = Array.isArray(this.cell.stateSize) ? this.cell.stateSize[index] : this.cell.stateSize;\n          const expectedShape = [batchSize, dim];\n          if (!util.arraysEqual(value.shape, expectedShape)) {\n            throw new ValueError(`State ${index} is incompatible with layer ${this.name}: ` + `expected shape=${expectedShape}, received shape=${value.shape}`);\n          }\n          this.states_[index] = value;\n        }\n      }\n      this.states_ = this.states_.map(state => tfc.keep(state.clone()));\n    });\n  }\n  apply(inputs, kwargs) {\n    // TODO(cais): Figure out whether initialState is in kwargs or inputs.\n    let initialState = kwargs == null ? null : kwargs['initialState'];\n    let constants = kwargs == null ? null : kwargs['constants'];\n    if (kwargs == null) {\n      kwargs = {};\n    }\n    const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n    // If any of `initial_state` or `constants` are specified and are\n    // `tf.SymbolicTensor`s, then add them to the inputs and temporarily modify\n    // the input_spec to include them.\n    let additionalInputs = [];\n    let additionalSpecs = [];\n    if (initialState != null) {\n      kwargs['initialState'] = initialState;\n      additionalInputs = additionalInputs.concat(initialState);\n      this.stateSpec = [];\n      for (const state of initialState) {\n        this.stateSpec.push(new InputSpec({\n          shape: state.shape\n        }));\n      }\n      // TODO(cais): Use the following instead.\n      // this.stateSpec = initialState.map(state => new InputSpec({shape:\n      // state.shape}));\n      additionalSpecs = additionalSpecs.concat(this.stateSpec);\n    }\n    if (constants != null) {\n      kwargs['constants'] = constants;\n      additionalInputs = additionalInputs.concat(constants);\n      // TODO(cais): Add this.constantsSpec.\n      this.numConstants = constants.length;\n    }\n    const isTensor = additionalInputs[0] instanceof SymbolicTensor;\n    if (isTensor) {\n      // Compute full input spec, including state and constants.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n      // Perform the call with temporarily replaced inputSpec.\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output = super.apply(fullInput, kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n  // tslint:disable-next-line:no-any\n  call(inputs, kwargs) {\n    // Input shape: `[samples, time (padded with zeros), input_dim]`.\n    // Note that the .build() method of subclasses **must** define\n    // this.inputSpec and this.stateSpec owith complete input shapes.\n    return tidy(() => {\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      let initialState = kwargs == null ? null : kwargs['initialState'];\n      inputs = getExactlyOneTensor(inputs);\n      if (initialState == null) {\n        if (this.stateful) {\n          initialState = this.states_;\n        } else {\n          initialState = this.getInitialState(inputs);\n        }\n      }\n      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      if (initialState.length !== numStates) {\n        throw new ValueError(`RNN Layer has ${numStates} state(s) but was passed ` + `${initialState.length} initial state(s).`);\n      }\n      if (this.unroll) {\n        console.warn('Ignoring unroll = true for RNN layer, due to imperative backend.');\n      }\n      const cellCallKwargs = {\n        training\n      };\n      // TODO(cais): Add support for constants.\n      const step = (inputs, states) => {\n        // `inputs` and `states` are concatenated to form a single `Array` of\n        // `tf.Tensor`s as the input to `cell.call()`.\n        const outputs = this.cell.call([inputs].concat(states), cellCallKwargs);\n        // Marshall the return value into output and new states.\n        return [outputs[0], outputs.slice(1)];\n      };\n      // TODO(cais): Add support for constants.\n      const rnnOutputs = rnn(step, inputs, initialState, this.goBackwards, mask, null, this.unroll, this.returnSequences);\n      const lastOutput = rnnOutputs[0];\n      const outputs = rnnOutputs[1];\n      const states = rnnOutputs[2];\n      if (this.stateful) {\n        this.resetStates(states, training);\n      }\n      const output = this.returnSequences ? outputs : lastOutput;\n      // TODO(cais): Porperty set learning phase flag.\n      if (this.returnState) {\n        return [output].concat(states);\n      } else {\n        return output;\n      }\n    });\n  }\n  getInitialState(inputs) {\n    return tidy(() => {\n      // Build an all-zero tensor of shape [samples, outputDim].\n      // [Samples, timeSteps, inputDim].\n      let initialState = tfc.zeros(inputs.shape);\n      // [Samples].\n      initialState = tfc.sum(initialState, [1, 2]);\n      initialState = K.expandDims(initialState); // [Samples, 1].\n      if (Array.isArray(this.cell.stateSize)) {\n        return this.cell.stateSize.map(dim => dim > 1 ? K.tile(initialState, [1, dim]) : initialState);\n      } else {\n        return this.cell.stateSize > 1 ? [K.tile(initialState, [1, this.cell.stateSize])] : [initialState];\n      }\n    });\n  }\n  get trainableWeights() {\n    if (!this.trainable) {\n      return [];\n    }\n    // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n    return this.cell.trainableWeights;\n  }\n  get nonTrainableWeights() {\n    // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n    if (!this.trainable) {\n      return this.cell.weights;\n    }\n    return this.cell.nonTrainableWeights;\n  }\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.cell != null) {\n      this.cell.setFastWeightInitDuringBuild(value);\n    }\n  }\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      returnSequences: this.returnSequences,\n      returnState: this.returnState,\n      goBackwards: this.goBackwards,\n      stateful: this.stateful,\n      unroll: this.unroll\n    };\n    if (this.numConstants != null) {\n      config['numConstants'] = this.numConstants;\n    }\n    const cellConfig = this.cell.getConfig();\n    if (this.getClassName() === RNN.className) {\n      config['cell'] = {\n        'className': this.cell.getClassName(),\n        'config': cellConfig\n      };\n    }\n    // this order is necessary, to prevent cell name from replacing layer name\n    return Object.assign({}, cellConfig, baseConfig, config);\n  }\n  /** @nocollapse */\n  static fromConfig(cls, config) {\n    let customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    const cellConfig = config['cell'];\n    const cell = deserialize(cellConfig, customObjects);\n    return new cls(Object.assign(config, {\n      cell\n    }));\n  }\n}\n/** @nocollapse */\nRNN.className = 'RNN';\nserialization.registerClass(RNN);\n// Porting Note: This is a common parent class for RNN cells. There is no\n// equivalent of this in PyKeras. Having a common parent class forgoes the\n//  need for `has_attr(cell, ...)` checks or its TypeScript equivalent.\n/**\n * An RNNCell layer.\n *\n * @doc {heading: 'Layers', subheading: 'Classes'}\n */\nexport class RNNCell extends Layer {}\nexport class SimpleRNNCell extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_ACTIVATION = 'tanh';\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    this.units = args.units;\n    assertPositiveInteger(this.units, `units`);\n    this.activation = getActivation(args.activation == null ? this.DEFAULT_ACTIVATION : args.activation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    this.stateSize = this.units;\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    // TODO(cais): Use regularizer.\n    this.kernel = this.addWeight('kernel', [inputShape[inputShape.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n    if (this.useBias) {\n      this.bias = this.addWeight('bias', [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n    this.built = true;\n  }\n  // Porting Note: PyKeras' equivalent of this method takes two tensor inputs:\n  //   `inputs` and `states`. Here, the two tensors are combined into an\n  //   `Tensor[]` Array as the first input argument.\n  //   Similarly, PyKeras' equivalent of this method returns two values:\n  //    `output` and `[output]`. Here the two are combined into one length-2\n  //    `Tensor[]`, consisting of `output` repeated.\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n      if (inputs.length !== 2) {\n        throw new ValueError(`SimpleRNNCell expects 2 input Tensors, got ${inputs.length}.`);\n      }\n      let prevOutput = inputs[1];\n      inputs = inputs[0];\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(inputs),\n          rate: this.dropout,\n          training\n        });\n      }\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(prevOutput),\n          rate: this.recurrentDropout,\n          training\n        });\n      }\n      let h;\n      const dpMask = this.dropoutMask;\n      const recDpMask = this.recurrentDropoutMask;\n      if (dpMask != null) {\n        h = K.dot(tfc.mul(inputs, dpMask), this.kernel.read());\n      } else {\n        h = K.dot(inputs, this.kernel.read());\n      }\n      if (this.bias != null) {\n        h = K.biasAdd(h, this.bias.read());\n      }\n      if (recDpMask != null) {\n        prevOutput = tfc.mul(prevOutput, recDpMask);\n      }\n      let output = tfc.add(h, K.dot(prevOutput, this.recurrentKernel.read()));\n      if (this.activation != null) {\n        output = this.activation.apply(output);\n      }\n      // TODO(cais): Properly set learning phase on output tensor?\n      return [output, output];\n    });\n  }\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout\n    };\n    return Object.assign({}, baseConfig, config);\n  }\n}\n/** @nocollapse */\nSimpleRNNCell.className = 'SimpleRNNCell';\nserialization.registerClass(SimpleRNNCell);\nexport class SimpleRNN extends RNN {\n  constructor(args) {\n    args.cell = new SimpleRNNCell(args);\n    super(args);\n    // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState = kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {\n        mask,\n        training,\n        initialState\n      });\n    });\n  }\n  /** @nocollapse */\n  static fromConfig(cls, config) {\n    return new cls(config);\n  }\n}\n/** @nocollapse */\nSimpleRNN.className = 'SimpleRNN';\nserialization.registerClass(SimpleRNN);\nexport class GRUCell extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_ACTIVATION = 'tanh';\n    this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    if (args.resetAfter) {\n      throw new ValueError(`GRUCell does not support reset_after parameter set to true.`);\n    }\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION : args.activation);\n    this.recurrentActivation = getActivation(args.recurrentActivation === undefined ? this.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    this.implementation = args.implementation;\n    this.stateSize = this.units;\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight('kernel', [inputDim, this.units * 3], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 3], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n    if (this.useBias) {\n      this.bias = this.addWeight('bias', [this.units * 3], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n    // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n    //   of the weights and bias in the call() method, at execution time.\n    this.built = true;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n      if (inputs.length !== 2) {\n        throw new ValueError(`GRUCell expects 2 input Tensors (inputs, h, c), got ` + `${inputs.length}.`);\n      }\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      let hTMinus1 = inputs[1]; // Previous memory state.\n      inputs = inputs[0];\n      // Note: For superior performance, TensorFlow.js always uses\n      // implementation 2, regardless of the actual value of\n      // config.implementation.\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(inputs),\n          rate: this.dropout,\n          training,\n          count: 3\n        });\n      }\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(hTMinus1),\n          rate: this.recurrentDropout,\n          training,\n          count: 3\n        });\n      }\n      const dpMask = this.dropoutMask;\n      const recDpMask = this.recurrentDropoutMask;\n      let z;\n      let r;\n      let hh;\n      if (0 < this.dropout && this.dropout < 1) {\n        inputs = tfc.mul(inputs, dpMask[0]);\n      }\n      let matrixX = K.dot(inputs, this.kernel.read());\n      if (this.useBias) {\n        matrixX = K.biasAdd(matrixX, this.bias.read());\n      }\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n        hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n      }\n      const recurrentKernelValue = this.recurrentKernel.read();\n      const [rk1, rk2] = tfc.split(recurrentKernelValue, [2 * this.units, this.units], recurrentKernelValue.rank - 1);\n      const matrixInner = K.dot(hTMinus1, rk1);\n      const [xZ, xR, xH] = tfc.split(matrixX, 3, matrixX.rank - 1);\n      const [recurrentZ, recurrentR] = tfc.split(matrixInner, 2, matrixInner.rank - 1);\n      z = this.recurrentActivation.apply(tfc.add(xZ, recurrentZ));\n      r = this.recurrentActivation.apply(tfc.add(xR, recurrentR));\n      const recurrentH = K.dot(tfc.mul(r, hTMinus1), rk2);\n      hh = this.activation.apply(tfc.add(xH, recurrentH));\n      const h = tfc.add(tfc.mul(z, hTMinus1), tfc.mul(tfc.add(1, tfc.neg(z)), hh));\n      // TODO(cais): Add use_learning_phase flag properly.\n      return [h, h];\n    });\n  }\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation,\n      resetAfter: false\n    };\n    return Object.assign({}, baseConfig, config);\n  }\n}\n/** @nocollapse */\nGRUCell.className = 'GRUCell';\nserialization.registerClass(GRUCell);\nexport class GRU extends RNN {\n  constructor(args) {\n    if (args.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n    args.cell = new GRUCell(args);\n    super(args);\n    // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState = kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {\n        mask,\n        training,\n        initialState\n      });\n    });\n  }\n  /** @nocollapse */\n  static fromConfig(cls, config) {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n    return new cls(config);\n  }\n}\n/** @nocollapse */\nGRU.className = 'GRU';\nserialization.registerClass(GRU);\nexport class LSTMCell extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_ACTIVATION = 'tanh';\n    this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION : args.activation);\n    this.recurrentActivation = getActivation(args.recurrentActivation === undefined ? this.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.unitForgetBias = args.unitForgetBias;\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    this.implementation = args.implementation;\n    this.stateSize = [this.units, this.units];\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n  build(inputShape) {\n    var _a;\n    inputShape = getExactlyOneShape(inputShape);\n    const inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight('kernel', [inputDim, this.units * 4], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 4], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n    let biasInitializer;\n    if (this.useBias) {\n      if (this.unitForgetBias) {\n        const capturedBiasInit = this.biasInitializer;\n        const capturedUnits = this.units;\n        biasInitializer = new (_a = class CustomInit extends Initializer {\n          apply(shape, dtype) {\n            // TODO(cais): More informative variable names?\n            const bI = capturedBiasInit.apply([capturedUnits]);\n            const bF = new Ones().apply([capturedUnits]);\n            const bCAndH = capturedBiasInit.apply([capturedUnits * 2]);\n            return K.concatAlongFirstAxis(K.concatAlongFirstAxis(bI, bF), bCAndH);\n          }\n        }, /** @nocollapse */\n        _a.className = 'CustomInit', _a)();\n      } else {\n        biasInitializer = this.biasInitializer;\n      }\n      this.bias = this.addWeight('bias', [this.units * 4], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n    // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n    //   of the weights and bias in the call() method, at execution time.\n    this.built = true;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      inputs = inputs;\n      if (inputs.length !== 3) {\n        throw new ValueError(`LSTMCell expects 3 input Tensors (inputs, h, c), got ` + `${inputs.length}.`);\n      }\n      let hTMinus1 = inputs[1]; // Previous memory state.\n      const cTMinus1 = inputs[2]; // Previous carry state.\n      inputs = inputs[0];\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(inputs),\n          rate: this.dropout,\n          training,\n          count: 4\n        });\n      }\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(hTMinus1),\n          rate: this.recurrentDropout,\n          training,\n          count: 4\n        });\n      }\n      const dpMask = this.dropoutMask;\n      const recDpMask = this.recurrentDropoutMask;\n      // Note: For superior performance, TensorFlow.js always uses\n      // implementation 2 regardless of the actual value of\n      // config.implementation.\n      let i;\n      let f;\n      let c;\n      let o;\n      if (0 < this.dropout && this.dropout < 1) {\n        inputs = tfc.mul(inputs, dpMask[0]);\n      }\n      let z = K.dot(inputs, this.kernel.read());\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n        hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n      }\n      z = tfc.add(z, K.dot(hTMinus1, this.recurrentKernel.read()));\n      if (this.useBias) {\n        z = K.biasAdd(z, this.bias.read());\n      }\n      const [z0, z1, z2, z3] = tfc.split(z, 4, z.rank - 1);\n      i = this.recurrentActivation.apply(z0);\n      f = this.recurrentActivation.apply(z1);\n      c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, this.activation.apply(z2)));\n      o = this.recurrentActivation.apply(z3);\n      const h = tfc.mul(o, this.activation.apply(c));\n      // TODO(cais): Add use_learning_phase flag properly.\n      return [h, h, c];\n    });\n  }\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    };\n    return Object.assign({}, baseConfig, config);\n  }\n}\n/** @nocollapse */\nLSTMCell.className = 'LSTMCell';\nserialization.registerClass(LSTMCell);\nexport class LSTM extends RNN {\n  constructor(args) {\n    if (args.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n    args.cell = new LSTMCell(args);\n    super(args);\n    // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState = kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {\n        mask,\n        training,\n        initialState\n      });\n    });\n  }\n  /** @nocollapse */\n  static fromConfig(cls, config) {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n    return new cls(config);\n  }\n}\n/** @nocollapse */\nLSTM.className = 'LSTM';\nserialization.registerClass(LSTM);\nexport class StackedRNNCells extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.cells = args.cells;\n  }\n  get stateSize() {\n    // States are a flat list in reverse order of the cell stack.\n    // This allows perserving the requirement `stack.statesize[0] ===\n    // outputDim`. E.g., states of a 2-layer LSTM would be `[h2, c2, h1, c1]`,\n    // assuming one LSTM has states `[h, c]`.\n    const stateSize = [];\n    for (const cell of this.cells.slice().reverse()) {\n      if (Array.isArray(cell.stateSize)) {\n        stateSize.push(...cell.stateSize);\n      } else {\n        stateSize.push(cell.stateSize);\n      }\n    }\n    return stateSize;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n      let states = inputs.slice(1);\n      // Recover per-cell states.\n      const nestedStates = [];\n      for (const cell of this.cells.slice().reverse()) {\n        if (Array.isArray(cell.stateSize)) {\n          nestedStates.push(states.splice(0, cell.stateSize.length));\n        } else {\n          nestedStates.push(states.splice(0, 1));\n        }\n      }\n      nestedStates.reverse();\n      // Call the cells in order and store the returned states.\n      const newNestedStates = [];\n      let callInputs;\n      for (let i = 0; i < this.cells.length; ++i) {\n        const cell = this.cells[i];\n        states = nestedStates[i];\n        // TODO(cais): Take care of constants.\n        if (i === 0) {\n          callInputs = [inputs[0]].concat(states);\n        } else {\n          callInputs = [callInputs[0]].concat(states);\n        }\n        callInputs = cell.call(callInputs, kwargs);\n        newNestedStates.push(callInputs.slice(1));\n      }\n      // Format the new states as a flat list in reverse cell order.\n      states = [];\n      for (const cellStates of newNestedStates.slice().reverse()) {\n        states.push(...cellStates);\n      }\n      return [callInputs[0]].concat(states);\n    });\n  }\n  build(inputShape) {\n    if (isArrayOfShapes(inputShape)) {\n      // TODO(cais): Take care of input constants.\n      // const constantShape = inputShape.slice(1);\n      inputShape = inputShape[0];\n    }\n    inputShape = inputShape;\n    let outputDim;\n    this.cells.forEach((cell, i) => {\n      nameScope(`RNNCell_${i}`, () => {\n        // TODO(cais): Take care of input constants.\n        cell.build(inputShape);\n        if (Array.isArray(cell.stateSize)) {\n          outputDim = cell.stateSize[0];\n        } else {\n          outputDim = cell.stateSize;\n        }\n        inputShape = [inputShape[0], outputDim];\n      });\n    });\n    this.built = true;\n  }\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const getCellConfig = cell => {\n      return {\n        'className': cell.getClassName(),\n        'config': cell.getConfig()\n      };\n    };\n    const cellConfigs = this.cells.map(getCellConfig);\n    const config = {\n      'cells': cellConfigs\n    };\n    return Object.assign({}, baseConfig, config);\n  }\n  /** @nocollapse */\n  static fromConfig(cls, config) {\n    let customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    const cells = [];\n    for (const cellConfig of config['cells']) {\n      cells.push(deserialize(cellConfig, customObjects));\n    }\n    return new cls({\n      cells\n    });\n  }\n  get trainableWeights() {\n    if (!this.trainable) {\n      return [];\n    }\n    const weights = [];\n    for (const cell of this.cells) {\n      weights.push(...cell.trainableWeights);\n    }\n    return weights;\n  }\n  get nonTrainableWeights() {\n    const weights = [];\n    for (const cell of this.cells) {\n      weights.push(...cell.nonTrainableWeights);\n    }\n    if (!this.trainable) {\n      const trainableWeights = [];\n      for (const cell of this.cells) {\n        trainableWeights.push(...cell.trainableWeights);\n      }\n      return trainableWeights.concat(weights);\n    }\n    return weights;\n  }\n  /**\n   * Retrieve the weights of a the model.\n   *\n   * @returns A flat `Array` of `tf.Tensor`s.\n   */\n  getWeights() {\n    const weights = [];\n    for (const cell of this.cells) {\n      weights.push(...cell.weights);\n    }\n    return batchGetValue(weights);\n  }\n  /**\n   * Set the weights of the model.\n   *\n   * @param weights An `Array` of `tf.Tensor`s with shapes and types matching\n   *     the output of `getWeights()`.\n   */\n  setWeights(weights) {\n    const tuples = [];\n    for (const cell of this.cells) {\n      const numParams = cell.weights.length;\n      const inputWeights = weights.splice(numParams);\n      for (let i = 0; i < cell.weights.length; ++i) {\n        tuples.push([cell.weights[i], inputWeights[i]]);\n      }\n    }\n    batchSetValue(tuples);\n  }\n}\n/** @nocollapse */\nStackedRNNCells.className = 'StackedRNNCells';\nserialization.registerClass(StackedRNNCells);\nexport function generateDropoutMask(args) {\n  const {\n    ones,\n    rate,\n    training = false,\n    count = 1\n  } = args;\n  const droppedInputs = () => K.dropout(ones(), rate);\n  const createMask = () => K.inTrainPhase(droppedInputs, ones, training);\n  // just in case count is provided with null or undefined\n  if (!count || count <= 1) {\n    return tfc.keep(createMask().clone());\n  }\n  const masks = Array(count).fill(undefined).map(createMask);\n  return masks.map(m => tfc.keep(m.clone()));\n}","map":{"version":3,"sources":["../../src/layers/recurrent.ts"],"names":[],"mappings":"AAAA;;;;;;;;AAQG;AAEH;;AAEG;AAEH,OAAO,KAAK,GAAG,MAAM,uBAAuB;AAC5C,SAAkB,aAAa,EAAU,IAAI,EAAE,IAAI,QAAO,uBAAuB;AAEjF,SAAoB,aAAa,EAAE,mBAAmB,QAAO,gBAAgB;AAC7E,OAAO,KAAK,CAAC,MAAM,yBAAyB;AAC5C,SAAQ,SAAS,QAAO,WAAW;AACnC,SAA0C,aAAa,EAAE,mBAAmB,QAAO,gBAAgB;AACnG,SAAQ,SAAS,EAAE,cAAc,QAAO,oBAAoB;AAC5D,SAAQ,KAAK,QAAkB,oBAAoB;AACnD,SAAQ,cAAc,EAAE,mBAAmB,EAAE,UAAU,QAAO,WAAW;AACzE,SAAQ,cAAc,EAAE,WAAW,EAAyB,IAAI,EAAE,oBAAoB,QAAO,iBAAiB;AAG9G,SAAQ,cAAc,EAAsC,oBAAoB,QAAO,iBAAiB;AAExG,SAAQ,qBAAqB,QAAO,wBAAwB;AAC5D,OAAO,KAAK,UAAU,MAAM,qBAAqB;AACjD,SAAQ,kBAAkB,EAAE,mBAAmB,EAAE,eAAe,QAAO,sBAAsB;AAC7F,SAAQ,aAAa,EAAE,aAAa,QAAsB,cAAc;AACxE,SAAQ,WAAW,QAAO,iBAAiB;AAE3C;;;;;;;;;;;;;;;;;;;;;AAqBG;AACH,OAAM,SAAU,eAAe,CAC3B,MAAuD,EACvD,YAA6D,EAC7D,SAA0D,EAC1D,YAAqB,EAAA;EAKvB,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;IACzB,IAAI,YAAY,IAAI,IAAI,IAAI,SAAS,IAAI,IAAI,EAAE;MAC7C,MAAM,IAAI,UAAU,CAChB,6DAA6D,GAC7D,oBAAoB,CAAC;IAC1B;IACD,IAAI,YAAY,IAAI,IAAI,EAAE;MACxB,SAAS,GAAG,MAAM,CAAC,KAAK,CAAC,MAAM,CAAC,MAAM,GAAG,YAAY,EAAE,MAAM,CAAC,MAAM,CAAC;MACrE,MAAM,GAAG,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,MAAM,CAAC,MAAM,GAAG,YAAY,CAAC;IACvD;IACD,IAAI,MAAM,CAAC,MAAM,GAAG,CAAC,EAAE;MACrB,YAAY,GAAG,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,MAAM,CAAC,MAAM,CAAC;IAC9C;IACD,MAAM,GAAG,MAAM,CAAC,CAAC,CAAC;EACnB;EAED,SAAS,YAAY,CAAC,CACgB,EAAA;IACpC,IAAI,CAAC,IAAI,IAAI,IAAI,KAAK,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE;MACjC,OAAO,CAAgC;KACxC,MAAM;MACL,OAAO,CAAC,CAAC,CAAgC;IAC1C;EACH;EAEA,YAAY,GAAG,YAAY,CAAC,YAAY,CAAC;EACzC,SAAS,GAAG,YAAY,CAAC,SAAS,CAAC;EAEnC,OAAO;IAAC,MAAM;IAAE,YAAY;IAAE;EAAS,CAAC;AAC1C;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA0CG;AACH,OAAM,SAAU,GAAG,CACf,YAA6B,EAAE,MAAc,EAAE,aAAuB,EAE5C;EAAA,IAD1B,WAAW,uEAAG,KAAK;EAAA,IAAE,IAAa;EAAA,IAAE,SAAoB;EAAA,IAAE,MAAM,uEAAG,KAAK;EAAA,IACxE,kBAAkB,uEAAG,KAAK;EAC5B,OAAO,GAAG,CAAC,IAAI,CAAC,MAAK;IACnB,MAAM,IAAI,GAAG,MAAM,CAAC,KAAK,CAAC,MAAM;IAChC,IAAI,IAAI,GAAG,CAAC,EAAE;MACZ,MAAM,IAAI,UAAU,CAAC,uCAAuC,IAAI,IAAI,CAAC;IACtE;IAED;IACA;IACA,MAAM,IAAI,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;IACrD,MAAM,GAAG,GAAG,CAAC,SAAS,CAAC,MAAM,EAAE,IAAI,CAAC;IAEpC,IAAI,SAAS,IAAI,IAAI,EAAE;MACrB,MAAM,IAAI,mBAAmB,CACzB,kEAAkE,GAClE,gBAAgB,CAAC;IACtB;IAED;IACA,IAAI,MAAM,EAAE;MACV,OAAO,CAAC,IAAI,CACR,mEAAmE,GACnE,kCAAkC,CAAC;IACxC;IAED,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,IAAI,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC,MAAM,CAAC,SAAS,CAAC;MAC5C,IAAI,IAAI,CAAC,IAAI,KAAK,IAAI,GAAG,CAAC,EAAE;QAC1B,IAAI,GAAG,GAAG,CAAC,UAAU,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC;MAChC;MACD,IAAI,GAAG,GAAG,CAAC,SAAS,CAAC,IAAI,EAAE,IAAI,CAAC;IACjC;IAED,IAAI,WAAW,EAAE;MACf,MAAM,GAAG,GAAG,CAAC,OAAO,CAAC,MAAM,EAAE,CAAC,CAAC;MAC/B,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,IAAI,GAAG,GAAG,CAAC,OAAO,CAAC,IAAI,EAAE,CAAC,CAAC;MAC5B;IACF;IAED;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IAEA,MAAM,cAAc,GAAa,EAAE;IACnC,IAAI,UAAkB;IACtB,IAAI,MAAM,GAAG,aAAa;IAC1B,MAAM,SAAS,GAAG,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC;IACjC,MAAM,aAAa,GAAG,GAAG,CAAC,OAAO,CAAC,MAAM,CAAC;IACzC,IAAI,YAAsB;IAC1B,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,YAAY,GAAG,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC;IACjC;IAED,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,EAAE,EAAE,CAAC,EAAE;MAClC,MAAM,YAAY,GAAG,aAAa,CAAC,CAAC,CAAC;MACrC,MAAM,WAAW,GAAG,GAAG,CAAC,IAAI,CAAC,MAAM,YAAY,CAAC,YAAY,EAAE,MAAM,CAAC,CAAC;MAEtE,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,UAAU,GAAG,WAAW,CAAC,CAAC,CAAC;QAC3B,MAAM,GAAG,WAAW,CAAC,CAAC,CAAC;OACxB,MAAM;QACL,MAAM,aAAa,GAAG,GAAG,CAAC,IAAI,CAAC,MAAK;UAClC,MAAM,QAAQ,GAAG,YAAY,CAAC,CAAC,CAAC;UAChC,MAAM,WAAW,GAAG,GAAG,CAAC,QAAQ,CAAC,QAAQ,CAAC,CAAC,GAAG,CAAC,QAAQ,CAAC;UACxD;UACA,MAAM,MAAM,GACR,WAAW,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,QAAQ,CAAC,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,WAAW,CAAC,CAAC;UAChE,MAAM,SAAS,GAAG,MAAM,CAAC,GAAG,CAAC,CAAC,KAAK,EAAE,CAAC,KAAI;YACxC,OAAO,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,QAAQ,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,GAAG,CAAC,WAAW,CAAC,CAAC;UACpE,CAAC,CAAC;UACF,OAAO;YAAC,MAAM;YAAE;UAAS,CAAC;QAC5B,CAAC,CAAC;QACF,UAAU,GAAG,aAAa,CAAC,MAAM;QACjC,MAAM,GAAG,aAAa,CAAC,SAAS;MACjC;MAED,IAAI,kBAAkB,EAAE;QACtB,cAAc,CAAC,IAAI,CAAC,UAAU,CAAC;MAChC;IACF;IACD,IAAI,OAAe;IACnB,IAAI,kBAAkB,EAAE;MACtB,MAAM,IAAI,GAAG,CAAC;MACd,OAAO,GAAG,GAAG,CAAC,KAAK,CAAC,cAAc,EAAE,IAAI,CAAC;IAC1C;IACD,OAAO,CAAC,UAAU,EAAE,OAAO,EAAE,MAAM,CAA+B;EACpE,CAAC,CAAC;AACJ;AAuGA,OAAM,MAAO,GAAI,SAAQ,KAAK,CAAA;EAqB5B,WAAA,CAAY,IAAkB,EAAA;IAC5B,KAAK,CAAC,IAAI,CAAC;IACX,IAAI,IAAa;IACjB,IAAI,IAAI,CAAC,IAAI,IAAI,IAAI,EAAE;MACrB,MAAM,IAAI,UAAU,CAChB,sDAAsD,CAAC;KAC5D,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE;MACnC,IAAI,GAAG,IAAI,eAAe,CAAC;QAAC,KAAK,EAAE,IAAI,CAAC;MAAI,CAAC,CAAC;KAC/C,MAAM;MACL,IAAI,GAAG,IAAI,CAAC,IAAI;IACjB;IACD,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;MAC1B,MAAM,IAAI,UAAU,CAChB,8DAA8D,GAC9D,uCAAuC,CAAC;IAC7C;IACD,IAAI,CAAC,IAAI,GAAG,IAAI;IAChB,IAAI,CAAC,eAAe,GAChB,IAAI,CAAC,eAAe,IAAI,IAAI,GAAG,KAAK,GAAG,IAAI,CAAC,eAAe;IAC/D,IAAI,CAAC,WAAW,GAAG,IAAI,CAAC,WAAW,IAAI,IAAI,GAAG,KAAK,GAAG,IAAI,CAAC,WAAW;IACtE,IAAI,CAAC,WAAW,GAAG,IAAI,CAAC,WAAW,IAAI,IAAI,GAAG,KAAK,GAAG,IAAI,CAAC,WAAW;IACtE,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC,QAAQ,IAAI,IAAI,GAAG,KAAK,GAAG,IAAI,CAAC,QAAQ;IAC9D,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,IAAI,IAAI,GAAG,KAAK,GAAG,IAAI,CAAC,MAAM;IAEvD,IAAI,CAAC,eAAe,GAAG,IAAI;IAC3B,IAAI,CAAC,SAAS,GAAG,CAAC,IAAI,SAAS,CAAC;MAAC,IAAI,EAAE;IAAC,CAAC,CAAC,CAAC;IAC3C,IAAI,CAAC,SAAS,GAAG,IAAI;IACrB,IAAI,CAAC,OAAO,GAAG,IAAI;IACnB;IACA,IAAI,CAAC,YAAY,GAAG,IAAI;IACxB;IACA;IAEA,IAAI,CAAC,UAAU,GAAG,EAAE;EACtB;EAEA;EACA;EACA,SAAS,GAAA;IACP,IAAI,IAAI,CAAC,OAAO,IAAI,IAAI,EAAE;MACxB,MAAM,SAAS,GACX,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,GAAG,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,MAAM,GAAG,CAAC;MACvE,OAAO,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,SAAS,CAAC,CAAC,GAAG,CAAC,CAAC,IAAI,IAAI,CAAC;KACrD,MAAM;MACL,OAAO,IAAI,CAAC,OAAO;IACpB;EACH;EAEA;EACA;EACA,SAAS,CAAC,MAAgB,EAAA;IACxB,IAAI,CAAC,OAAO,GAAG,MAAM;EACvB;EAEA,kBAAkB,CAAC,UAAyB,EAAA;IAC1C,IAAI,eAAe,CAAC,UAAU,CAAC,EAAE;MAC/B,UAAU,GAAI,UAAsB,CAAC,CAAC,CAAC;IACxC;IACD,UAAU,GAAG,UAAmB;IAEhC;IACA,IAAI,SAAS,GAAG,IAAI,CAAC,IAAI,CAAC,SAAS;IACnC,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,SAAS,CAAC,EAAE;MAC7B,SAAS,GAAG,CAAC,SAAS,CAAC;IACxB;IACD,MAAM,SAAS,GAAG,SAAS,CAAC,CAAC,CAAC;IAC9B,IAAI,WAA0B;IAC9B,IAAI,IAAI,CAAC,eAAe,EAAE;MACxB,WAAW,GAAG,CAAC,UAAU,CAAC,CAAC,CAAC,EAAE,UAAU,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC;KACxD,MAAM;MACL,WAAW,GAAG,CAAC,UAAU,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC;IACzC;IAED,IAAI,IAAI,CAAC,WAAW,EAAE;MACpB,MAAM,UAAU,GAAY,EAAE;MAC9B,KAAK,MAAM,GAAG,IAAI,SAAS,EAAE;QAC3B,UAAU,CAAC,IAAI,CAAC,CAAC,UAAU,CAAC,CAAC,CAAC,EAAE,GAAG,CAAC,CAAC;MACtC;MACD,OAAO,CAAC,WAAW,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC;KACxC,MAAM;MACL,OAAO,WAAW;IACnB;EACH;EAEA,WAAW,CAAC,MAAuB,EAAE,IAAsB,EAAA;IAEzD,OAAO,GAAG,CAAC,IAAI,CAAC,MAAK;MACnB,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE;QACvB,IAAI,GAAG,IAAI,CAAC,CAAC,CAAC;MACf;MACD,MAAM,UAAU,GAAG,IAAI,CAAC,eAAe,GAAG,IAAI,GAAG,IAAI;MAErD,IAAI,IAAI,CAAC,WAAW,EAAE;QACpB,MAAM,SAAS,GAAG,IAAI,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,IAAI,IAAI,CAAC;QAC5C,OAAO,CAAC,UAAU,CAAC,CAAC,MAAM,CAAC,SAAS,CAAC;OACtC,MAAM;QACL,OAAO,UAAU;MAClB;IACH,CAAC,CAAC;EACJ;EAEA;;;;;AAKG;EACH,IAAI,MAAM,GAAA;IACR,IAAI,IAAI,CAAC,OAAO,IAAI,IAAI,EAAE;MACxB,MAAM,SAAS,GACX,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,GAAG,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,MAAM,GAAG,CAAC;MACvE,MAAM,MAAM,GAAa,EAAE;MAC3B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,EAAE,EAAE,CAAC,EAAE;QAClC,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC;MAClB;MACD,OAAO,MAAM;KACd,MAAM;MACL,OAAO,IAAI,CAAC,OAAO;IACpB;EACH;EAEA,IAAI,MAAM,CAAC,CAAW,EAAA;IACpB,IAAI,CAAC,OAAO,GAAG,CAAC;EAClB;EAEO,KAAK,CAAC,UAAyB,EAAA;IACpC;IACA;IACA,MAAM,aAAa,GAAY,IAAI;IACnC,IAAI,IAAI,CAAC,YAAY,IAAI,IAAI,EAAE;MAC7B,MAAM,IAAI,mBAAmB,CACzB,kDAAkD,CAAC;IACxD;IAED,IAAI,eAAe,CAAC,UAAU,CAAC,EAAE;MAC/B,UAAU,GAAI,UAAsB,CAAC,CAAC,CAAC;IACxC;IACD,UAAU,GAAG,UAAmB;IAEhC,MAAM,SAAS,GAAW,IAAI,CAAC,QAAQ,GAAG,UAAU,CAAC,CAAC,CAAC,GAAG,IAAI;IAC9D,MAAM,QAAQ,GAAG,UAAU,CAAC,KAAK,CAAC,CAAC,CAAC;IACpC,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC,GAAG,IAAI,SAAS,CAAC;MAAC,KAAK,EAAE,CAAC,SAAS,EAAE,IAAI,EAAE,GAAG,QAAQ;IAAC,CAAC,CAAC;IAE1E;IACA;IACA,MAAM,cAAc,GAAG,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;IAClE,IAAI,aAAa,IAAI,IAAI,EAAE;MACzB,MAAM,IAAI,mBAAmB,CACzB,kDAAkD,CAAC;KACxD,MAAM;MACL,IAAI,CAAC,IAAI,CAAC,KAAK,CAAC,cAAc,CAAC;IAChC;IAED;IACA,IAAI,SAAmB;IACvB,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,EAAE;MACtC,SAAS,GAAG,IAAI,CAAC,IAAI,CAAC,SAAS;KAChC,MAAM;MACL,SAAS,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC;IAClC;IAED,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;MAC1B,IAAI,CAAC,IAAI,CAAC,WAAW,CACb,IAAI,CAAC,SAAS,CAAC,GAAG,CAAC,IAAI,IAAI,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,EAC7D,SAAS,CAAC,EAAE;QAClB,MAAM,IAAI,UAAU,CAChB,yDAAyD,GACzD,sCAAsC,IAAI,CAAC,SAAS,IAAI,GACxD,6BAA6B,IAAI,CAAC,IAAI,CAAC,SAAS,EAAE,CAAC;MACxD;KACF,MAAM;MACL,IAAI,CAAC,SAAS,GACV,SAAS,CAAC,GAAG,CAAC,GAAG,IAAI,IAAI,SAAS,CAAC;QAAC,KAAK,EAAE,CAAC,IAAI,EAAE,GAAG;MAAC,CAAC,CAAC,CAAC;IAC9D;IACD,IAAI,IAAI,CAAC,QAAQ,EAAE;MACjB,IAAI,CAAC,WAAW,EAAE;IACnB;EACH;EAEA;;;;;;;;;;;;;;;;AAgBG;EACH,WAAW,CAAC,MAAwB,EAAkB;IAAA,IAAhB,QAAQ,uEAAG,KAAK;IACpD,IAAI,CAAC,MAAK;MACR,IAAI,CAAC,IAAI,CAAC,QAAQ,EAAE;QAClB,MAAM,IAAI,cAAc,CACpB,iEAAiE,CAAC;MACvE;MACD,MAAM,SAAS,GAAG,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC;MAC5C,IAAI,SAAS,IAAI,IAAI,EAAE;QACrB,MAAM,IAAI,UAAU,CAChB,kEAAkE,GAClE,0CAA0C,GAC1C,2DAA2D,GAC3D,2DAA2D,GAC3D,2DAA2D,GAC3D,oDAAoD,CAAC;MAC1D;MACD;MACA,IAAI,IAAI,CAAC,OAAO,IAAI,IAAI,EAAE;QACxB,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,EAAE;UACtC,IAAI,CAAC,OAAO,GACR,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,GAAG,CAAC,GAAG,IAAI,GAAG,CAAC,KAAK,CAAC,CAAC,SAAS,EAAE,GAAG,CAAC,CAAC,CAAC;SAChE,MAAM;UACL,IAAI,CAAC,OAAO,GAAG,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC,SAAS,EAAE,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC;QAC7D;OACF,MAAM,IAAI,MAAM,IAAI,IAAI,EAAE;QACzB;QACA,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,OAAO,CAAC;QACzB;QACA,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;UAC3B,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,UAAU,CAAC;UAC5B,IAAI,CAAC,UAAU,GAAG,EAAE;QACrB;QAED,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,EAAE;UACtC,IAAI,CAAC,OAAO,GACR,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,GAAG,CAAC,GAAG,IAAI,GAAG,CAAC,KAAK,CAAC,CAAC,SAAS,EAAE,GAAG,CAAC,CAAC,CAAC;SAChE,MAAM;UACL,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,GAAG,GAAG,CAAC,KAAK,CAAC,CAAC,SAAS,EAAE,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;QAC9D;OACF,MAAM;QACL,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;UAC1B,MAAM,GAAG,CAAC,MAAM,CAAC;QAClB;QACD,IAAI,MAAM,CAAC,MAAM,KAAK,IAAI,CAAC,OAAO,CAAC,MAAM,EAAE;UACzC,MAAM,IAAI,UAAU,CAChB,SAAS,IAAI,CAAC,IAAI,YAAY,IAAI,CAAC,OAAO,CAAC,MAAM,aAAa,GAC9D,mBAAmB,MAAM,CAAC,MAAM,yBAAyB,GACzD,aAAa,MAAM,EAAE,CAAC;QAC3B;QAED,IAAI,QAAQ,KAAK,IAAI,EAAE;UACrB;UACA;UACA;UACA;UACA,IAAI,CAAC,UAAU,CAAC,IAAI,CAAC,IAAI,CAAC,OAAO,CAAC,KAAK,EAAE,CAAC;SAC3C,MAAM;UACL,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,OAAO,CAAC;QAC1B;QAED,KAAK,IAAI,KAAK,GAAG,CAAC,EAAE,KAAK,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,EAAE,EAAE,KAAK,EAAE;UACxD,MAAM,KAAK,GAAG,MAAM,CAAC,KAAK,CAAC;UAC3B,MAAM,GAAG,GAAG,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,GAC1C,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,KAAK,CAAC,GAC1B,IAAI,CAAC,IAAI,CAAC,SAAS;UACvB,MAAM,aAAa,GAAG,CAAC,SAAS,EAAE,GAAG,CAAC;UACtC,IAAI,CAAC,IAAI,CAAC,WAAW,CAAC,KAAK,CAAC,KAAK,EAAE,aAAa,CAAC,EAAE;YACjD,MAAM,IAAI,UAAU,CAChB,SAAS,KAAK,+BAA+B,IAAI,CAAC,IAAI,IAAI,GAC1D,kBAAkB,aAAa,oBAC3B,KAAK,CAAC,KAAK,EAAE,CAAC;UACvB;UACD,IAAI,CAAC,OAAO,CAAC,KAAK,CAAC,GAAG,KAAK;QAC5B;MACF;MACD,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,KAAK,IAAI,GAAG,CAAC,IAAI,CAAC,KAAK,CAAC,KAAK,EAAE,CAAC,CAAC;IACnE,CAAC,CAAC;EACJ;EAEA,KAAK,CACD,MAAuD,EACvD,MAAe,EAAA;IACjB;IACA,IAAI,YAAY,GACZ,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,cAAc,CAAC;IAClD,IAAI,SAAS,GACT,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,WAAW,CAAC;IAC/C,IAAI,MAAM,IAAI,IAAI,EAAE;MAClB,MAAM,GAAG,CAAA,CAAE;IACZ;IAED,MAAM,YAAY,GACd,eAAe,CAAC,MAAM,EAAE,YAAY,EAAE,SAAS,EAAE,IAAI,CAAC,YAAY,CAAC;IACvE,MAAM,GAAG,YAAY,CAAC,MAAM;IAC5B,YAAY,GAAG,YAAY,CAAC,YAAY;IACxC,SAAS,GAAG,YAAY,CAAC,SAAS;IAElC;IACA;IACA;IAEA,IAAI,gBAAgB,GAAiC,EAAE;IACvD,IAAI,eAAe,GAAgB,EAAE;IACrC,IAAI,YAAY,IAAI,IAAI,EAAE;MACxB,MAAM,CAAC,cAAc,CAAC,GAAG,YAAY;MACrC,gBAAgB,GAAG,gBAAgB,CAAC,MAAM,CAAC,YAAY,CAAC;MACxD,IAAI,CAAC,SAAS,GAAG,EAAE;MACnB,KAAK,MAAM,KAAK,IAAI,YAAY,EAAE;QAChC,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,IAAI,SAAS,CAAC;UAAC,KAAK,EAAE,KAAK,CAAC;QAAK,CAAC,CAAC,CAAC;MACzD;MACD;MACA;MACA;MACA,eAAe,GAAG,eAAe,CAAC,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC;IACzD;IACD,IAAI,SAAS,IAAI,IAAI,EAAE;MACrB,MAAM,CAAC,WAAW,CAAC,GAAG,SAAS;MAC/B,gBAAgB,GAAG,gBAAgB,CAAC,MAAM,CAAC,SAAS,CAAC;MACrD;MACA,IAAI,CAAC,YAAY,GAAG,SAAS,CAAC,MAAM;IACrC;IAED,MAAM,QAAQ,GAAG,gBAAgB,CAAC,CAAC,CAAC,YAAY,cAAc;IAC9D,IAAI,QAAQ,EAAE;MACZ;MACA,MAAM,SAAS,GACX,CAAC,MAAM,CAAC,CAAC,MAAM,CAAC,gBAAgB,CAAgC;MACpE,MAAM,aAAa,GAAG,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,eAAe,CAAC;MAC5D;MACA,MAAM,iBAAiB,GAAG,IAAI,CAAC,SAAS;MACxC,IAAI,CAAC,SAAS,GAAG,aAAa;MAC9B,MAAM,MAAM,GAAG,KAAK,CAAC,KAAK,CAAC,SAAS,EAAE,MAAM,CAAC;MAC7C,IAAI,CAAC,SAAS,GAAG,iBAAiB;MAClC,OAAO,MAAM;KACd,MAAM;MACL,OAAO,KAAK,CAAC,KAAK,CAAC,MAAM,EAAE,MAAM,CAAC;IACnC;EACH;EAEA;EACA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C;IACA;IACA;IACA,OAAO,IAAI,CAAC,MAAK;MACf,MAAM,IAAI,GAAG,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,MAAM,CAAW;MAC7D,MAAM,QAAQ,GAAG,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,UAAU,CAAC;MAC3D,IAAI,YAAY,GACZ,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,cAAc,CAAC;MAElD,MAAM,GAAG,mBAAmB,CAAC,MAAM,CAAC;MACpC,IAAI,YAAY,IAAI,IAAI,EAAE;QACxB,IAAI,IAAI,CAAC,QAAQ,EAAE;UACjB,YAAY,GAAG,IAAI,CAAC,OAAO;SAC5B,MAAM;UACL,YAAY,GAAG,IAAI,CAAC,eAAe,CAAC,MAAM,CAAC;QAC5C;MACF;MAED,MAAM,SAAS,GACX,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,GAAG,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,MAAM,GAAG,CAAC;MACvE,IAAI,YAAY,CAAC,MAAM,KAAK,SAAS,EAAE;QACrC,MAAM,IAAI,UAAU,CAChB,iBAAiB,SAAS,2BAA2B,GACrD,GAAG,YAAY,CAAC,MAAM,oBAAoB,CAAC;MAChD;MACD,IAAI,IAAI,CAAC,MAAM,EAAE;QACf,OAAO,CAAC,IAAI,CACR,kEAAkE,CAAC;MACxE;MAED,MAAM,cAAc,GAAW;QAAC;MAAQ,CAAC;MAEzC;MACA,MAAM,IAAI,GAAG,CAAC,MAAc,EAAE,MAAgB,KAAI;QAChD;QACA;QACA,MAAM,OAAO,GACT,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,MAAM,CAAC,CAAC,MAAM,CAAC,MAAM,CAAC,EAAE,cAAc,CAAa;QACvE;QACA,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,CAAuB;MAC7D,CAAC;MAED;MAEA,MAAM,UAAU,GACZ,GAAG,CAAC,IAAI,EAAE,MAAM,EAAE,YAAY,EAAE,IAAI,CAAC,WAAW,EAAE,IAAI,EAAE,IAAI,EACxD,IAAI,CAAC,MAAM,EAAE,IAAI,CAAC,eAAe,CAAC;MAC1C,MAAM,UAAU,GAAG,UAAU,CAAC,CAAC,CAAC;MAChC,MAAM,OAAO,GAAG,UAAU,CAAC,CAAC,CAAC;MAC7B,MAAM,MAAM,GAAG,UAAU,CAAC,CAAC,CAAC;MAE5B,IAAI,IAAI,CAAC,QAAQ,EAAE;QACjB,IAAI,CAAC,WAAW,CAAC,MAAM,EAAE,QAAQ,CAAC;MACnC;MAED,MAAM,MAAM,GAAG,IAAI,CAAC,eAAe,GAAG,OAAO,GAAG,UAAU;MAE1D;MAEA,IAAI,IAAI,CAAC,WAAW,EAAE;QACpB,OAAO,CAAC,MAAM,CAAC,CAAC,MAAM,CAAC,MAAM,CAAC;OAC/B,MAAM;QACL,OAAO,MAAM;MACd;IACH,CAAC,CAAC;EACJ;EAEA,eAAe,CAAC,MAAc,EAAA;IAC5B,OAAO,IAAI,CAAC,MAAK;MACf;MACA;MACA,IAAI,YAAY,GAAG,GAAG,CAAC,KAAK,CAAC,MAAM,CAAC,KAAK,CAAC;MAC1C;MACA,YAAY,GAAG,GAAG,CAAC,GAAG,CAAC,YAAY,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;MAC5C,YAAY,GAAG,CAAC,CAAC,UAAU,CAAC,YAAY,CAAC,CAAC,CAAE;MAE5C,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,EAAE;QACtC,OAAO,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,GAAG,CAC1B,GAAG,IAAI,GAAG,GAAG,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC,YAAY,EAAE,CAAC,CAAC,EAAE,GAAG,CAAC,CAAC,GAAG,YAAY,CAAC;OACpE,MAAM;QACL,OAAO,IAAI,CAAC,IAAI,CAAC,SAAS,GAAG,CAAC,GAC1B,CAAC,CAAC,CAAC,IAAI,CAAC,YAAY,EAAE,CAAC,CAAC,EAAE,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC,GAChD,CAAC,YAAY,CAAC;MACnB;IACH,CAAC,CAAC;EACJ;EAEA,IAAI,gBAAgB,GAAA;IAClB,IAAI,CAAC,IAAI,CAAC,SAAS,EAAE;MACnB,OAAO,EAAE;IACV;IACD;IACA,OAAO,IAAI,CAAC,IAAI,CAAC,gBAAgB;EACnC;EAEA,IAAI,mBAAmB,GAAA;IACrB;IACA,IAAI,CAAC,IAAI,CAAC,SAAS,EAAE;MACnB,OAAO,IAAI,CAAC,IAAI,CAAC,OAAO;IACzB;IACD,OAAO,IAAI,CAAC,IAAI,CAAC,mBAAmB;EACtC;EAEA,4BAA4B,CAAC,KAAc,EAAA;IACzC,KAAK,CAAC,4BAA4B,CAAC,KAAK,CAAC;IACzC,IAAI,IAAI,CAAC,IAAI,IAAI,IAAI,EAAE;MACrB,IAAI,CAAC,IAAI,CAAC,4BAA4B,CAAC,KAAK,CAAC;IAC9C;EACH;EAEA,SAAS,GAAA;IACP,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IAEpC,MAAM,MAAM,GAA6B;MACvC,eAAe,EAAE,IAAI,CAAC,eAAe;MACrC,WAAW,EAAE,IAAI,CAAC,WAAW;MAC7B,WAAW,EAAE,IAAI,CAAC,WAAW;MAC7B,QAAQ,EAAE,IAAI,CAAC,QAAQ;MACvB,MAAM,EAAE,IAAI,CAAC;KACd;IAED,IAAI,IAAI,CAAC,YAAY,IAAI,IAAI,EAAE;MAC7B,MAAM,CAAC,cAAc,CAAC,GAAG,IAAI,CAAC,YAAY;IAC3C;IAED,MAAM,UAAU,GAAG,IAAI,CAAC,IAAI,CAAC,SAAS,EAAE;IAExC,IAAI,IAAI,CAAC,YAAY,EAAE,KAAK,GAAG,CAAC,SAAS,EAAE;MACzC,MAAM,CAAC,MAAM,CAAC,GAAG;QACf,WAAW,EAAE,IAAI,CAAC,IAAI,CAAC,YAAY,EAAE;QACrC,QAAQ,EAAE;OACsB;IACnC;IAED;IACA,OAAA,MAAA,CAAA,MAAA,CAAA,CAAA,CAAA,EAAW,UAAU,EAAK,UAAU,EAAK,MAAM,CAAA;EACjD;EAEA;EACA,OAAO,UAAU,CACb,GAA6C,EAC7C,MAAgC,EACc;IAAA,IAA9C,aAAA,uEAAgB,CAAA,CAA8B;IAChD,MAAM,UAAU,GAAG,MAAM,CAAC,MAAM,CAA6B;IAC7D,MAAM,IAAI,GAAG,WAAW,CAAC,UAAU,EAAE,aAAa,CAAY;IAC9D,OAAO,IAAI,GAAG,CAAC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE;MAAC;IAAI,CAAC,CAAC,CAAC;EAC/C;;AAvfA;AACO,GAAA,CAAA,SAAS,GAAG,KAAK;AAwf1B,aAAa,CAAC,aAAa,CAAC,GAAG,CAAC;AAEhC;AACA;AACA;AACA;;;;AAIG;AACH,OAAM,MAAgB,OAAQ,SAAQ,KAAK,CAAA;AA0F3C,OAAM,MAAO,aAAc,SAAQ,OAAO,CAAA;EAiCxC,WAAA,CAAY,IAA4B,EAAA;IACtC,KAAK,CAAC,IAAI,CAAC;IANJ,IAAA,CAAA,kBAAkB,GAAG,MAAM;IAC3B,IAAA,CAAA,0BAA0B,GAAG,cAAc;IAC3C,IAAA,CAAA,6BAA6B,GAAG,YAAY;IAC5C,IAAA,CAAA,wBAAwB,GAA0B,OAAO;IAIhE,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK;IACvB,qBAAqB,CAAC,IAAI,CAAC,KAAK,EAAE,OAAO,CAAC;IAC1C,IAAI,CAAC,UAAU,GAAG,aAAa,CAC3B,IAAI,CAAC,UAAU,IAAI,IAAI,GAAG,IAAI,CAAC,kBAAkB,GAAG,IAAI,CAAC,UAAU,CAAC;IACxE,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,OAAO;IAEzD,IAAI,CAAC,iBAAiB,GAAG,cAAc,CACnC,IAAI,CAAC,iBAAiB,IAAI,IAAI,CAAC,0BAA0B,CAAC;IAC9D,IAAI,CAAC,oBAAoB,GAAG,cAAc,CACtC,IAAI,CAAC,oBAAoB,IAAI,IAAI,CAAC,6BAA6B,CAAC;IAEpE,IAAI,CAAC,eAAe,GAChB,cAAc,CAAC,IAAI,CAAC,eAAe,IAAI,IAAI,CAAC,wBAAwB,CAAC;IAEzE,IAAI,CAAC,iBAAiB,GAAG,cAAc,CAAC,IAAI,CAAC,iBAAiB,CAAC;IAC/D,IAAI,CAAC,oBAAoB,GAAG,cAAc,CAAC,IAAI,CAAC,oBAAoB,CAAC;IACrE,IAAI,CAAC,eAAe,GAAG,cAAc,CAAC,IAAI,CAAC,eAAe,CAAC;IAE3D,IAAI,CAAC,gBAAgB,GAAG,aAAa,CAAC,IAAI,CAAC,gBAAgB,CAAC;IAC5D,IAAI,CAAC,mBAAmB,GAAG,aAAa,CAAC,IAAI,CAAC,mBAAmB,CAAC;IAClE,IAAI,CAAC,cAAc,GAAG,aAAa,CAAC,IAAI,CAAC,cAAc,CAAC;IAExD,IAAI,CAAC,OAAO,GAAG,UAAU,CAAC,GAAG,CACzB,CAAC,CAAC,EAAE,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,OAAO,IAAI,IAAI,GAAG,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IACtE,IAAI,CAAC,gBAAgB,GAAG,UAAU,CAAC,GAAG,CAAC,CACrC,CAAC,EACD,UAAU,CAAC,GAAG,CACV,CAAC,CAAC,EAAE,IAAI,CAAC,gBAAgB,IAAI,IAAI,GAAG,CAAC,GAAG,IAAI,CAAC,gBAAgB,CAAC,CAAC,CACpE,CAAC;IACF,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC,KAAK;IAC3B,IAAI,CAAC,WAAW,GAAG,IAAI;IACvB,IAAI,CAAC,oBAAoB,GAAG,IAAI;EAClC;EAEA,KAAK,CAAC,UAAyB,EAAA;IAC7B,UAAU,GAAG,kBAAkB,CAAC,UAAU,CAAC;IAC3C;IACA,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,SAAS,CACxB,QAAQ,EAAE,CAAC,UAAU,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,KAAK,CAAC,EAAE,IAAI,EAC/D,IAAI,CAAC,iBAAiB,EAAE,IAAI,CAAC,iBAAiB,EAAE,IAAI,EACpD,IAAI,CAAC,gBAAgB,CAAC;IAC1B,IAAI,CAAC,eAAe,GAAG,IAAI,CAAC,SAAS,CACjC,kBAAkB,EAAE,CAAC,IAAI,CAAC,KAAK,EAAE,IAAI,CAAC,KAAK,CAAC,EAAE,IAAI,EAClD,IAAI,CAAC,oBAAoB,EAAE,IAAI,CAAC,oBAAoB,EAAE,IAAI,EAC1D,IAAI,CAAC,mBAAmB,CAAC;IAC7B,IAAI,IAAI,CAAC,OAAO,EAAE;MAChB,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,SAAS,CACtB,MAAM,EAAE,CAAC,IAAI,CAAC,KAAK,CAAC,EAAE,IAAI,EAAE,IAAI,CAAC,eAAe,EAChD,IAAI,CAAC,eAAe,EAAE,IAAI,EAAE,IAAI,CAAC,cAAc,CAAC;KACrD,MAAM;MACL,IAAI,CAAC,IAAI,GAAG,IAAI;IACjB;IACD,IAAI,CAAC,KAAK,GAAG,IAAI;EACnB;EAEA;EACA;EACA;EACA;EACA;EACA;EACA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,MAAM,GAAG,MAAkB;MAC3B,IAAI,MAAM,CAAC,MAAM,KAAK,CAAC,EAAE;QACvB,MAAM,IAAI,UAAU,CAChB,8CAA8C,MAAM,CAAC,MAAM,GAAG,CAAC;MACpE;MACD,IAAI,UAAU,GAAG,MAAM,CAAC,CAAC,CAAC;MAC1B,MAAM,GAAG,MAAM,CAAC,CAAC,CAAC;MAClB,MAAM,QAAQ,GAAG,MAAM,CAAC,UAAU,CAAC,IAAI,IAAI,GAAG,KAAK,GAAG,MAAM,CAAC,UAAU,CAAC;MAExE,IAAI,CAAC,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,CAAC,OAAO,GAAG,CAAC,IAAI,IAAI,CAAC,WAAW,IAAI,IAAI,EAAE;QACpE,IAAI,CAAC,WAAW,GAAG,mBAAmB,CAAC;UAClB,IAAI,EAAE,MAAM,GAAG,CAAC,QAAQ,CAAC,MAAgB,CAAC;UAC1C,IAAI,EAAE,IAAI,CAAC,OAAO;UAClB;SACD,CAAW;MAChC;MACD,IAAI,CAAC,GAAG,IAAI,CAAC,gBAAgB,IAAI,IAAI,CAAC,gBAAgB,GAAG,CAAC,IACtD,IAAI,CAAC,oBAAoB,IAAI,IAAI,EAAE;QACrC,IAAI,CAAC,oBAAoB,GAAG,mBAAmB,CAAC;UAClB,IAAI,EAAE,MAAM,GAAG,CAAC,QAAQ,CAAC,UAAU,CAAC;UACpC,IAAI,EAAE,IAAI,CAAC,gBAAgB;UAC3B;SACD,CAAW;MACzC;MACD,IAAI,CAAS;MACb,MAAM,MAAM,GAAW,IAAI,CAAC,WAAqB;MACjD,MAAM,SAAS,GAAW,IAAI,CAAC,oBAA8B;MAC7D,IAAI,MAAM,IAAI,IAAI,EAAE;QAClB,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,EAAE,MAAM,CAAC,EAAE,IAAI,CAAC,MAAM,CAAC,IAAI,EAAE,CAAC;OACvD,MAAM;QACL,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,MAAM,EAAE,IAAI,CAAC,MAAM,CAAC,IAAI,EAAE,CAAC;MACtC;MACD,IAAI,IAAI,CAAC,IAAI,IAAI,IAAI,EAAE;QACrB,CAAC,GAAG,CAAC,CAAC,OAAO,CAAC,CAAC,EAAE,IAAI,CAAC,IAAI,CAAC,IAAI,EAAE,CAAC;MACnC;MACD,IAAI,SAAS,IAAI,IAAI,EAAE;QACrB,UAAU,GAAG,GAAG,CAAC,GAAG,CAAC,UAAU,EAAE,SAAS,CAAC;MAC5C;MACD,IAAI,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,CAAC,UAAU,EAAE,IAAI,CAAC,eAAe,CAAC,IAAI,EAAE,CAAC,CAAC;MACvE,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;QAC3B,MAAM,GAAG,IAAI,CAAC,UAAU,CAAC,KAAK,CAAC,MAAM,CAAC;MACvC;MAED;MACA,OAAO,CAAC,MAAM,EAAE,MAAM,CAAC;IACzB,CAAC,CAAC;EACJ;EAEA,SAAS,GAAA;IACP,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IAEpC,MAAM,MAAM,GAA6B;MACvC,KAAK,EAAE,IAAI,CAAC,KAAK;MACjB,UAAU,EAAE,mBAAmB,CAAC,IAAI,CAAC,UAAU,CAAC;MAChD,OAAO,EAAE,IAAI,CAAC,OAAO;MACrB,iBAAiB,EAAE,oBAAoB,CAAC,IAAI,CAAC,iBAAiB,CAAC;MAC/D,oBAAoB,EAAE,oBAAoB,CAAC,IAAI,CAAC,oBAAoB,CAAC;MACrE,eAAe,EAAE,oBAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;MAC3D,iBAAiB,EAAE,oBAAoB,CAAC,IAAI,CAAC,iBAAiB,CAAC;MAC/D,oBAAoB,EAAE,oBAAoB,CAAC,IAAI,CAAC,oBAAoB,CAAC;MACrE,eAAe,EAAE,oBAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;MAC3D,mBAAmB,EAAE,oBAAoB,CAAC,IAAI,CAAC,mBAAmB,CAAC;MACnE,gBAAgB,EAAE,mBAAmB,CAAC,IAAI,CAAC,gBAAgB,CAAC;MAC5D,mBAAmB,EAAE,mBAAmB,CAAC,IAAI,CAAC,mBAAmB,CAAC;MAClE,cAAc,EAAE,mBAAmB,CAAC,IAAI,CAAC,cAAc,CAAC;MACxD,OAAO,EAAE,IAAI,CAAC,OAAO;MACrB,gBAAgB,EAAE,IAAI,CAAC;KACxB;IAED,OAAA,MAAA,CAAA,MAAA,CAAA,CAAA,CAAA,EAAW,UAAU,EAAK,MAAM,CAAA;EAClC;;AAvKA;AACO,aAAA,CAAA,SAAS,GAAG,eAAe;AAwKpC,aAAa,CAAC,aAAa,CAAC,aAAa,CAAC;AA2F1C,OAAM,MAAO,SAAU,SAAQ,GAAG,CAAA;EAGhC,WAAA,CAAY,IAAwB,EAAA;IAClC,IAAI,CAAC,IAAI,GAAG,IAAI,aAAa,CAAC,IAAI,CAAC;IACnC,KAAK,CAAC,IAAoB,CAAC;IAC3B;EACF;;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,IAAI,IAAI,CAAC,IAAI,CAAC,WAAW,IAAI,IAAI,EAAE;QACjC,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,WAAW,CAAC;QAClC,IAAI,CAAC,IAAI,CAAC,WAAW,GAAG,IAAI;MAC7B;MACD,IAAI,IAAI,CAAC,IAAI,CAAC,oBAAoB,IAAI,IAAI,EAAE;QAC1C,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,oBAAoB,CAAC;QAC3C,IAAI,CAAC,IAAI,CAAC,oBAAoB,GAAG,IAAI;MACtC;MACD,MAAM,IAAI,GAAG,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,MAAM,CAAC;MACnD,MAAM,QAAQ,GAAG,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,UAAU,CAAC;MAC3D,MAAM,YAAY,GACd,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,cAAc,CAAC;MAClD,OAAO,KAAK,CAAC,IAAI,CAAC,MAAM,EAAE;QAAC,IAAI;QAAE,QAAQ;QAAE;MAAY,CAAC,CAAC;IAC3D,CAAC,CAAC;EACJ;EAEA;EACA,OAAO,UAAU,CACb,GAA6C,EAC7C,MAAgC,EAAA;IAClC,OAAO,IAAI,GAAG,CAAC,MAAM,CAAC;EACxB;;AA/BA;AACO,SAAA,CAAA,SAAS,GAAG,WAAW;AAgChC,aAAa,CAAC,aAAa,CAAC,SAAS,CAAC;AAqCtC,OAAM,MAAO,OAAQ,SAAQ,OAAO,CAAA;EAqClC,WAAA,CAAY,IAAsB,EAAA;IAChC,KAAK,CAAC,IAAI,CAAC;IAZJ,IAAA,CAAA,kBAAkB,GAAG,MAAM;IAC3B,IAAA,CAAA,4BAA4B,GAAyB,aAAa;IAElE,IAAA,CAAA,0BAA0B,GAAG,cAAc;IAC3C,IAAA,CAAA,6BAA6B,GAAG,YAAY;IAC5C,IAAA,CAAA,wBAAwB,GAA0B,OAAO;IAQhE,IAAI,IAAI,CAAC,UAAU,EAAE;MACnB,MAAM,IAAI,UAAU,CAChB,6DAA6D,CAAC;IACnE;IACD,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK;IACvB,qBAAqB,CAAC,IAAI,CAAC,KAAK,EAAE,OAAO,CAAC;IAC1C,IAAI,CAAC,UAAU,GAAG,aAAa,CAC3B,IAAI,CAAC,UAAU,KAAK,SAAS,GAAG,IAAI,CAAC,kBAAkB,GACvB,IAAI,CAAC,UAAU,CAAC;IACpD,IAAI,CAAC,mBAAmB,GAAG,aAAa,CACpC,IAAI,CAAC,mBAAmB,KAAK,SAAS,GAClC,IAAI,CAAC,4BAA4B,GACjC,IAAI,CAAC,mBAAmB,CAAC;IACjC,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,OAAO;IAEzD,IAAI,CAAC,iBAAiB,GAAG,cAAc,CACnC,IAAI,CAAC,iBAAiB,IAAI,IAAI,CAAC,0BAA0B,CAAC;IAC9D,IAAI,CAAC,oBAAoB,GAAG,cAAc,CACtC,IAAI,CAAC,oBAAoB,IAAI,IAAI,CAAC,6BAA6B,CAAC;IAEpE,IAAI,CAAC,eAAe,GAChB,cAAc,CAAC,IAAI,CAAC,eAAe,IAAI,IAAI,CAAC,wBAAwB,CAAC;IAEzE,IAAI,CAAC,iBAAiB,GAAG,cAAc,CAAC,IAAI,CAAC,iBAAiB,CAAC;IAC/D,IAAI,CAAC,oBAAoB,GAAG,cAAc,CAAC,IAAI,CAAC,oBAAoB,CAAC;IACrE,IAAI,CAAC,eAAe,GAAG,cAAc,CAAC,IAAI,CAAC,eAAe,CAAC;IAE3D,IAAI,CAAC,gBAAgB,GAAG,aAAa,CAAC,IAAI,CAAC,gBAAgB,CAAC;IAC5D,IAAI,CAAC,mBAAmB,GAAG,aAAa,CAAC,IAAI,CAAC,mBAAmB,CAAC;IAClE,IAAI,CAAC,cAAc,GAAG,aAAa,CAAC,IAAI,CAAC,cAAc,CAAC;IAExD,IAAI,CAAC,OAAO,GAAG,UAAU,CAAC,GAAG,CACzB,CAAC,CAAC,EAAE,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,OAAO,IAAI,IAAI,GAAG,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IACtE,IAAI,CAAC,gBAAgB,GAAG,UAAU,CAAC,GAAG,CAAC,CACrC,CAAC,EACD,UAAU,CAAC,GAAG,CACV,CAAC,CAAC,EAAE,IAAI,CAAC,gBAAgB,IAAI,IAAI,GAAG,CAAC,GAAG,IAAI,CAAC,gBAAgB,CAAC,CAAC,CACpE,CAAC;IACF,IAAI,CAAC,cAAc,GAAG,IAAI,CAAC,cAAc;IACzC,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC,KAAK;IAC3B,IAAI,CAAC,WAAW,GAAG,IAAI;IACvB,IAAI,CAAC,oBAAoB,GAAG,IAAI;EAClC;EAEO,KAAK,CAAC,UAAyB,EAAA;IACpC,UAAU,GAAG,kBAAkB,CAAC,UAAU,CAAC;IAC3C,MAAM,QAAQ,GAAG,UAAU,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC;IAClD,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,SAAS,CACxB,QAAQ,EAAE,CAAC,QAAQ,EAAE,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAAC,iBAAiB,EAClE,IAAI,CAAC,iBAAiB,EAAE,IAAI,EAAE,IAAI,CAAC,gBAAgB,CAAC;IACxD,IAAI,CAAC,eAAe,GAAG,IAAI,CAAC,SAAS,CACjC,kBAAkB,EAAE,CAAC,IAAI,CAAC,KAAK,EAAE,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EACtD,IAAI,CAAC,oBAAoB,EAAE,IAAI,CAAC,oBAAoB,EAAE,IAAI,EAC1D,IAAI,CAAC,mBAAmB,CAAC;IAC7B,IAAI,IAAI,CAAC,OAAO,EAAE;MAChB,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,SAAS,CACtB,MAAM,EAAE,CAAC,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAAC,eAAe,EACpD,IAAI,CAAC,eAAe,EAAE,IAAI,EAAE,IAAI,CAAC,cAAc,CAAC;KACrD,MAAM;MACL,IAAI,CAAC,IAAI,GAAG,IAAI;IACjB;IACD;IACA;IACA,IAAI,CAAC,KAAK,GAAG,IAAI;EACnB;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,MAAM,GAAG,MAAkB;MAC3B,IAAI,MAAM,CAAC,MAAM,KAAK,CAAC,EAAE;QACvB,MAAM,IAAI,UAAU,CAChB,sDAAsD,GACtD,GAAG,MAAM,CAAC,MAAM,GAAG,CAAC;MACzB;MAED,MAAM,QAAQ,GAAG,MAAM,CAAC,UAAU,CAAC,IAAI,IAAI,GAAG,KAAK,GAAG,MAAM,CAAC,UAAU,CAAC;MACxE,IAAI,QAAQ,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,CAAE;MAC3B,MAAM,GAAG,MAAM,CAAC,CAAC,CAAC;MAElB;MACA;MACA;MACA,IAAI,CAAC,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,CAAC,OAAO,GAAG,CAAC,IAAI,IAAI,CAAC,WAAW,IAAI,IAAI,EAAE;QACpE,IAAI,CAAC,WAAW,GAAG,mBAAmB,CAAC;UAClB,IAAI,EAAE,MAAM,GAAG,CAAC,QAAQ,CAAC,MAAgB,CAAC;UAC1C,IAAI,EAAE,IAAI,CAAC,OAAO;UAClB,QAAQ;UACR,KAAK,EAAE;SACR,CAAa;MAClC;MACD,IAAI,CAAC,GAAG,IAAI,CAAC,gBAAgB,IAAI,IAAI,CAAC,gBAAgB,GAAG,CAAC,IACtD,IAAI,CAAC,oBAAoB,IAAI,IAAI,EAAE;QACrC,IAAI,CAAC,oBAAoB,GAAG,mBAAmB,CAAC;UAClB,IAAI,EAAE,MAAM,GAAG,CAAC,QAAQ,CAAC,QAAQ,CAAC;UAClC,IAAI,EAAE,IAAI,CAAC,gBAAgB;UAC3B,QAAQ;UACR,KAAK,EAAE;SACR,CAAa;MAC3C;MACD,MAAM,MAAM,GAAG,IAAI,CAAC,WAAuC;MAC3D,MAAM,SAAS,GAAG,IAAI,CAAC,oBAAgD;MACvE,IAAI,CAAS;MACb,IAAI,CAAS;MACb,IAAI,EAAU;MAEd,IAAI,CAAC,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,CAAC,OAAO,GAAG,CAAC,EAAE;QACxC,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC;MACpC;MACD,IAAI,OAAO,GAAG,CAAC,CAAC,GAAG,CAAC,MAAM,EAAE,IAAI,CAAC,MAAM,CAAC,IAAI,EAAE,CAAC;MAC/C,IAAI,IAAI,CAAC,OAAO,EAAE;QAChB,OAAO,GAAG,CAAC,CAAC,OAAO,CAAC,OAAO,EAAE,IAAI,CAAC,IAAI,CAAC,IAAI,EAAE,CAAC;MAC/C;MACD,IAAI,CAAC,GAAG,IAAI,CAAC,gBAAgB,IAAI,IAAI,CAAC,gBAAgB,GAAG,CAAC,EAAE;QAC1D,QAAQ,GAAG,GAAG,CAAC,GAAG,CAAC,QAAQ,EAAE,SAAS,CAAC,CAAC,CAAC,CAAC;MAC3C;MAED,MAAM,oBAAoB,GAAG,IAAI,CAAC,eAAe,CAAC,IAAI,EAAE;MACxD,MAAM,CAAC,GAAG,EAAE,GAAG,CAAC,GAAG,GAAG,CAAC,KAAK,CACxB,oBAAoB,EAAE,CAAC,CAAC,GAAG,IAAI,CAAC,KAAK,EAAE,IAAI,CAAC,KAAK,CAAC,EAClD,oBAAoB,CAAC,IAAI,GAAG,CAAC,CAAC;MAClC,MAAM,WAAW,GAAG,CAAC,CAAC,GAAG,CAAC,QAAQ,EAAE,GAAG,CAAC;MAExC,MAAM,CAAC,EAAE,EAAE,EAAE,EAAE,EAAE,CAAC,GAAG,GAAG,CAAC,KAAK,CAAC,OAAO,EAAE,CAAC,EAAE,OAAO,CAAC,IAAI,GAAG,CAAC,CAAC;MAC5D,MAAM,CAAC,UAAU,EAAE,UAAU,CAAC,GAC1B,GAAG,CAAC,KAAK,CAAC,WAAW,EAAE,CAAC,EAAE,WAAW,CAAC,IAAI,GAAG,CAAC,CAAC;MACnD,CAAC,GAAG,IAAI,CAAC,mBAAmB,CAAC,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,EAAE,EAAE,UAAU,CAAC,CAAC;MAC3D,CAAC,GAAG,IAAI,CAAC,mBAAmB,CAAC,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,EAAE,EAAE,UAAU,CAAC,CAAC;MAE3D,MAAM,UAAU,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,QAAQ,CAAC,EAAE,GAAG,CAAC;MACnD,EAAE,GAAG,IAAI,CAAC,UAAU,CAAC,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,EAAE,EAAE,UAAU,CAAC,CAAC;MAEnD,MAAM,CAAC,GACH,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,QAAQ,CAAC,EAAE,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,GAAG,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;MACtE;MACA,OAAO,CAAC,CAAC,EAAE,CAAC,CAAC;IACf,CAAC,CAAC;EACJ;EAEA,SAAS,GAAA;IACP,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IAEpC,MAAM,MAAM,GAA6B;MACvC,KAAK,EAAE,IAAI,CAAC,KAAK;MACjB,UAAU,EAAE,mBAAmB,CAAC,IAAI,CAAC,UAAU,CAAC;MAChD,mBAAmB,EAAE,mBAAmB,CAAC,IAAI,CAAC,mBAAmB,CAAC;MAClE,OAAO,EAAE,IAAI,CAAC,OAAO;MACrB,iBAAiB,EAAE,oBAAoB,CAAC,IAAI,CAAC,iBAAiB,CAAC;MAC/D,oBAAoB,EAAE,oBAAoB,CAAC,IAAI,CAAC,oBAAoB,CAAC;MACrE,eAAe,EAAE,oBAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;MAC3D,iBAAiB,EAAE,oBAAoB,CAAC,IAAI,CAAC,iBAAiB,CAAC;MAC/D,oBAAoB,EAAE,oBAAoB,CAAC,IAAI,CAAC,oBAAoB,CAAC;MACrE,eAAe,EAAE,oBAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;MAC3D,mBAAmB,EAAE,oBAAoB,CAAC,IAAI,CAAC,mBAAmB,CAAC;MACnE,gBAAgB,EAAE,mBAAmB,CAAC,IAAI,CAAC,gBAAgB,CAAC;MAC5D,mBAAmB,EAAE,mBAAmB,CAAC,IAAI,CAAC,mBAAmB,CAAC;MAClE,cAAc,EAAE,mBAAmB,CAAC,IAAI,CAAC,cAAc,CAAC;MACxD,OAAO,EAAE,IAAI,CAAC,OAAO;MACrB,gBAAgB,EAAE,IAAI,CAAC,gBAAgB;MACvC,cAAc,EAAE,IAAI,CAAC,cAAc;MACnC,UAAU,EAAE;KACb;IAED,OAAA,MAAA,CAAA,MAAA,CAAA,CAAA,CAAA,EAAW,UAAU,EAAK,MAAM,CAAA;EAClC;;AAzMA;AACO,OAAA,CAAA,SAAS,GAAG,SAAS;AA0M9B,aAAa,CAAC,aAAa,CAAC,OAAO,CAAC;AA8BpC,OAAM,MAAO,GAAI,SAAQ,GAAG,CAAA;EAG1B,WAAA,CAAY,IAAkB,EAAA;IAC5B,IAAI,IAAI,CAAC,cAAc,KAAK,CAAC,EAAE;MAC7B,OAAO,CAAC,IAAI,CACR,8DAA8D,GAC9D,oDAAoD,CAAC;IAC1D;IACD,IAAI,CAAC,IAAI,GAAG,IAAI,OAAO,CAAC,IAAI,CAAC;IAC7B,KAAK,CAAC,IAAoB,CAAC;IAC3B;EACF;;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,IAAI,IAAI,CAAC,IAAI,CAAC,WAAW,IAAI,IAAI,EAAE;QACjC,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,WAAW,CAAC;QAClC,IAAI,CAAC,IAAI,CAAC,WAAW,GAAG,IAAI;MAC7B;MACD,IAAI,IAAI,CAAC,IAAI,CAAC,oBAAoB,IAAI,IAAI,EAAE;QAC1C,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,oBAAoB,CAAC;QAC3C,IAAI,CAAC,IAAI,CAAC,oBAAoB,GAAG,IAAI;MACtC;MACD,MAAM,IAAI,GAAG,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,MAAM,CAAC;MACnD,MAAM,QAAQ,GAAG,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,UAAU,CAAC;MAC3D,MAAM,YAAY,GACd,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,cAAc,CAAC;MAClD,OAAO,KAAK,CAAC,IAAI,CAAC,MAAM,EAAE;QAAC,IAAI;QAAE,QAAQ;QAAE;MAAY,CAAC,CAAC;IAC3D,CAAC,CAAC;EACJ;EAEA;EACA,OAAO,UAAU,CACb,GAA6C,EAC7C,MAAgC,EAAA;IAClC,IAAI,MAAM,CAAC,eAAe,CAAC,KAAK,CAAC,EAAE;MACjC,MAAM,CAAC,gBAAgB,CAAC,GAAG,CAAC;IAC7B;IACD,OAAO,IAAI,GAAG,CAAC,MAAM,CAAC;EACxB;;AAvCA;AACO,GAAA,CAAA,SAAS,GAAG,KAAK;AAwC1B,aAAa,CAAC,aAAa,CAAC,GAAG,CAAC;AAuChC,OAAM,MAAO,QAAS,SAAQ,OAAO,CAAA;EAsCnC,WAAA,CAAY,IAAuB,EAAA;IACjC,KAAK,CAAC,IAAI,CAAC;IAZJ,IAAA,CAAA,kBAAkB,GAAG,MAAM;IAC3B,IAAA,CAAA,4BAA4B,GAAG,aAAa;IAC5C,IAAA,CAAA,0BAA0B,GAAG,cAAc;IAC3C,IAAA,CAAA,6BAA6B,GAAG,YAAY;IAE5C,IAAA,CAAA,wBAAwB,GAAG,OAAO;IASzC,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK;IACvB,qBAAqB,CAAC,IAAI,CAAC,KAAK,EAAE,OAAO,CAAC;IAC1C,IAAI,CAAC,UAAU,GAAG,aAAa,CAC3B,IAAI,CAAC,UAAU,KAAK,SAAS,GAAG,IAAI,CAAC,kBAAkB,GACvB,IAAI,CAAC,UAAU,CAAC;IACpD,IAAI,CAAC,mBAAmB,GAAG,aAAa,CACpC,IAAI,CAAC,mBAAmB,KAAK,SAAS,GAClC,IAAI,CAAC,4BAA4B,GACjC,IAAI,CAAC,mBAAmB,CAAC;IACjC,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,OAAO;IAEzD,IAAI,CAAC,iBAAiB,GAAG,cAAc,CACnC,IAAI,CAAC,iBAAiB,IAAI,IAAI,CAAC,0BAA0B,CAAC;IAC9D,IAAI,CAAC,oBAAoB,GAAG,cAAc,CACtC,IAAI,CAAC,oBAAoB,IAAI,IAAI,CAAC,6BAA6B,CAAC;IAEpE,IAAI,CAAC,eAAe,GAChB,cAAc,CAAC,IAAI,CAAC,eAAe,IAAI,IAAI,CAAC,wBAAwB,CAAC;IACzE,IAAI,CAAC,cAAc,GAAG,IAAI,CAAC,cAAc;IAEzC,IAAI,CAAC,iBAAiB,GAAG,cAAc,CAAC,IAAI,CAAC,iBAAiB,CAAC;IAC/D,IAAI,CAAC,oBAAoB,GAAG,cAAc,CAAC,IAAI,CAAC,oBAAoB,CAAC;IACrE,IAAI,CAAC,eAAe,GAAG,cAAc,CAAC,IAAI,CAAC,eAAe,CAAC;IAE3D,IAAI,CAAC,gBAAgB,GAAG,aAAa,CAAC,IAAI,CAAC,gBAAgB,CAAC;IAC5D,IAAI,CAAC,mBAAmB,GAAG,aAAa,CAAC,IAAI,CAAC,mBAAmB,CAAC;IAClE,IAAI,CAAC,cAAc,GAAG,aAAa,CAAC,IAAI,CAAC,cAAc,CAAC;IAExD,IAAI,CAAC,OAAO,GAAG,UAAU,CAAC,GAAG,CACzB,CAAC,CAAC,EAAE,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,OAAO,IAAI,IAAI,GAAG,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IACtE,IAAI,CAAC,gBAAgB,GAAG,UAAU,CAAC,GAAG,CAAC,CACrC,CAAC,EACD,UAAU,CAAC,GAAG,CACV,CAAC,CAAC,EAAE,IAAI,CAAC,gBAAgB,IAAI,IAAI,GAAG,CAAC,GAAG,IAAI,CAAC,gBAAgB,CAAC,CAAC,CACpE,CAAC;IACF,IAAI,CAAC,cAAc,GAAG,IAAI,CAAC,cAAc;IACzC,IAAI,CAAC,SAAS,GAAG,CAAC,IAAI,CAAC,KAAK,EAAE,IAAI,CAAC,KAAK,CAAC;IACzC,IAAI,CAAC,WAAW,GAAG,IAAI;IACvB,IAAI,CAAC,oBAAoB,GAAG,IAAI;EAClC;EAEO,KAAK,CAAC,UAAyB,EAAA;;IACpC,UAAU,GAAG,kBAAkB,CAAC,UAAU,CAAC;IAC3C,MAAM,QAAQ,GAAG,UAAU,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC;IAClD,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,SAAS,CACxB,QAAQ,EAAE,CAAC,QAAQ,EAAE,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAAC,iBAAiB,EAClE,IAAI,CAAC,iBAAiB,EAAE,IAAI,EAAE,IAAI,CAAC,gBAAgB,CAAC;IACxD,IAAI,CAAC,eAAe,GAAG,IAAI,CAAC,SAAS,CACjC,kBAAkB,EAAE,CAAC,IAAI,CAAC,KAAK,EAAE,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EACtD,IAAI,CAAC,oBAAoB,EAAE,IAAI,CAAC,oBAAoB,EAAE,IAAI,EAC1D,IAAI,CAAC,mBAAmB,CAAC;IAC7B,IAAI,eAA4B;IAChC,IAAI,IAAI,CAAC,OAAO,EAAE;MAChB,IAAI,IAAI,CAAC,cAAc,EAAE;QACvB,MAAM,gBAAgB,GAAG,IAAI,CAAC,eAAe;QAC7C,MAAM,aAAa,GAAG,IAAI,CAAC,KAAK;QAChC,eAAe,GAAG,KAAI,EAAA,GAAC,MAAM,UAAW,SAAQ,WAAW,CAAA;UAIzD,KAAK,CAAC,KAAY,EAAE,KAAgB,EAAA;YAClC;YACA,MAAM,EAAE,GAAG,gBAAgB,CAAC,KAAK,CAAC,CAAC,aAAa,CAAC,CAAC;YAClD,MAAM,EAAE,GAAI,IAAI,IAAI,EAAE,CAAE,KAAK,CAAC,CAAC,aAAa,CAAC,CAAC;YAC9C,MAAM,MAAM,GAAG,gBAAgB,CAAC,KAAK,CAAC,CAAC,aAAa,GAAG,CAAC,CAAC,CAAC;YAC1D,OAAO,CAAC,CAAC,oBAAoB,CACzB,CAAC,CAAC,oBAAoB,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,MAAM,CAAC;UAC7C;SACD,EAXC;QACO,EAAA,CAAA,SAAS,GAAG,YAAa,E,EAUhC,GAAE;OACL,MAAM;QACL,eAAe,GAAG,IAAI,CAAC,eAAe;MACvC;MACD,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,SAAS,CACtB,MAAM,EAAE,CAAC,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EAAE,eAAe,EAAE,IAAI,CAAC,eAAe,EACrE,IAAI,EAAE,IAAI,CAAC,cAAc,CAAC;KAC/B,MAAM;MACL,IAAI,CAAC,IAAI,GAAG,IAAI;IACjB;IACD;IACA;IACA,IAAI,CAAC,KAAK,GAAG,IAAI;EACnB;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,MAAM,QAAQ,GAAG,MAAM,CAAC,UAAU,CAAC,IAAI,IAAI,GAAG,KAAK,GAAG,MAAM,CAAC,UAAU,CAAC;MACxE,MAAM,GAAG,MAAkB;MAC3B,IAAI,MAAM,CAAC,MAAM,KAAK,CAAC,EAAE;QACvB,MAAM,IAAI,UAAU,CAChB,uDAAuD,GACvD,GAAG,MAAM,CAAC,MAAM,GAAG,CAAC;MACzB;MACD,IAAI,QAAQ,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,CAAI;MAC7B,MAAM,QAAQ,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,CAAE;MAC7B,MAAM,GAAG,MAAM,CAAC,CAAC,CAAC;MAClB,IAAI,CAAC,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,CAAC,OAAO,GAAG,CAAC,IAAI,IAAI,CAAC,WAAW,IAAI,IAAI,EAAE;QACpE,IAAI,CAAC,WAAW,GAAG,mBAAmB,CAAC;UAClB,IAAI,EAAE,MAAM,GAAG,CAAC,QAAQ,CAAC,MAAgB,CAAC;UAC1C,IAAI,EAAE,IAAI,CAAC,OAAO;UAClB,QAAQ;UACR,KAAK,EAAE;SACR,CAAa;MAClC;MACD,IAAI,CAAC,GAAG,IAAI,CAAC,gBAAgB,IAAI,IAAI,CAAC,gBAAgB,GAAG,CAAC,IACtD,IAAI,CAAC,oBAAoB,IAAI,IAAI,EAAE;QACrC,IAAI,CAAC,oBAAoB,GAAG,mBAAmB,CAAC;UAClB,IAAI,EAAE,MAAM,GAAG,CAAC,QAAQ,CAAC,QAAQ,CAAC;UAClC,IAAI,EAAE,IAAI,CAAC,gBAAgB;UAC3B,QAAQ;UACR,KAAK,EAAE;SACR,CAAa;MAC3C;MACD,MAAM,MAAM,GAAG,IAAI,CAAC,WAA+C;MACnE,MAAM,SAAS,GACX,IAAI,CAAC,oBAAwD;MAEjE;MACA;MACA;MACA,IAAI,CAAS;MACb,IAAI,CAAS;MACb,IAAI,CAAS;MACb,IAAI,CAAS;MACb,IAAI,CAAC,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,CAAC,OAAO,GAAG,CAAC,EAAE;QACxC,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC;MACpC;MACD,IAAI,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,MAAM,EAAE,IAAI,CAAC,MAAM,CAAC,IAAI,EAAE,CAAC;MACzC,IAAI,CAAC,GAAG,IAAI,CAAC,gBAAgB,IAAI,IAAI,CAAC,gBAAgB,GAAG,CAAC,EAAE;QAC1D,QAAQ,GAAG,GAAG,CAAC,GAAG,CAAC,QAAQ,EAAE,SAAS,CAAC,CAAC,CAAC,CAAC;MAC3C;MACD,CAAC,GAAG,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,CAAC,QAAQ,EAAE,IAAI,CAAC,eAAe,CAAC,IAAI,EAAE,CAAC,CAAC;MAC5D,IAAI,IAAI,CAAC,OAAO,EAAE;QAChB,CAAC,GAAG,CAAC,CAAC,OAAO,CAAC,CAAC,EAAE,IAAI,CAAC,IAAI,CAAC,IAAI,EAAE,CAAC;MACnC;MAED,MAAM,CAAC,EAAE,EAAE,EAAE,EAAE,EAAE,EAAE,EAAE,CAAC,GAAG,GAAG,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,IAAI,GAAG,CAAC,CAAC;MAEpD,CAAC,GAAG,IAAI,CAAC,mBAAmB,CAAC,KAAK,CAAC,EAAE,CAAC;MACtC,CAAC,GAAG,IAAI,CAAC,mBAAmB,CAAC,KAAK,CAAC,EAAE,CAAC;MACtC,CAAC,GAAG,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,QAAQ,CAAC,EAAE,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,UAAU,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC;MACxE,CAAC,GAAG,IAAI,CAAC,mBAAmB,CAAC,KAAK,CAAC,EAAE,CAAC;MAEtC,MAAM,CAAC,GAAG,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;MAC9C;MACA,OAAO,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;IAClB,CAAC,CAAC;EACJ;EAEA,SAAS,GAAA;IACP,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IAEpC,MAAM,MAAM,GAA6B;MACvC,KAAK,EAAE,IAAI,CAAC,KAAK;MACjB,UAAU,EAAE,mBAAmB,CAAC,IAAI,CAAC,UAAU,CAAC;MAChD,mBAAmB,EAAE,mBAAmB,CAAC,IAAI,CAAC,mBAAmB,CAAC;MAClE,OAAO,EAAE,IAAI,CAAC,OAAO;MACrB,iBAAiB,EAAE,oBAAoB,CAAC,IAAI,CAAC,iBAAiB,CAAC;MAC/D,oBAAoB,EAAE,oBAAoB,CAAC,IAAI,CAAC,oBAAoB,CAAC;MACrE,eAAe,EAAE,oBAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;MAC3D,cAAc,EAAE,IAAI,CAAC,cAAc;MACnC,iBAAiB,EAAE,oBAAoB,CAAC,IAAI,CAAC,iBAAiB,CAAC;MAC/D,oBAAoB,EAAE,oBAAoB,CAAC,IAAI,CAAC,oBAAoB,CAAC;MACrE,eAAe,EAAE,oBAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;MAC3D,mBAAmB,EAAE,oBAAoB,CAAC,IAAI,CAAC,mBAAmB,CAAC;MACnE,gBAAgB,EAAE,mBAAmB,CAAC,IAAI,CAAC,gBAAgB,CAAC;MAC5D,mBAAmB,EAAE,mBAAmB,CAAC,IAAI,CAAC,mBAAmB,CAAC;MAClE,cAAc,EAAE,mBAAmB,CAAC,IAAI,CAAC,cAAc,CAAC;MACxD,OAAO,EAAE,IAAI,CAAC,OAAO;MACrB,gBAAgB,EAAE,IAAI,CAAC,gBAAgB;MACvC,cAAc,EAAE,IAAI,CAAC;KACtB;IAED,OAAA,MAAA,CAAA,MAAA,CAAA,CAAA,CAAA,EAAW,UAAU,EAAK,MAAM,CAAA;EAClC;;AArNA;AACO,QAAA,CAAA,SAAS,GAAG,UAAU;AAsN/B,aAAa,CAAC,aAAa,CAAC,QAAQ,CAAC;AAqCrC,OAAM,MAAO,IAAK,SAAQ,GAAG,CAAA;EAG3B,WAAA,CAAY,IAAmB,EAAA;IAC7B,IAAI,IAAI,CAAC,cAAc,KAAK,CAAC,EAAE;MAC7B,OAAO,CAAC,IAAI,CACR,8DAA8D,GAC9D,oDAAoD,CAAC;IAC1D;IACD,IAAI,CAAC,IAAI,GAAG,IAAI,QAAQ,CAAC,IAAI,CAAC;IAC9B,KAAK,CAAC,IAAoB,CAAC;IAC3B;EACF;;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,IAAI,IAAI,CAAC,IAAI,CAAC,WAAW,IAAI,IAAI,EAAE;QACjC,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,WAAW,CAAC;QAClC,IAAI,CAAC,IAAI,CAAC,WAAW,GAAG,IAAI;MAC7B;MACD,IAAI,IAAI,CAAC,IAAI,CAAC,oBAAoB,IAAI,IAAI,EAAE;QAC1C,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,oBAAoB,CAAC;QAC3C,IAAI,CAAC,IAAI,CAAC,oBAAoB,GAAG,IAAI;MACtC;MACD,MAAM,IAAI,GAAG,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,MAAM,CAAC;MACnD,MAAM,QAAQ,GAAG,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,UAAU,CAAC;MAC3D,MAAM,YAAY,GACd,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,cAAc,CAAC;MAClD,OAAO,KAAK,CAAC,IAAI,CAAC,MAAM,EAAE;QAAC,IAAI;QAAE,QAAQ;QAAE;MAAY,CAAC,CAAC;IAC3D,CAAC,CAAC;EACJ;EAEA;EACA,OAAO,UAAU,CACb,GAA6C,EAC7C,MAAgC,EAAA;IAClC,IAAI,MAAM,CAAC,eAAe,CAAC,KAAK,CAAC,EAAE;MACjC,MAAM,CAAC,gBAAgB,CAAC,GAAG,CAAC;IAC7B;IACD,OAAO,IAAI,GAAG,CAAC,MAAM,CAAC;EACxB;;AAvCA;AACO,IAAA,CAAA,SAAS,GAAG,MAAM;AAwC3B,aAAa,CAAC,aAAa,CAAC,IAAI,CAAC;AASjC,OAAM,MAAO,eAAgB,SAAQ,OAAO,CAAA;EAK1C,WAAA,CAAY,IAAyB,EAAA;IACnC,KAAK,CAAC,IAAI,CAAC;IACX,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK;EACzB;EAEA,IAAI,SAAS,GAAA;IACX;IACA;IACA;IACA;IACA,MAAM,SAAS,GAAa,EAAE;IAC9B,KAAK,MAAM,IAAI,IAAI,IAAI,CAAC,KAAK,CAAC,KAAK,EAAE,CAAC,OAAO,EAAE,EAAE;MAC/C,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,SAAS,CAAC,EAAE;QACjC,SAAS,CAAC,IAAI,CAAC,GAAG,IAAI,CAAC,SAAS,CAAC;OAClC,MAAM;QACL,SAAS,CAAC,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC;MAC/B;IACF;IACD,OAAO,SAAS;EAClB;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,MAAM,GAAG,MAAkB;MAC3B,IAAI,MAAM,GAAG,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC;MAE5B;MACA,MAAM,YAAY,GAAe,EAAE;MACnC,KAAK,MAAM,IAAI,IAAI,IAAI,CAAC,KAAK,CAAC,KAAK,EAAE,CAAC,OAAO,EAAE,EAAE;QAC/C,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,SAAS,CAAC,EAAE;UACjC,YAAY,CAAC,IAAI,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC,EAAE,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,CAAC;SAC3D,MAAM;UACL,YAAY,CAAC,IAAI,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QACvC;MACF;MACD,YAAY,CAAC,OAAO,EAAE;MAEtB;MACA,MAAM,eAAe,GAAe,EAAE;MACtC,IAAI,UAAoB;MACxB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;QAC1C,MAAM,IAAI,GAAG,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC;QAC1B,MAAM,GAAG,YAAY,CAAC,CAAC,CAAC;QACxB;QACA,IAAI,CAAC,KAAK,CAAC,EAAE;UACX,UAAU,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,MAAM,CAAC;SACxC,MAAM;UACL,UAAU,GAAG,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,MAAM,CAAC;QAC5C;QACD,UAAU,GAAG,IAAI,CAAC,IAAI,CAAC,UAAU,EAAE,MAAM,CAAa;QACtD,eAAe,CAAC,IAAI,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;MAC1C;MAED;MACA,MAAM,GAAG,EAAE;MACX,KAAK,MAAM,UAAU,IAAI,eAAe,CAAC,KAAK,EAAE,CAAC,OAAO,EAAE,EAAE;QAC1D,MAAM,CAAC,IAAI,CAAC,GAAG,UAAU,CAAC;MAC3B;MACD,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,MAAM,CAAC;IACvC,CAAC,CAAC;EACJ;EAEO,KAAK,CAAC,UAAyB,EAAA;IACpC,IAAI,eAAe,CAAC,UAAU,CAAC,EAAE;MAC/B;MACA;MACA,UAAU,GAAI,UAAsB,CAAC,CAAC,CAAC;IACxC;IACD,UAAU,GAAG,UAAmB;IAChC,IAAI,SAAiB;IACrB,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,CAAC,IAAI,EAAE,CAAC,KAAI;MAC7B,SAAS,CAAC,WAAW,CAAC,EAAE,EAAE,MAAK;QAC7B;QAEA,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC;QACtB,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,SAAS,CAAC,EAAE;UACjC,SAAS,GAAG,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC;SAC9B,MAAM;UACL,SAAS,GAAG,IAAI,CAAC,SAAS;QAC3B;QACD,UAAU,GAAG,CAAC,UAAU,CAAC,CAAC,CAAC,EAAE,SAAS,CAAU;MAClD,CAAC,CAAC;IACJ,CAAC,CAAC;IACF,IAAI,CAAC,KAAK,GAAG,IAAI;EACnB;EAEA,SAAS,GAAA;IACP,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IAEpC,MAAM,aAAa,GAAI,IAAa,IAAI;MACtC,OAAO;QACL,WAAW,EAAE,IAAI,CAAC,YAAY,EAAE;QAChC,QAAQ,EAAE,IAAI,CAAC,SAAS;OACzB;IACH,CAAC;IAED,MAAM,WAAW,GAAG,IAAI,CAAC,KAAK,CAAC,GAAG,CAAC,aAAa,CAAC;IAEjD,MAAM,MAAM,GAAG;MAAC,OAAO,EAAE;IAAW,CAAC;IAErC,OAAA,MAAA,CAAA,MAAA,CAAA,CAAA,CAAA,EAAW,UAAU,EAAK,MAAM,CAAA;EAClC;EAEA;EACA,OAAO,UAAU,CACb,GAA6C,EAC7C,MAAgC,EACc;IAAA,IAA9C,aAAA,uEAAgB,CAAA,CAA8B;IAChD,MAAM,KAAK,GAAc,EAAE;IAC3B,KAAK,MAAM,UAAU,IAAK,MAAM,CAAC,OAAO,CAAgC,EAAE;MACxE,KAAK,CAAC,IAAI,CAAC,WAAW,CAAC,UAAU,EAAE,aAAa,CAAY,CAAC;IAC9D;IACD,OAAO,IAAI,GAAG,CAAC;MAAC;IAAK,CAAC,CAAC;EACzB;EAEA,IAAI,gBAAgB,GAAA;IAClB,IAAI,CAAC,IAAI,CAAC,SAAS,EAAE;MACnB,OAAO,EAAE;IACV;IACD,MAAM,OAAO,GAAoB,EAAE;IACnC,KAAK,MAAM,IAAI,IAAI,IAAI,CAAC,KAAK,EAAE;MAC7B,OAAO,CAAC,IAAI,CAAC,GAAG,IAAI,CAAC,gBAAgB,CAAC;IACvC;IACD,OAAO,OAAO;EAChB;EAEA,IAAI,mBAAmB,GAAA;IACrB,MAAM,OAAO,GAAoB,EAAE;IACnC,KAAK,MAAM,IAAI,IAAI,IAAI,CAAC,KAAK,EAAE;MAC7B,OAAO,CAAC,IAAI,CAAC,GAAG,IAAI,CAAC,mBAAmB,CAAC;IAC1C;IACD,IAAI,CAAC,IAAI,CAAC,SAAS,EAAE;MACnB,MAAM,gBAAgB,GAAoB,EAAE;MAC5C,KAAK,MAAM,IAAI,IAAI,IAAI,CAAC,KAAK,EAAE;QAC7B,gBAAgB,CAAC,IAAI,CAAC,GAAG,IAAI,CAAC,gBAAgB,CAAC;MAChD;MACD,OAAO,gBAAgB,CAAC,MAAM,CAAC,OAAO,CAAC;IACxC;IACD,OAAO,OAAO;EAChB;EAEA;;;;AAIG;EACH,UAAU,GAAA;IACR,MAAM,OAAO,GAAoB,EAAE;IACnC,KAAK,MAAM,IAAI,IAAI,IAAI,CAAC,KAAK,EAAE;MAC7B,OAAO,CAAC,IAAI,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC;IAC9B;IACD,OAAO,aAAa,CAAC,OAAO,CAAC;EAC/B;EAEA;;;;;AAKG;EACH,UAAU,CAAC,OAAiB,EAAA;IAC1B,MAAM,MAAM,GAAmC,EAAE;IACjD,KAAK,MAAM,IAAI,IAAI,IAAI,CAAC,KAAK,EAAE;MAC7B,MAAM,SAAS,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM;MACrC,MAAM,YAAY,GAAG,OAAO,CAAC,MAAM,CAAC,SAAS,CAAC;MAC9C,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;QAC5C,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,YAAY,CAAC,CAAC,CAAC,CAAC,CAAC;MAChD;IACF;IACD,aAAa,CAAC,MAAM,CAAC;EACvB;;AA9KA;AACO,eAAA,CAAA,SAAS,GAAG,iBAAiB;AAiLtC,aAAa,CAAC,aAAa,CAAC,eAAe,CAAC;AAE5C,OAAM,SAAU,mBAAmB,CAAC,IAKnC,EAAA;EACC,MAAM;IAAC,IAAI;IAAE,IAAI;IAAE,QAAQ,GAAG,KAAK;IAAE,KAAK,GAAG;EAAC,CAAC,GAAG,IAAI;EAEtD,MAAM,aAAa,GAAG,MAAM,CAAC,CAAC,OAAO,CAAC,IAAI,EAAE,EAAE,IAAI,CAAC;EAEnD,MAAM,UAAU,GAAG,MAAM,CAAC,CAAC,YAAY,CAAC,aAAa,EAAE,IAAI,EAAE,QAAQ,CAAC;EAEtE;EACA,IAAI,CAAC,KAAK,IAAI,KAAK,IAAI,CAAC,EAAE;IACxB,OAAO,GAAG,CAAC,IAAI,CAAC,UAAU,EAAE,CAAC,KAAK,EAAE,CAAC;EACtC;EAED,MAAM,KAAK,GAAG,KAAK,CAAC,KAAK,CAAC,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC,GAAG,CAAC,UAAU,CAAC;EAE1D,OAAO,KAAK,CAAC,GAAG,CAAC,CAAC,IAAI,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC;AAC5C","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Recurrent Neural Network Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getActivation, serializeActivation } from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, SymbolicTensor } from '../engine/topology';\nimport { Layer } from '../engine/topology';\nimport { AttributeError, NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, Initializer, Ones, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { assertPositiveInteger } from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor, isArrayOfShapes } from '../utils/types_utils';\nimport { batchGetValue, batchSetValue } from '../variables';\nimport { deserialize } from './serialization';\n/**\n * Standardize `apply()` args to a single list of tensor inputs.\n *\n * When running a model loaded from file, the input tensors `initialState` and\n * `constants` are passed to `RNN.apply()` as part of `inputs` instead of the\n * dedicated kwargs fields. `inputs` consists of\n * `[inputs, initialState0, initialState1, ..., constant0, constant1]` in this\n * case.\n * This method makes sure that arguments are\n * separated and that `initialState` and `constants` are `Array`s of tensors\n * (or None).\n *\n * @param inputs Tensor or `Array` of  tensors.\n * @param initialState Tensor or `Array` of tensors or `null`/`undefined`.\n * @param constants Tensor or `Array` of tensors or `null`/`undefined`.\n * @returns An object consisting of\n *   inputs: A tensor.\n *   initialState: `Array` of tensors or `null`.\n *   constants: `Array` of tensors or `null`.\n * @throws ValueError, if `inputs` is an `Array` but either `initialState` or\n *   `constants` is provided.\n */\nexport function standardizeArgs(inputs, initialState, constants, numConstants) {\n    if (Array.isArray(inputs)) {\n        if (initialState != null || constants != null) {\n            throw new ValueError('When inputs is an array, neither initialState or constants ' +\n                'should be provided');\n        }\n        if (numConstants != null) {\n            constants = inputs.slice(inputs.length - numConstants, inputs.length);\n            inputs = inputs.slice(0, inputs.length - numConstants);\n        }\n        if (inputs.length > 1) {\n            initialState = inputs.slice(1, inputs.length);\n        }\n        inputs = inputs[0];\n    }\n    function toListOrNull(x) {\n        if (x == null || Array.isArray(x)) {\n            return x;\n        }\n        else {\n            return [x];\n        }\n    }\n    initialState = toListOrNull(initialState);\n    constants = toListOrNull(constants);\n    return { inputs, initialState, constants };\n}\n/**\n * Iterates over the time dimension of a tensor.\n *\n * @param stepFunction RNN step function.\n *   Parameters:\n *     inputs: tensor with shape `[samples, ...]` (no time dimension),\n *       representing input for the batch of samples at a certain time step.\n *     states: an Array of tensors.\n *   Returns:\n *     outputs: tensor with shape `[samples, outputDim]` (no time dimension).\n *     newStates: list of tensors, same length and shapes as `states`. The first\n *       state in the list must be the output tensor at the previous timestep.\n * @param inputs Tensor of temporal data of shape `[samples, time, ...]` (at\n *   least 3D).\n * @param initialStates Tensor with shape `[samples, outputDim]` (no time\n *   dimension), containing the initial values of the states used in the step\n *   function.\n * @param goBackwards If `true`, do the iteration over the time dimension in\n *   reverse order and return the reversed sequence.\n * @param mask Binary tensor with shape `[sample, time, 1]`, with a zero for\n *   every element that is masked.\n * @param constants An Array of constant values passed at each step.\n * @param unroll Whether to unroll the RNN or to use a symbolic loop. *Not*\n *   applicable to this imperative deeplearn.js backend. Its value is ignored.\n * @param needPerStepOutputs Whether the per-step outputs are to be\n *   concatenated into a single tensor and returned (as the second return\n *   value). Default: `false`. This arg is included so that the relatively\n *   expensive concatenation of the stepwise outputs can be omitted unless\n *   the stepwise outputs need to be kept (e.g., for an LSTM layer of which\n *   `returnSequence` is `true`.)\n * @returns An Array: `[lastOutput, outputs, newStates]`.\n *   lastOutput: the lastest output of the RNN, of shape `[samples, ...]`.\n *   outputs: tensor with shape `[samples, time, ...]` where each entry\n *     `output[s, t]` is the output of the step function at time `t` for sample\n *     `s`. This return value is provided if and only if the\n *     `needPerStepOutputs` is set as `true`. If it is set as `false`, this\n *     return value will be `undefined`.\n *   newStates: Array of tensors, latest states returned by the step function,\n *      of shape `(samples, ...)`.\n * @throws ValueError If input dimension is less than 3.\n *\n * TODO(nielsene): This needs to be tidy-ed.\n */\nexport function rnn(stepFunction, inputs, initialStates, goBackwards = false, mask, constants, unroll = false, needPerStepOutputs = false) {\n    return tfc.tidy(() => {\n        const ndim = inputs.shape.length;\n        if (ndim < 3) {\n            throw new ValueError(`Input should be at least 3D, but is ${ndim}D.`);\n        }\n        // Transpose to time-major, i.e., from [batch, time, ...] to [time, batch,\n        // ...].\n        const axes = [1, 0].concat(math_utils.range(2, ndim));\n        inputs = tfc.transpose(inputs, axes);\n        if (constants != null) {\n            throw new NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' +\n                'constants yet.');\n        }\n        // Porting Note: the unroll option is ignored by the imperative backend.\n        if (unroll) {\n            console.warn('Backend rnn(): the unroll = true option is not applicable to the ' +\n                'imperative deeplearn.js backend.');\n        }\n        if (mask != null) {\n            mask = mask.asType('bool').asType('float32');\n            if (mask.rank === ndim - 1) {\n                mask = tfc.expandDims(mask, -1);\n            }\n            mask = tfc.transpose(mask, axes);\n        }\n        if (goBackwards) {\n            inputs = tfc.reverse(inputs, 0);\n            if (mask != null) {\n                mask = tfc.reverse(mask, 0);\n            }\n        }\n        // Porting Note: PyKeras with TensorFlow backend uses a symbolic loop\n        //   (tf.while_loop). But for the imperative deeplearn.js backend, we just\n        //   use the usual TypeScript control flow to iterate over the time steps in\n        //   the inputs.\n        // Porting Note: PyKeras patches a \"_use_learning_phase\" attribute to\n        // outputs.\n        //   This is not idiomatic in TypeScript. The info regarding whether we are\n        //   in a learning (i.e., training) phase for RNN is passed in a different\n        //   way.\n        const perStepOutputs = [];\n        let lastOutput;\n        let states = initialStates;\n        const timeSteps = inputs.shape[0];\n        const perStepInputs = tfc.unstack(inputs);\n        let perStepMasks;\n        if (mask != null) {\n            perStepMasks = tfc.unstack(mask);\n        }\n        for (let t = 0; t < timeSteps; ++t) {\n            const currentInput = perStepInputs[t];\n            const stepOutputs = tfc.tidy(() => stepFunction(currentInput, states));\n            if (mask == null) {\n                lastOutput = stepOutputs[0];\n                states = stepOutputs[1];\n            }\n            else {\n                const maskedOutputs = tfc.tidy(() => {\n                    const stepMask = perStepMasks[t];\n                    const negStepMask = tfc.onesLike(stepMask).sub(stepMask);\n                    // TODO(cais): Would tfc.where() be better for performance?\n                    const output = stepOutputs[0].mul(stepMask).add(states[0].mul(negStepMask));\n                    const newStates = states.map((state, i) => {\n                        return stepOutputs[1][i].mul(stepMask).add(state.mul(negStepMask));\n                    });\n                    return { output, newStates };\n                });\n                lastOutput = maskedOutputs.output;\n                states = maskedOutputs.newStates;\n            }\n            if (needPerStepOutputs) {\n                perStepOutputs.push(lastOutput);\n            }\n        }\n        let outputs;\n        if (needPerStepOutputs) {\n            const axis = 1;\n            outputs = tfc.stack(perStepOutputs, axis);\n        }\n        return [lastOutput, outputs, states];\n    });\n}\nexport class RNN extends Layer {\n    constructor(args) {\n        super(args);\n        let cell;\n        if (args.cell == null) {\n            throw new ValueError('cell property is missing for the constructor of RNN.');\n        }\n        else if (Array.isArray(args.cell)) {\n            cell = new StackedRNNCells({ cells: args.cell });\n        }\n        else {\n            cell = args.cell;\n        }\n        if (cell.stateSize == null) {\n            throw new ValueError('The RNN cell should have an attribute `stateSize` (tuple of ' +\n                'integers, one integer per RNN state).');\n        }\n        this.cell = cell;\n        this.returnSequences =\n            args.returnSequences == null ? false : args.returnSequences;\n        this.returnState = args.returnState == null ? false : args.returnState;\n        this.goBackwards = args.goBackwards == null ? false : args.goBackwards;\n        this._stateful = args.stateful == null ? false : args.stateful;\n        this.unroll = args.unroll == null ? false : args.unroll;\n        this.supportsMasking = true;\n        this.inputSpec = [new InputSpec({ ndim: 3 })];\n        this.stateSpec = null;\n        this.states_ = null;\n        // TODO(cais): Add constantsSpec and numConstants.\n        this.numConstants = null;\n        // TODO(cais): Look into the use of initial_state in the kwargs of the\n        //   constructor.\n        this.keptStates = [];\n    }\n    // Porting Note: This is the equivalent of `RNN.states` property getter in\n    //   PyKeras.\n    getStates() {\n        if (this.states_ == null) {\n            const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n            return math_utils.range(0, numStates).map(x => null);\n        }\n        else {\n            return this.states_;\n        }\n    }\n    // Porting Note: This is the equivalent of the `RNN.states` property setter in\n    //   PyKeras.\n    setStates(states) {\n        this.states_ = states;\n    }\n    computeOutputShape(inputShape) {\n        if (isArrayOfShapes(inputShape)) {\n            inputShape = inputShape[0];\n        }\n        inputShape = inputShape;\n        // TODO(cais): Remove the casting once stacked RNN cells become supported.\n        let stateSize = this.cell.stateSize;\n        if (!Array.isArray(stateSize)) {\n            stateSize = [stateSize];\n        }\n        const outputDim = stateSize[0];\n        let outputShape;\n        if (this.returnSequences) {\n            outputShape = [inputShape[0], inputShape[1], outputDim];\n        }\n        else {\n            outputShape = [inputShape[0], outputDim];\n        }\n        if (this.returnState) {\n            const stateShape = [];\n            for (const dim of stateSize) {\n                stateShape.push([inputShape[0], dim]);\n            }\n            return [outputShape].concat(stateShape);\n        }\n        else {\n            return outputShape;\n        }\n    }\n    computeMask(inputs, mask) {\n        return tfc.tidy(() => {\n            if (Array.isArray(mask)) {\n                mask = mask[0];\n            }\n            const outputMask = this.returnSequences ? mask : null;\n            if (this.returnState) {\n                const stateMask = this.states.map(s => null);\n                return [outputMask].concat(stateMask);\n            }\n            else {\n                return outputMask;\n            }\n        });\n    }\n    /**\n     * Get the current state tensors of the RNN.\n     *\n     * If the state hasn't been set, return an array of `null`s of the correct\n     * length.\n     */\n    get states() {\n        if (this.states_ == null) {\n            const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n            const output = [];\n            for (let i = 0; i < numStates; ++i) {\n                output.push(null);\n            }\n            return output;\n        }\n        else {\n            return this.states_;\n        }\n    }\n    set states(s) {\n        this.states_ = s;\n    }\n    build(inputShape) {\n        // Note inputShape will be an Array of Shapes of initial states and\n        // constants if these are passed in apply().\n        const constantShape = null;\n        if (this.numConstants != null) {\n            throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n        }\n        if (isArrayOfShapes(inputShape)) {\n            inputShape = inputShape[0];\n        }\n        inputShape = inputShape;\n        const batchSize = this.stateful ? inputShape[0] : null;\n        const inputDim = inputShape.slice(2);\n        this.inputSpec[0] = new InputSpec({ shape: [batchSize, null, ...inputDim] });\n        // Allow cell (if RNNCell Layer) to build before we set or validate\n        // stateSpec.\n        const stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        if (constantShape != null) {\n            throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n        }\n        else {\n            this.cell.build(stepInputShape);\n        }\n        // Set or validate stateSpec.\n        let stateSize;\n        if (Array.isArray(this.cell.stateSize)) {\n            stateSize = this.cell.stateSize;\n        }\n        else {\n            stateSize = [this.cell.stateSize];\n        }\n        if (this.stateSpec != null) {\n            if (!util.arraysEqual(this.stateSpec.map(spec => spec.shape[spec.shape.length - 1]), stateSize)) {\n                throw new ValueError(`An initialState was passed that is not compatible with ` +\n                    `cell.stateSize. Received stateSpec=${this.stateSpec}; ` +\n                    `However cell.stateSize is ${this.cell.stateSize}`);\n            }\n        }\n        else {\n            this.stateSpec =\n                stateSize.map(dim => new InputSpec({ shape: [null, dim] }));\n        }\n        if (this.stateful) {\n            this.resetStates();\n        }\n    }\n    /**\n     * Reset the state tensors of the RNN.\n     *\n     * If the `states` argument is `undefined` or `null`, will set the\n     * state tensor(s) of the RNN to all-zero tensors of the appropriate\n     * shape(s).\n     *\n     * If `states` is provided, will set the state tensors of the RNN to its\n     * value.\n     *\n     * @param states Optional externally-provided initial states.\n     * @param training Whether this call is done during training. For stateful\n     *   RNNs, this affects whether the old states are kept or discarded. In\n     *   particular, if `training` is `true`, the old states will be kept so\n     *   that subsequent backpropgataion through time (BPTT) may work properly.\n     *   Else, the old states will be discarded.\n     */\n    resetStates(states, training = false) {\n        tidy(() => {\n            if (!this.stateful) {\n                throw new AttributeError('Cannot call resetStates() on an RNN Layer that is not stateful.');\n            }\n            const batchSize = this.inputSpec[0].shape[0];\n            if (batchSize == null) {\n                throw new ValueError('If an RNN is stateful, it needs to know its batch size. Specify ' +\n                    'the batch size of your input tensors: \\n' +\n                    '- If using a Sequential model, specify the batch size by ' +\n                    'passing a `batchInputShape` option to your first layer.\\n' +\n                    '- If using the functional API, specify the batch size by ' +\n                    'passing a `batchShape` option to your Input layer.');\n            }\n            // Initialize state if null.\n            if (this.states_ == null) {\n                if (Array.isArray(this.cell.stateSize)) {\n                    this.states_ =\n                        this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n                }\n                else {\n                    this.states_ = [tfc.zeros([batchSize, this.cell.stateSize])];\n                }\n            }\n            else if (states == null) {\n                // Dispose old state tensors.\n                tfc.dispose(this.states_);\n                // For stateful RNNs, fully dispose kept old states.\n                if (this.keptStates != null) {\n                    tfc.dispose(this.keptStates);\n                    this.keptStates = [];\n                }\n                if (Array.isArray(this.cell.stateSize)) {\n                    this.states_ =\n                        this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n                }\n                else {\n                    this.states_[0] = tfc.zeros([batchSize, this.cell.stateSize]);\n                }\n            }\n            else {\n                if (!Array.isArray(states)) {\n                    states = [states];\n                }\n                if (states.length !== this.states_.length) {\n                    throw new ValueError(`Layer ${this.name} expects ${this.states_.length} state(s), ` +\n                        `but it received ${states.length} state value(s). Input ` +\n                        `received: ${states}`);\n                }\n                if (training === true) {\n                    // Store old state tensors for complete disposal later, i.e., during\n                    // the next no-arg call to this method. We do not dispose the old\n                    // states immediately because that BPTT (among other things) require\n                    // them.\n                    this.keptStates.push(this.states_.slice());\n                }\n                else {\n                    tfc.dispose(this.states_);\n                }\n                for (let index = 0; index < this.states_.length; ++index) {\n                    const value = states[index];\n                    const dim = Array.isArray(this.cell.stateSize) ?\n                        this.cell.stateSize[index] :\n                        this.cell.stateSize;\n                    const expectedShape = [batchSize, dim];\n                    if (!util.arraysEqual(value.shape, expectedShape)) {\n                        throw new ValueError(`State ${index} is incompatible with layer ${this.name}: ` +\n                            `expected shape=${expectedShape}, received shape=${value.shape}`);\n                    }\n                    this.states_[index] = value;\n                }\n            }\n            this.states_ = this.states_.map(state => tfc.keep(state.clone()));\n        });\n    }\n    apply(inputs, kwargs) {\n        // TODO(cais): Figure out whether initialState is in kwargs or inputs.\n        let initialState = kwargs == null ? null : kwargs['initialState'];\n        let constants = kwargs == null ? null : kwargs['constants'];\n        if (kwargs == null) {\n            kwargs = {};\n        }\n        const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n        inputs = standardized.inputs;\n        initialState = standardized.initialState;\n        constants = standardized.constants;\n        // If any of `initial_state` or `constants` are specified and are\n        // `tf.SymbolicTensor`s, then add them to the inputs and temporarily modify\n        // the input_spec to include them.\n        let additionalInputs = [];\n        let additionalSpecs = [];\n        if (initialState != null) {\n            kwargs['initialState'] = initialState;\n            additionalInputs = additionalInputs.concat(initialState);\n            this.stateSpec = [];\n            for (const state of initialState) {\n                this.stateSpec.push(new InputSpec({ shape: state.shape }));\n            }\n            // TODO(cais): Use the following instead.\n            // this.stateSpec = initialState.map(state => new InputSpec({shape:\n            // state.shape}));\n            additionalSpecs = additionalSpecs.concat(this.stateSpec);\n        }\n        if (constants != null) {\n            kwargs['constants'] = constants;\n            additionalInputs = additionalInputs.concat(constants);\n            // TODO(cais): Add this.constantsSpec.\n            this.numConstants = constants.length;\n        }\n        const isTensor = additionalInputs[0] instanceof SymbolicTensor;\n        if (isTensor) {\n            // Compute full input spec, including state and constants.\n            const fullInput = [inputs].concat(additionalInputs);\n            const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n            // Perform the call with temporarily replaced inputSpec.\n            const originalInputSpec = this.inputSpec;\n            this.inputSpec = fullInputSpec;\n            const output = super.apply(fullInput, kwargs);\n            this.inputSpec = originalInputSpec;\n            return output;\n        }\n        else {\n            return super.apply(inputs, kwargs);\n        }\n    }\n    // tslint:disable-next-line:no-any\n    call(inputs, kwargs) {\n        // Input shape: `[samples, time (padded with zeros), input_dim]`.\n        // Note that the .build() method of subclasses **must** define\n        // this.inputSpec and this.stateSpec owith complete input shapes.\n        return tidy(() => {\n            const mask = kwargs == null ? null : kwargs['mask'];\n            const training = kwargs == null ? null : kwargs['training'];\n            let initialState = kwargs == null ? null : kwargs['initialState'];\n            inputs = getExactlyOneTensor(inputs);\n            if (initialState == null) {\n                if (this.stateful) {\n                    initialState = this.states_;\n                }\n                else {\n                    initialState = this.getInitialState(inputs);\n                }\n            }\n            const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n            if (initialState.length !== numStates) {\n                throw new ValueError(`RNN Layer has ${numStates} state(s) but was passed ` +\n                    `${initialState.length} initial state(s).`);\n            }\n            if (this.unroll) {\n                console.warn('Ignoring unroll = true for RNN layer, due to imperative backend.');\n            }\n            const cellCallKwargs = { training };\n            // TODO(cais): Add support for constants.\n            const step = (inputs, states) => {\n                // `inputs` and `states` are concatenated to form a single `Array` of\n                // `tf.Tensor`s as the input to `cell.call()`.\n                const outputs = this.cell.call([inputs].concat(states), cellCallKwargs);\n                // Marshall the return value into output and new states.\n                return [outputs[0], outputs.slice(1)];\n            };\n            // TODO(cais): Add support for constants.\n            const rnnOutputs = rnn(step, inputs, initialState, this.goBackwards, mask, null, this.unroll, this.returnSequences);\n            const lastOutput = rnnOutputs[0];\n            const outputs = rnnOutputs[1];\n            const states = rnnOutputs[2];\n            if (this.stateful) {\n                this.resetStates(states, training);\n            }\n            const output = this.returnSequences ? outputs : lastOutput;\n            // TODO(cais): Porperty set learning phase flag.\n            if (this.returnState) {\n                return [output].concat(states);\n            }\n            else {\n                return output;\n            }\n        });\n    }\n    getInitialState(inputs) {\n        return tidy(() => {\n            // Build an all-zero tensor of shape [samples, outputDim].\n            // [Samples, timeSteps, inputDim].\n            let initialState = tfc.zeros(inputs.shape);\n            // [Samples].\n            initialState = tfc.sum(initialState, [1, 2]);\n            initialState = K.expandDims(initialState); // [Samples, 1].\n            if (Array.isArray(this.cell.stateSize)) {\n                return this.cell.stateSize.map(dim => dim > 1 ? K.tile(initialState, [1, dim]) : initialState);\n            }\n            else {\n                return this.cell.stateSize > 1 ?\n                    [K.tile(initialState, [1, this.cell.stateSize])] :\n                    [initialState];\n            }\n        });\n    }\n    get trainableWeights() {\n        if (!this.trainable) {\n            return [];\n        }\n        // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n        return this.cell.trainableWeights;\n    }\n    get nonTrainableWeights() {\n        // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n        if (!this.trainable) {\n            return this.cell.weights;\n        }\n        return this.cell.nonTrainableWeights;\n    }\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.cell != null) {\n            this.cell.setFastWeightInitDuringBuild(value);\n        }\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = {\n            returnSequences: this.returnSequences,\n            returnState: this.returnState,\n            goBackwards: this.goBackwards,\n            stateful: this.stateful,\n            unroll: this.unroll,\n        };\n        if (this.numConstants != null) {\n            config['numConstants'] = this.numConstants;\n        }\n        const cellConfig = this.cell.getConfig();\n        if (this.getClassName() === RNN.className) {\n            config['cell'] = {\n                'className': this.cell.getClassName(),\n                'config': cellConfig,\n            };\n        }\n        // this order is necessary, to prevent cell name from replacing layer name\n        return Object.assign({}, cellConfig, baseConfig, config);\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config, customObjects = {}) {\n        const cellConfig = config['cell'];\n        const cell = deserialize(cellConfig, customObjects);\n        return new cls(Object.assign(config, { cell }));\n    }\n}\n/** @nocollapse */\nRNN.className = 'RNN';\nserialization.registerClass(RNN);\n// Porting Note: This is a common parent class for RNN cells. There is no\n// equivalent of this in PyKeras. Having a common parent class forgoes the\n//  need for `has_attr(cell, ...)` checks or its TypeScript equivalent.\n/**\n * An RNNCell layer.\n *\n * @doc {heading: 'Layers', subheading: 'Classes'}\n */\nexport class RNNCell extends Layer {\n}\nexport class SimpleRNNCell extends RNNCell {\n    constructor(args) {\n        super(args);\n        this.DEFAULT_ACTIVATION = 'tanh';\n        this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n        this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n        this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n        this.units = args.units;\n        assertPositiveInteger(this.units, `units`);\n        this.activation = getActivation(args.activation == null ? this.DEFAULT_ACTIVATION : args.activation);\n        this.useBias = args.useBias == null ? true : args.useBias;\n        this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n        this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n        this.biasInitializer =\n            getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n        this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n        this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n        this.biasRegularizer = getRegularizer(args.biasRegularizer);\n        this.kernelConstraint = getConstraint(args.kernelConstraint);\n        this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n        this.biasConstraint = getConstraint(args.biasConstraint);\n        this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n        this.recurrentDropout = math_utils.min([\n            1,\n            math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n        ]);\n        this.stateSize = this.units;\n        this.dropoutMask = null;\n        this.recurrentDropoutMask = null;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        // TODO(cais): Use regularizer.\n        this.kernel = this.addWeight('kernel', [inputShape[inputShape.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n        this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n        if (this.useBias) {\n            this.bias = this.addWeight('bias', [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n        }\n        else {\n            this.bias = null;\n        }\n        this.built = true;\n    }\n    // Porting Note: PyKeras' equivalent of this method takes two tensor inputs:\n    //   `inputs` and `states`. Here, the two tensors are combined into an\n    //   `Tensor[]` Array as the first input argument.\n    //   Similarly, PyKeras' equivalent of this method returns two values:\n    //    `output` and `[output]`. Here the two are combined into one length-2\n    //    `Tensor[]`, consisting of `output` repeated.\n    call(inputs, kwargs) {\n        return tidy(() => {\n            inputs = inputs;\n            if (inputs.length !== 2) {\n                throw new ValueError(`SimpleRNNCell expects 2 input Tensors, got ${inputs.length}.`);\n            }\n            let prevOutput = inputs[1];\n            inputs = inputs[0];\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n                this.dropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(inputs),\n                    rate: this.dropout,\n                    training\n                });\n            }\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n                this.recurrentDropoutMask == null) {\n                this.recurrentDropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(prevOutput),\n                    rate: this.recurrentDropout,\n                    training\n                });\n            }\n            let h;\n            const dpMask = this.dropoutMask;\n            const recDpMask = this.recurrentDropoutMask;\n            if (dpMask != null) {\n                h = K.dot(tfc.mul(inputs, dpMask), this.kernel.read());\n            }\n            else {\n                h = K.dot(inputs, this.kernel.read());\n            }\n            if (this.bias != null) {\n                h = K.biasAdd(h, this.bias.read());\n            }\n            if (recDpMask != null) {\n                prevOutput = tfc.mul(prevOutput, recDpMask);\n            }\n            let output = tfc.add(h, K.dot(prevOutput, this.recurrentKernel.read()));\n            if (this.activation != null) {\n                output = this.activation.apply(output);\n            }\n            // TODO(cais): Properly set learning phase on output tensor?\n            return [output, output];\n        });\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = {\n            units: this.units,\n            activation: serializeActivation(this.activation),\n            useBias: this.useBias,\n            kernelInitializer: serializeInitializer(this.kernelInitializer),\n            recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n            biasInitializer: serializeInitializer(this.biasInitializer),\n            kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n            recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n            biasRegularizer: serializeRegularizer(this.biasRegularizer),\n            activityRegularizer: serializeRegularizer(this.activityRegularizer),\n            kernelConstraint: serializeConstraint(this.kernelConstraint),\n            recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n            biasConstraint: serializeConstraint(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n        };\n        return Object.assign({}, baseConfig, config);\n    }\n}\n/** @nocollapse */\nSimpleRNNCell.className = 'SimpleRNNCell';\nserialization.registerClass(SimpleRNNCell);\nexport class SimpleRNN extends RNN {\n    constructor(args) {\n        args.cell = new SimpleRNNCell(args);\n        super(args);\n        // TODO(cais): Add activityRegularizer.\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.cell.dropoutMask != null) {\n                tfc.dispose(this.cell.dropoutMask);\n                this.cell.dropoutMask = null;\n            }\n            if (this.cell.recurrentDropoutMask != null) {\n                tfc.dispose(this.cell.recurrentDropoutMask);\n                this.cell.recurrentDropoutMask = null;\n            }\n            const mask = kwargs == null ? null : kwargs['mask'];\n            const training = kwargs == null ? null : kwargs['training'];\n            const initialState = kwargs == null ? null : kwargs['initialState'];\n            return super.call(inputs, { mask, training, initialState });\n        });\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config);\n    }\n}\n/** @nocollapse */\nSimpleRNN.className = 'SimpleRNN';\nserialization.registerClass(SimpleRNN);\nexport class GRUCell extends RNNCell {\n    constructor(args) {\n        super(args);\n        this.DEFAULT_ACTIVATION = 'tanh';\n        this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n        this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n        this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n        this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n        if (args.resetAfter) {\n            throw new ValueError(`GRUCell does not support reset_after parameter set to true.`);\n        }\n        this.units = args.units;\n        assertPositiveInteger(this.units, 'units');\n        this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION :\n            args.activation);\n        this.recurrentActivation = getActivation(args.recurrentActivation === undefined ?\n            this.DEFAULT_RECURRENT_ACTIVATION :\n            args.recurrentActivation);\n        this.useBias = args.useBias == null ? true : args.useBias;\n        this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n        this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n        this.biasInitializer =\n            getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n        this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n        this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n        this.biasRegularizer = getRegularizer(args.biasRegularizer);\n        this.kernelConstraint = getConstraint(args.kernelConstraint);\n        this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n        this.biasConstraint = getConstraint(args.biasConstraint);\n        this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n        this.recurrentDropout = math_utils.min([\n            1,\n            math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n        ]);\n        this.implementation = args.implementation;\n        this.stateSize = this.units;\n        this.dropoutMask = null;\n        this.recurrentDropoutMask = null;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const inputDim = inputShape[inputShape.length - 1];\n        this.kernel = this.addWeight('kernel', [inputDim, this.units * 3], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n        this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 3], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n        if (this.useBias) {\n            this.bias = this.addWeight('bias', [this.units * 3], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n        }\n        else {\n            this.bias = null;\n        }\n        // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n        //   of the weights and bias in the call() method, at execution time.\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            inputs = inputs;\n            if (inputs.length !== 2) {\n                throw new ValueError(`GRUCell expects 2 input Tensors (inputs, h, c), got ` +\n                    `${inputs.length}.`);\n            }\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            let hTMinus1 = inputs[1]; // Previous memory state.\n            inputs = inputs[0];\n            // Note: For superior performance, TensorFlow.js always uses\n            // implementation 2, regardless of the actual value of\n            // config.implementation.\n            if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n                this.dropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(inputs),\n                    rate: this.dropout,\n                    training,\n                    count: 3\n                });\n            }\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n                this.recurrentDropoutMask == null) {\n                this.recurrentDropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(hTMinus1),\n                    rate: this.recurrentDropout,\n                    training,\n                    count: 3\n                });\n            }\n            const dpMask = this.dropoutMask;\n            const recDpMask = this.recurrentDropoutMask;\n            let z;\n            let r;\n            let hh;\n            if (0 < this.dropout && this.dropout < 1) {\n                inputs = tfc.mul(inputs, dpMask[0]);\n            }\n            let matrixX = K.dot(inputs, this.kernel.read());\n            if (this.useBias) {\n                matrixX = K.biasAdd(matrixX, this.bias.read());\n            }\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n                hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n            }\n            const recurrentKernelValue = this.recurrentKernel.read();\n            const [rk1, rk2] = tfc.split(recurrentKernelValue, [2 * this.units, this.units], recurrentKernelValue.rank - 1);\n            const matrixInner = K.dot(hTMinus1, rk1);\n            const [xZ, xR, xH] = tfc.split(matrixX, 3, matrixX.rank - 1);\n            const [recurrentZ, recurrentR] = tfc.split(matrixInner, 2, matrixInner.rank - 1);\n            z = this.recurrentActivation.apply(tfc.add(xZ, recurrentZ));\n            r = this.recurrentActivation.apply(tfc.add(xR, recurrentR));\n            const recurrentH = K.dot(tfc.mul(r, hTMinus1), rk2);\n            hh = this.activation.apply(tfc.add(xH, recurrentH));\n            const h = tfc.add(tfc.mul(z, hTMinus1), tfc.mul(tfc.add(1, tfc.neg(z)), hh));\n            // TODO(cais): Add use_learning_phase flag properly.\n            return [h, h];\n        });\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = {\n            units: this.units,\n            activation: serializeActivation(this.activation),\n            recurrentActivation: serializeActivation(this.recurrentActivation),\n            useBias: this.useBias,\n            kernelInitializer: serializeInitializer(this.kernelInitializer),\n            recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n            biasInitializer: serializeInitializer(this.biasInitializer),\n            kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n            recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n            biasRegularizer: serializeRegularizer(this.biasRegularizer),\n            activityRegularizer: serializeRegularizer(this.activityRegularizer),\n            kernelConstraint: serializeConstraint(this.kernelConstraint),\n            recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n            biasConstraint: serializeConstraint(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n            implementation: this.implementation,\n            resetAfter: false\n        };\n        return Object.assign({}, baseConfig, config);\n    }\n}\n/** @nocollapse */\nGRUCell.className = 'GRUCell';\nserialization.registerClass(GRUCell);\nexport class GRU extends RNN {\n    constructor(args) {\n        if (args.implementation === 0) {\n            console.warn('`implementation=0` has been deprecated, and now defaults to ' +\n                '`implementation=1`. Please update your layer call.');\n        }\n        args.cell = new GRUCell(args);\n        super(args);\n        // TODO(cais): Add activityRegularizer.\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.cell.dropoutMask != null) {\n                tfc.dispose(this.cell.dropoutMask);\n                this.cell.dropoutMask = null;\n            }\n            if (this.cell.recurrentDropoutMask != null) {\n                tfc.dispose(this.cell.recurrentDropoutMask);\n                this.cell.recurrentDropoutMask = null;\n            }\n            const mask = kwargs == null ? null : kwargs['mask'];\n            const training = kwargs == null ? null : kwargs['training'];\n            const initialState = kwargs == null ? null : kwargs['initialState'];\n            return super.call(inputs, { mask, training, initialState });\n        });\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        if (config['implmentation'] === 0) {\n            config['implementation'] = 1;\n        }\n        return new cls(config);\n    }\n}\n/** @nocollapse */\nGRU.className = 'GRU';\nserialization.registerClass(GRU);\nexport class LSTMCell extends RNNCell {\n    constructor(args) {\n        super(args);\n        this.DEFAULT_ACTIVATION = 'tanh';\n        this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n        this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n        this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n        this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n        this.units = args.units;\n        assertPositiveInteger(this.units, 'units');\n        this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION :\n            args.activation);\n        this.recurrentActivation = getActivation(args.recurrentActivation === undefined ?\n            this.DEFAULT_RECURRENT_ACTIVATION :\n            args.recurrentActivation);\n        this.useBias = args.useBias == null ? true : args.useBias;\n        this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n        this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n        this.biasInitializer =\n            getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n        this.unitForgetBias = args.unitForgetBias;\n        this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n        this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n        this.biasRegularizer = getRegularizer(args.biasRegularizer);\n        this.kernelConstraint = getConstraint(args.kernelConstraint);\n        this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n        this.biasConstraint = getConstraint(args.biasConstraint);\n        this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n        this.recurrentDropout = math_utils.min([\n            1,\n            math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n        ]);\n        this.implementation = args.implementation;\n        this.stateSize = [this.units, this.units];\n        this.dropoutMask = null;\n        this.recurrentDropoutMask = null;\n    }\n    build(inputShape) {\n        var _a;\n        inputShape = getExactlyOneShape(inputShape);\n        const inputDim = inputShape[inputShape.length - 1];\n        this.kernel = this.addWeight('kernel', [inputDim, this.units * 4], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n        this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 4], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n        let biasInitializer;\n        if (this.useBias) {\n            if (this.unitForgetBias) {\n                const capturedBiasInit = this.biasInitializer;\n                const capturedUnits = this.units;\n                biasInitializer = new (_a = class CustomInit extends Initializer {\n                        apply(shape, dtype) {\n                            // TODO(cais): More informative variable names?\n                            const bI = capturedBiasInit.apply([capturedUnits]);\n                            const bF = (new Ones()).apply([capturedUnits]);\n                            const bCAndH = capturedBiasInit.apply([capturedUnits * 2]);\n                            return K.concatAlongFirstAxis(K.concatAlongFirstAxis(bI, bF), bCAndH);\n                        }\n                    },\n                    /** @nocollapse */\n                    _a.className = 'CustomInit',\n                    _a)();\n            }\n            else {\n                biasInitializer = this.biasInitializer;\n            }\n            this.bias = this.addWeight('bias', [this.units * 4], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n        }\n        else {\n            this.bias = null;\n        }\n        // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n        //   of the weights and bias in the call() method, at execution time.\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            inputs = inputs;\n            if (inputs.length !== 3) {\n                throw new ValueError(`LSTMCell expects 3 input Tensors (inputs, h, c), got ` +\n                    `${inputs.length}.`);\n            }\n            let hTMinus1 = inputs[1]; // Previous memory state.\n            const cTMinus1 = inputs[2]; // Previous carry state.\n            inputs = inputs[0];\n            if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n                this.dropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(inputs),\n                    rate: this.dropout,\n                    training,\n                    count: 4\n                });\n            }\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n                this.recurrentDropoutMask == null) {\n                this.recurrentDropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(hTMinus1),\n                    rate: this.recurrentDropout,\n                    training,\n                    count: 4\n                });\n            }\n            const dpMask = this.dropoutMask;\n            const recDpMask = this.recurrentDropoutMask;\n            // Note: For superior performance, TensorFlow.js always uses\n            // implementation 2 regardless of the actual value of\n            // config.implementation.\n            let i;\n            let f;\n            let c;\n            let o;\n            if (0 < this.dropout && this.dropout < 1) {\n                inputs = tfc.mul(inputs, dpMask[0]);\n            }\n            let z = K.dot(inputs, this.kernel.read());\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n                hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n            }\n            z = tfc.add(z, K.dot(hTMinus1, this.recurrentKernel.read()));\n            if (this.useBias) {\n                z = K.biasAdd(z, this.bias.read());\n            }\n            const [z0, z1, z2, z3] = tfc.split(z, 4, z.rank - 1);\n            i = this.recurrentActivation.apply(z0);\n            f = this.recurrentActivation.apply(z1);\n            c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, this.activation.apply(z2)));\n            o = this.recurrentActivation.apply(z3);\n            const h = tfc.mul(o, this.activation.apply(c));\n            // TODO(cais): Add use_learning_phase flag properly.\n            return [h, h, c];\n        });\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = {\n            units: this.units,\n            activation: serializeActivation(this.activation),\n            recurrentActivation: serializeActivation(this.recurrentActivation),\n            useBias: this.useBias,\n            kernelInitializer: serializeInitializer(this.kernelInitializer),\n            recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n            biasInitializer: serializeInitializer(this.biasInitializer),\n            unitForgetBias: this.unitForgetBias,\n            kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n            recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n            biasRegularizer: serializeRegularizer(this.biasRegularizer),\n            activityRegularizer: serializeRegularizer(this.activityRegularizer),\n            kernelConstraint: serializeConstraint(this.kernelConstraint),\n            recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n            biasConstraint: serializeConstraint(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n            implementation: this.implementation,\n        };\n        return Object.assign({}, baseConfig, config);\n    }\n}\n/** @nocollapse */\nLSTMCell.className = 'LSTMCell';\nserialization.registerClass(LSTMCell);\nexport class LSTM extends RNN {\n    constructor(args) {\n        if (args.implementation === 0) {\n            console.warn('`implementation=0` has been deprecated, and now defaults to ' +\n                '`implementation=1`. Please update your layer call.');\n        }\n        args.cell = new LSTMCell(args);\n        super(args);\n        // TODO(cais): Add activityRegularizer.\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.cell.dropoutMask != null) {\n                tfc.dispose(this.cell.dropoutMask);\n                this.cell.dropoutMask = null;\n            }\n            if (this.cell.recurrentDropoutMask != null) {\n                tfc.dispose(this.cell.recurrentDropoutMask);\n                this.cell.recurrentDropoutMask = null;\n            }\n            const mask = kwargs == null ? null : kwargs['mask'];\n            const training = kwargs == null ? null : kwargs['training'];\n            const initialState = kwargs == null ? null : kwargs['initialState'];\n            return super.call(inputs, { mask, training, initialState });\n        });\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        if (config['implmentation'] === 0) {\n            config['implementation'] = 1;\n        }\n        return new cls(config);\n    }\n}\n/** @nocollapse */\nLSTM.className = 'LSTM';\nserialization.registerClass(LSTM);\nexport class StackedRNNCells extends RNNCell {\n    constructor(args) {\n        super(args);\n        this.cells = args.cells;\n    }\n    get stateSize() {\n        // States are a flat list in reverse order of the cell stack.\n        // This allows perserving the requirement `stack.statesize[0] ===\n        // outputDim`. E.g., states of a 2-layer LSTM would be `[h2, c2, h1, c1]`,\n        // assuming one LSTM has states `[h, c]`.\n        const stateSize = [];\n        for (const cell of this.cells.slice().reverse()) {\n            if (Array.isArray(cell.stateSize)) {\n                stateSize.push(...cell.stateSize);\n            }\n            else {\n                stateSize.push(cell.stateSize);\n            }\n        }\n        return stateSize;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            inputs = inputs;\n            let states = inputs.slice(1);\n            // Recover per-cell states.\n            const nestedStates = [];\n            for (const cell of this.cells.slice().reverse()) {\n                if (Array.isArray(cell.stateSize)) {\n                    nestedStates.push(states.splice(0, cell.stateSize.length));\n                }\n                else {\n                    nestedStates.push(states.splice(0, 1));\n                }\n            }\n            nestedStates.reverse();\n            // Call the cells in order and store the returned states.\n            const newNestedStates = [];\n            let callInputs;\n            for (let i = 0; i < this.cells.length; ++i) {\n                const cell = this.cells[i];\n                states = nestedStates[i];\n                // TODO(cais): Take care of constants.\n                if (i === 0) {\n                    callInputs = [inputs[0]].concat(states);\n                }\n                else {\n                    callInputs = [callInputs[0]].concat(states);\n                }\n                callInputs = cell.call(callInputs, kwargs);\n                newNestedStates.push(callInputs.slice(1));\n            }\n            // Format the new states as a flat list in reverse cell order.\n            states = [];\n            for (const cellStates of newNestedStates.slice().reverse()) {\n                states.push(...cellStates);\n            }\n            return [callInputs[0]].concat(states);\n        });\n    }\n    build(inputShape) {\n        if (isArrayOfShapes(inputShape)) {\n            // TODO(cais): Take care of input constants.\n            // const constantShape = inputShape.slice(1);\n            inputShape = inputShape[0];\n        }\n        inputShape = inputShape;\n        let outputDim;\n        this.cells.forEach((cell, i) => {\n            nameScope(`RNNCell_${i}`, () => {\n                // TODO(cais): Take care of input constants.\n                cell.build(inputShape);\n                if (Array.isArray(cell.stateSize)) {\n                    outputDim = cell.stateSize[0];\n                }\n                else {\n                    outputDim = cell.stateSize;\n                }\n                inputShape = [inputShape[0], outputDim];\n            });\n        });\n        this.built = true;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const getCellConfig = (cell) => {\n            return {\n                'className': cell.getClassName(),\n                'config': cell.getConfig(),\n            };\n        };\n        const cellConfigs = this.cells.map(getCellConfig);\n        const config = { 'cells': cellConfigs };\n        return Object.assign({}, baseConfig, config);\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config, customObjects = {}) {\n        const cells = [];\n        for (const cellConfig of config['cells']) {\n            cells.push(deserialize(cellConfig, customObjects));\n        }\n        return new cls({ cells });\n    }\n    get trainableWeights() {\n        if (!this.trainable) {\n            return [];\n        }\n        const weights = [];\n        for (const cell of this.cells) {\n            weights.push(...cell.trainableWeights);\n        }\n        return weights;\n    }\n    get nonTrainableWeights() {\n        const weights = [];\n        for (const cell of this.cells) {\n            weights.push(...cell.nonTrainableWeights);\n        }\n        if (!this.trainable) {\n            const trainableWeights = [];\n            for (const cell of this.cells) {\n                trainableWeights.push(...cell.trainableWeights);\n            }\n            return trainableWeights.concat(weights);\n        }\n        return weights;\n    }\n    /**\n     * Retrieve the weights of a the model.\n     *\n     * @returns A flat `Array` of `tf.Tensor`s.\n     */\n    getWeights() {\n        const weights = [];\n        for (const cell of this.cells) {\n            weights.push(...cell.weights);\n        }\n        return batchGetValue(weights);\n    }\n    /**\n     * Set the weights of the model.\n     *\n     * @param weights An `Array` of `tf.Tensor`s with shapes and types matching\n     *     the output of `getWeights()`.\n     */\n    setWeights(weights) {\n        const tuples = [];\n        for (const cell of this.cells) {\n            const numParams = cell.weights.length;\n            const inputWeights = weights.splice(numParams);\n            for (let i = 0; i < cell.weights.length; ++i) {\n                tuples.push([cell.weights[i], inputWeights[i]]);\n            }\n        }\n        batchSetValue(tuples);\n    }\n}\n/** @nocollapse */\nStackedRNNCells.className = 'StackedRNNCells';\nserialization.registerClass(StackedRNNCells);\nexport function generateDropoutMask(args) {\n    const { ones, rate, training = false, count = 1 } = args;\n    const droppedInputs = () => K.dropout(ones(), rate);\n    const createMask = () => K.inTrainPhase(droppedInputs, ones, training);\n    // just in case count is provided with null or undefined\n    if (!count || count <= 1) {\n        return tfc.keep(createMask().clone());\n    }\n    const masks = Array(count).fill(undefined).map(createMask);\n    return masks.map(m => tfc.keep(m.clone()));\n}\n//# sourceMappingURL=recurrent.js.map"]},"metadata":{},"sourceType":"module"}
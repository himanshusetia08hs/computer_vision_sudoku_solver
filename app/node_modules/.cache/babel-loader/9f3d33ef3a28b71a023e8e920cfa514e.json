{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { env, util } from '@tensorflow/tfjs-core';\nimport { Im2ColPackedProgram } from '../im2col_packed_gpu';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\nimport { batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD } from './BatchMatMul_impl';\nimport { identity } from './Identity';\nimport { reshape } from './Reshape';\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nexport function conv2dByMatMul(_ref) {\n  let {\n    x,\n    filter,\n    convInfo,\n    backend,\n    bias = null,\n    preluActivationWeights = null,\n    leakyreluAlpha = 0,\n    activation = null\n  } = _ref;\n  // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n  // result from 2D to 4D.\n  const xShape = x.shape;\n  const xTexData = backend.texData.get(x.dataId);\n  const sharedMatMulDim = convInfo.inChannels;\n  const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n  const outerShapeFilter = convInfo.outChannels;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = false;\n  const transposeB = false;\n  let out;\n  const intermediates = [];\n  // TODO: Once reduction ops are packed, batchMatMul will always be packed\n  // and we can remove this condition.\n  const batchMatMulWillBeUnpacked = (outerShapeX === 1 || outerShapeFilter === 1) && sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD;\n  const reshapeWillBeExpensive = xShape[2] % 2 !== 0 && !!xTexData.isPacked;\n  if (batchMatMulWillBeUnpacked || !env().getBool('WEBGL_LAZILY_UNPACK') || !env().getBool('WEBGL_PACK_BINARY_OPERATIONS') || !reshapeWillBeExpensive) {\n    const targetShape = isChannelsLast ? xShape[0] * xShape[1] * xShape[2] : xShape[0] * xShape[2] * xShape[3];\n    const xReshaped = reshape({\n      inputs: {\n        x\n      },\n      backend,\n      attrs: {\n        shape: [1, targetShape, convInfo.inChannels]\n      }\n    });\n    const filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    const result = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      transposeA,\n      transposeB,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n    out = reshape({\n      inputs: {\n        x: result\n      },\n      backend,\n      attrs: {\n        shape: convInfo.outShape\n      }\n    });\n    intermediates.push(xReshaped);\n    intermediates.push(filterReshaped);\n    intermediates.push(result);\n  } else {\n    // Following optimization is specific to packed |x| with odd row count\n    // (For example, in channelLast mode, 'row count' refers to x.shape[2]):\n    // we avoid expensive packed 2x2 reshape by padding row count to next,\n    // even number. When x.shape[2] is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for even x.shape[2] + 1. We make the odd-rows tensor to look like\n    // even-rows tensor before the operation and, after the batchMatMul,\n    // fix the even-rows result to have odd number of rows.\n    const targetShape = isChannelsLast ? xShape[0] * xShape[1] * (xShape[2] + 1) : xShape[0] * xShape[2] * (xShape[3] + 1);\n    const xReshaped = {\n      dataId: x.dataId,\n      shape: [1, targetShape, convInfo.inChannels],\n      dtype: x.dtype\n    };\n    // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing row count, after batchMatMul->...->compileProgram leads to\n    // invalid row count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even row count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n    const originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape), () => `packed reshape ${xTexData.shape} to ${xReshaped.shape} isn't free`);\n    const filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    intermediates.push(filterReshaped);\n    const pointwiseConv = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      backend,\n      transposeA,\n      transposeB,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n    const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n    util.assert(pointwiseConvTexData.isPacked, () => 'batchMatMul result is expected to be packed');\n    // Restore the input shape to original.\n    xTexData.shape = originalXTexDataShape;\n    // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n    pointwiseConvTexData.shape = convInfo.outShape;\n    out = identity({\n      inputs: {\n        x: pointwiseConv\n      },\n      backend\n    });\n    out.shape = convInfo.outShape;\n    intermediates.push(pointwiseConv);\n  }\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n  return out;\n}\n// Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\nexport function conv2dWithIm2Row(_ref2) {\n  let {\n    x,\n    filter,\n    convInfo,\n    backend,\n    bias = null,\n    preluActivationWeights = null,\n    leakyreluAlpha = 0,\n    activation = null\n  } = _ref2;\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // column of a new matrix with shape [filterWidth * filterHeight *\n  // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n  // output channel forms a row of a new matrix with shape [outChannels,\n  // filterWidth * filterHeight * inChannels]. The convolution is then\n  // computed by multiplying these matrices and reshaping the result.\n  const {\n    filterWidth,\n    filterHeight,\n    inChannels,\n    outWidth,\n    outHeight,\n    dataFormat\n  } = convInfo;\n  const isChannelsLast = dataFormat === 'channelsLast';\n  const sharedDim = filterWidth * filterHeight * inChannels;\n  const numCols = outHeight * outWidth;\n  const x2ColShape = [sharedDim, numCols];\n  const transposeA = true;\n  const transposeB = false;\n  const intermediates = [];\n  const xSqueezed = reshape({\n    inputs: {\n      x\n    },\n    backend,\n    attrs: {\n      shape: x.shape.slice(1)\n    }\n  });\n  const w2Row = reshape({\n    inputs: {\n      x: filter\n    },\n    backend,\n    attrs: {\n      shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim]\n    }\n  });\n  intermediates.push(xSqueezed);\n  intermediates.push(w2Row);\n  const im2ColProgram = new Im2ColPackedProgram(x2ColShape, xSqueezed.shape, convInfo);\n  const im2Col = backend.runWebGLProgram(im2ColProgram, [xSqueezed], 'float32');\n  const im2ColReshaped = reshape({\n    inputs: {\n      x: im2Col\n    },\n    backend,\n    attrs: {\n      shape: [1, x2ColShape[0], x2ColShape[1]]\n    }\n  });\n  intermediates.push(im2Col);\n  intermediates.push(im2ColReshaped);\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n  const matmulProgram = new MatMulPackedProgram(im2ColReshaped.shape, w2Row.shape, [1, numCols, convInfo.outChannels], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n  const inputs = [im2ColReshaped, w2Row];\n  if (bias) {\n    inputs.push(bias);\n  }\n  if (hasPreluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n  if (hasLeakyreluAlpha) {\n    const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n    inputs.push($leakyreluAlpha);\n    intermediates.push($leakyreluAlpha);\n  }\n  const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n  const outShape = isChannelsLast ? [1, outHeight, outWidth, convInfo.outChannels] : [1, convInfo.outChannels, outHeight, outWidth];\n  const out = reshape({\n    inputs: {\n      x: product\n    },\n    backend,\n    attrs: {\n      shape: outShape\n    }\n  });\n  intermediates.push(product);\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n  return out;\n}","map":{"version":3,"sources":["../../src/kernels/Conv2D_impl.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAsB,GAAG,EAAc,IAAI,QAAO,uBAAuB;AAGzE,SAAQ,mBAAmB,QAAO,sBAAsB;AACxD,SAAQ,4BAA4B,QAAO,oCAAoC;AAC/E,SAAQ,mBAAmB,QAAO,sBAAsB;AACxD,OAAO,KAAK,UAAU,MAAM,eAAe;AAE3C,SAAQ,eAAe,EAAE,2BAA2B,QAAO,oBAAoB;AAC/E,SAAQ,QAAQ,QAAO,YAAY;AACnC,SAAQ,OAAO,QAAO,WAAW;AAajC;AACA;AACA;AACA,OAAM,SAAU,cAAc,OASf;EAAA,IATgB;IAC7B,CAAC;IACD,MAAM;IACN,QAAQ;IACR,OAAO;IACP,IAAI,GAAG,IAAI;IACX,sBAAsB,GAAG,IAAI;IAC7B,cAAc,GAAG,CAAC;IAClB,UAAU,GAAG;EAAI,CACJ;EACb;EACA;EACA,MAAM,MAAM,GAAG,CAAC,CAAC,KAAK;EACtB,MAAM,QAAQ,GAAG,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,MAAM,CAAC;EAC9C,MAAM,eAAe,GAAG,QAAQ,CAAC,UAAU;EAC3C,MAAM,WAAW,GAAG,MAAM,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC;EACrD,MAAM,gBAAgB,GAAG,QAAQ,CAAC,WAAW;EAC7C,MAAM,cAAc,GAAG,QAAQ,CAAC,UAAU,KAAK,cAAc;EAC7D,MAAM,UAAU,GAAG,KAAK;EACxB,MAAM,UAAU,GAAG,KAAK;EAExB,IAAI,GAAe;EACnB,MAAM,aAAa,GAAiB,EAAE;EAEtC;EACA;EACA,MAAM,yBAAyB,GAC3B,CAAC,WAAW,KAAK,CAAC,IAAI,gBAAgB,KAAK,CAAC,KAC5C,eAAe,GAAG,2BAA2B;EACjD,MAAM,sBAAsB,GAAG,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,QAAQ,CAAC,QAAQ;EAEzE,IAAI,yBAAyB,IAAI,CAAC,GAAG,EAAE,CAAC,OAAO,CAAC,qBAAqB,CAAC,IAClE,CAAC,GAAG,EAAE,CAAC,OAAO,CAAC,8BAA8B,CAAC,IAC9C,CAAC,sBAAsB,EAAE;IAC3B,MAAM,WAAW,GAAG,cAAc,GAAG,MAAM,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,GACjC,MAAM,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC;IACtE,MAAM,SAAS,GAAG,OAAO,CAAC;MACxB,MAAM,EAAE;QAAC;MAAC,CAAC;MACX,OAAO;MACP,KAAK,EAAE;QAAC,KAAK,EAAE,CAAC,CAAC,EAAE,WAAW,EAAE,QAAQ,CAAC,UAAU;MAAC;KACrD,CAAC;IACF,MAAM,cAAc,GAAG,OAAO,CAAC;MAC7B,MAAM,EAAE;QAAC,CAAC,EAAE;MAAM,CAAC;MACnB,OAAO;MACP,KAAK,EAAE;QAAC,KAAK,EAAE,CAAC,CAAC,EAAE,QAAQ,CAAC,UAAU,EAAE,QAAQ,CAAC,WAAW;MAAC;KAC9D,CAAC;IACF,MAAM,MAAM,GAAG,eAAe,CAAC;MAC7B,CAAC,EAAE,SAAS;MACZ,CAAC,EAAE,cAAc;MACjB,UAAU;MACV,UAAU;MACV,OAAO;MACP,IAAI;MACJ,UAAU;MACV,sBAAsB;MACtB;KACD,CAAC;IAEF,GAAG,GAAG,OAAO,CACT;MAAC,MAAM,EAAE;QAAC,CAAC,EAAE;MAAM,CAAC;MAAE,OAAO;MAAE,KAAK,EAAE;QAAC,KAAK,EAAE,QAAQ,CAAC;MAAQ;IAAC,CAAC,CAAC;IAEtE,aAAa,CAAC,IAAI,CAAC,SAAS,CAAC;IAC7B,aAAa,CAAC,IAAI,CAAC,cAAc,CAAC;IAClC,aAAa,CAAC,IAAI,CAAC,MAAM,CAAC;GAC3B,MAAM;IACL;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA,MAAM,WAAW,GAAG,cAAc,GAC9B,MAAM,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,IAAI,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,GACvC,MAAM,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,IAAI,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC;IAC3C,MAAM,SAAS,GAAe;MAC5B,MAAM,EAAE,CAAC,CAAC,MAAM;MAChB,KAAK,EAAE,CAAC,CAAC,EAAE,WAAW,EAAE,QAAQ,CAAC,UAAU,CAAC;MAC5C,KAAK,EAAE,CAAC,CAAC;KACV;IACD;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA,MAAM,qBAAqB,GAAG,QAAQ,CAAC,KAAK;IAC5C,QAAQ,CAAC,KAAK,GAAG,QAAQ,CAAC,KAAK,CAAC,KAAK,EAAE;IACvC,QAAQ,CAAC,KAAK,CAAC,QAAQ,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,EAAE;IAC3C,IAAI,CAAC,MAAM,CACP,UAAU,CAAC,aAAa,CAAC,QAAQ,CAAC,KAAK,EAAE,SAAS,CAAC,KAAK,CAAC,EACzD,MAAM,kBAAkB,QAAQ,CAAC,KAAK,OAClC,SAAS,CAAC,KAAK,aAAa,CAAC;IACrC,MAAM,cAAc,GAAG,OAAO,CAAC;MAC7B,MAAM,EAAE;QAAC,CAAC,EAAE;MAAM,CAAC;MACnB,OAAO;MACP,KAAK,EAAE;QAAC,KAAK,EAAE,CAAC,CAAC,EAAE,QAAQ,CAAC,UAAU,EAAE,QAAQ,CAAC,WAAW;MAAC;KAC9D,CAAC;IACF,aAAa,CAAC,IAAI,CAAC,cAAc,CAAC;IAClC,MAAM,aAAa,GAAG,eAAe,CAAC;MACpC,CAAC,EAAE,SAAS;MACZ,CAAC,EAAE,cAAc;MACjB,OAAO;MACP,UAAU;MACV,UAAU;MACV,IAAI;MACJ,UAAU;MACV,sBAAsB;MACtB;KACD,CAAC;IAEF,MAAM,oBAAoB,GAAG,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,aAAa,CAAC,MAAM,CAAC;IACtE,IAAI,CAAC,MAAM,CACP,oBAAoB,CAAC,QAAQ,EAC7B,MAAM,6CAA6C,CAAC;IACxD;IACA,QAAQ,CAAC,KAAK,GAAG,qBAAqB;IACtC;IACA;IACA,oBAAoB,CAAC,KAAK,GAAG,QAAQ,CAAC,QAAQ;IAE9C,GAAG,GAAG,QAAQ,CAAC;MAAC,MAAM,EAAE;QAAC,CAAC,EAAE;MAAa,CAAC;MAAE;IAAO,CAAC,CAAC;IACrD,GAAG,CAAC,KAAK,GAAG,QAAQ,CAAC,QAAQ;IAE7B,aAAa,CAAC,IAAI,CAAC,aAAa,CAAC;EAClC;EAED,KAAK,MAAM,CAAC,IAAI,aAAa,EAAE;IAC7B,OAAO,CAAC,6BAA6B,CAAC,CAAC,CAAC;EACzC;EAED,OAAO,GAAG;AACZ;AAEA;AACA;AACA,OAAM,SAAU,gBAAgB,QASjB;EAAA,IATkB;IAC/B,CAAC;IACD,MAAM;IACN,QAAQ;IACR,OAAO;IACP,IAAI,GAAG,IAAI;IACX,sBAAsB,GAAG,IAAI;IAC7B,cAAc,GAAG,CAAC;IAClB,UAAU,GAAG;EAAI,CACJ;EACb;EACA;EACA;EACA;EACA;EACA;EACA,MAAM;IACJ,WAAW;IACX,YAAY;IACZ,UAAU;IACV,QAAQ;IACR,SAAS;IACT;EAAU,CACX,GAAG,QAAQ;EAEZ,MAAM,cAAc,GAAG,UAAU,KAAK,cAAc;EAEpD,MAAM,SAAS,GAAG,WAAW,GAAG,YAAY,GAAG,UAAU;EACzD,MAAM,OAAO,GAAG,SAAS,GAAG,QAAQ;EACpC,MAAM,UAAU,GAAG,CAAC,SAAS,EAAE,OAAO,CAAC;EACvC,MAAM,UAAU,GAAG,IAAI;EACvB,MAAM,UAAU,GAAG,KAAK;EAExB,MAAM,aAAa,GAAiB,EAAE;EAEtC,MAAM,SAAS,GACX,OAAO,CAAC;IAAC,MAAM,EAAE;MAAC;IAAC,CAAC;IAAE,OAAO;IAAE,KAAK,EAAE;MAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC;IAAC;EAAC,CAAC,CAAC;EACrE,MAAM,KAAK,GAAG,OAAO,CAAC;IACpB,MAAM,EAAE;MAAC,CAAC,EAAE;IAAM,CAAC;IACnB,OAAO;IACP,KAAK,EAAE;MAAC,KAAK,EAAE,CAAC,CAAC,EAAE,SAAS,EAAE,IAAI,CAAC,aAAa,CAAC,MAAM,CAAC,KAAK,CAAC,GAAG,SAAS;IAAC;GAC5E,CAAC;EAEF,aAAa,CAAC,IAAI,CAAC,SAAS,CAAC;EAC7B,aAAa,CAAC,IAAI,CAAC,KAAK,CAAC;EAEzB,MAAM,aAAa,GACf,IAAI,mBAAmB,CAAC,UAAU,EAAE,SAAS,CAAC,KAAK,EAAE,QAAQ,CAAC;EAClE,MAAM,MAAM,GAAG,OAAO,CAAC,eAAe,CAAC,aAAa,EAAE,CAAC,SAAS,CAAC,EAAE,SAAS,CAAC;EAC7E,MAAM,cAAc,GAAG,OAAO,CAAC;IAC7B,MAAM,EAAE;MAAC,CAAC,EAAE;IAAM,CAAC;IACnB,OAAO;IACP,KAAK,EAAE;MAAC,KAAK,EAAE,CAAC,CAAC,EAAE,UAAU,CAAC,CAAC,CAAC,EAAE,UAAU,CAAC,CAAC,CAAC;IAAC;GACjD,CAAC;EAEF,aAAa,CAAC,IAAI,CAAC,MAAM,CAAC;EAC1B,aAAa,CAAC,IAAI,CAAC,cAAc,CAAC;EAElC,MAAM,OAAO,GAAG,IAAI,IAAI,IAAI;EAC5B,MAAM,yBAAyB,GAAG,sBAAsB,IAAI,IAAI;EAChE,MAAM,iBAAiB,GAAG,UAAU,KAAK,WAAW;EACpD,MAAM,eAAe,GACjB,UAAU,GAAG,4BAA4B,CAAC,UAAU,EAAE,IAAI,CAAC,GAAG,IAAI;EACtE,MAAM,aAAa,GAAG,IAAI,mBAAmB,CACzC,cAAc,CAAC,KAAiC,EAChD,KAAK,CAAC,KAAiC,EACvC,CAAC,CAAC,EAAE,OAAO,EAAE,QAAQ,CAAC,WAAW,CAAC,EAAE,UAAU,EAAE,UAAU,EAAE,OAAO,EACnE,eAAe,EAAE,yBAAyB,EAAE,iBAAiB,CAAC;EAClE,MAAM,MAAM,GAAiB,CAAC,cAAc,EAAE,KAAK,CAAC;EACpD,IAAI,IAAI,EAAE;IACR,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC;EAClB;EACD,IAAI,yBAAyB,EAAE;IAC7B,MAAM,CAAC,IAAI,CAAC,sBAAsB,CAAC;EACpC;EACD,IAAI,iBAAiB,EAAE;IACrB,MAAM,eAAe,GAAG,OAAO,CAAC,cAAc,CAC1C,EAAE,EAAE,SAAS,EACb,IAAI,CAAC,iBAAiB,CAAC,cAAiC,EAAE,SAAS,CAAC,CAAC;IACzE,MAAM,CAAC,IAAI,CAAC,eAAe,CAAC;IAC5B,aAAa,CAAC,IAAI,CAAC,eAAe,CAAC;EACpC;EACD,MAAM,OAAO,GAAG,OAAO,CAAC,eAAe,CAAC,aAAa,EAAE,MAAM,EAAE,SAAS,CAAC;EAEzE,MAAM,QAAQ,GAAG,cAAc,GAC3B,CAAC,CAAC,EAAE,SAAS,EAAE,QAAQ,EAAE,QAAQ,CAAC,WAAW,CAAC,GAC9C,CAAC,CAAC,EAAE,QAAQ,CAAC,WAAW,EAAE,SAAS,EAAE,QAAQ,CAAC;EAClD,MAAM,GAAG,GACL,OAAO,CAAC;IAAC,MAAM,EAAE;MAAC,CAAC,EAAE;IAAO,CAAC;IAAE,OAAO;IAAE,KAAK,EAAE;MAAC,KAAK,EAAE;IAAQ;EAAC,CAAC,CAAC;EAEtE,aAAa,CAAC,IAAI,CAAC,OAAO,CAAC;EAC3B,KAAK,MAAM,CAAC,IAAI,aAAa,EAAE;IAC7B,OAAO,CAAC,6BAA6B,CAAC,CAAC,CAAC;EACzC;EAED,OAAO,GAAG;AACZ","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { env, util } from '@tensorflow/tfjs-core';\nimport { Im2ColPackedProgram } from '../im2col_packed_gpu';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\nimport { batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD } from './BatchMatMul_impl';\nimport { identity } from './Identity';\nimport { reshape } from './Reshape';\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nexport function conv2dByMatMul({ x, filter, convInfo, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {\n    // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n    // result from 2D to 4D.\n    const xShape = x.shape;\n    const xTexData = backend.texData.get(x.dataId);\n    const sharedMatMulDim = convInfo.inChannels;\n    const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n    const outerShapeFilter = convInfo.outChannels;\n    const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n    const transposeA = false;\n    const transposeB = false;\n    let out;\n    const intermediates = [];\n    // TODO: Once reduction ops are packed, batchMatMul will always be packed\n    // and we can remove this condition.\n    const batchMatMulWillBeUnpacked = (outerShapeX === 1 || outerShapeFilter === 1) &&\n        sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD;\n    const reshapeWillBeExpensive = xShape[2] % 2 !== 0 && !!xTexData.isPacked;\n    if (batchMatMulWillBeUnpacked || !env().getBool('WEBGL_LAZILY_UNPACK') ||\n        !env().getBool('WEBGL_PACK_BINARY_OPERATIONS') ||\n        !reshapeWillBeExpensive) {\n        const targetShape = isChannelsLast ? xShape[0] * xShape[1] * xShape[2] :\n            xShape[0] * xShape[2] * xShape[3];\n        const xReshaped = reshape({\n            inputs: { x },\n            backend,\n            attrs: { shape: [1, targetShape, convInfo.inChannels] }\n        });\n        const filterReshaped = reshape({\n            inputs: { x: filter },\n            backend,\n            attrs: { shape: [1, convInfo.inChannels, convInfo.outChannels] }\n        });\n        const result = batchMatMulImpl({\n            a: xReshaped,\n            b: filterReshaped,\n            transposeA,\n            transposeB,\n            backend,\n            bias,\n            activation,\n            preluActivationWeights,\n            leakyreluAlpha\n        });\n        out = reshape({ inputs: { x: result }, backend, attrs: { shape: convInfo.outShape } });\n        intermediates.push(xReshaped);\n        intermediates.push(filterReshaped);\n        intermediates.push(result);\n    }\n    else {\n        // Following optimization is specific to packed |x| with odd row count\n        // (For example, in channelLast mode, 'row count' refers to x.shape[2]):\n        // we avoid expensive packed 2x2 reshape by padding row count to next,\n        // even number. When x.shape[2] is odd, the result of packed batchMatMul is\n        // the same (has the same texture layout and and values in the texture) as\n        // it is for even x.shape[2] + 1. We make the odd-rows tensor to look like\n        // even-rows tensor before the operation and, after the batchMatMul,\n        // fix the even-rows result to have odd number of rows.\n        const targetShape = isChannelsLast ?\n            xShape[0] * xShape[1] * (xShape[2] + 1) :\n            xShape[0] * xShape[2] * (xShape[3] + 1);\n        const xReshaped = {\n            dataId: x.dataId,\n            shape: [1, targetShape, convInfo.inChannels],\n            dtype: x.dtype\n        };\n        // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n        // Decrementing row count, after batchMatMul->...->compileProgram leads to\n        // invalid row count within the reference in GPGPUBinary.inShapeInfos.\n        // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n        // in compileProgram method, but that would affect compilation of all\n        // programs - instead, provide a copy here, with even row count, before\n        // calling batchMatMul->...->compileProgram and after that, the original\n        // xTexData.shape is restored.\n        const originalXTexDataShape = xTexData.shape;\n        xTexData.shape = xTexData.shape.slice();\n        xTexData.shape[xTexData.shape.length - 2]++;\n        util.assert(webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape), () => `packed reshape ${xTexData.shape} to ${xReshaped.shape} isn't free`);\n        const filterReshaped = reshape({\n            inputs: { x: filter },\n            backend,\n            attrs: { shape: [1, convInfo.inChannels, convInfo.outChannels] }\n        });\n        intermediates.push(filterReshaped);\n        const pointwiseConv = batchMatMulImpl({\n            a: xReshaped,\n            b: filterReshaped,\n            backend,\n            transposeA,\n            transposeB,\n            bias,\n            activation,\n            preluActivationWeights,\n            leakyreluAlpha\n        });\n        const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n        util.assert(pointwiseConvTexData.isPacked, () => 'batchMatMul result is expected to be packed');\n        // Restore the input shape to original.\n        xTexData.shape = originalXTexDataShape;\n        // Set the output shape - there is no need for expensive reshape as data\n        // layout is already correct.\n        pointwiseConvTexData.shape = convInfo.outShape;\n        out = identity({ inputs: { x: pointwiseConv }, backend });\n        out.shape = convInfo.outShape;\n        intermediates.push(pointwiseConv);\n    }\n    for (const i of intermediates) {\n        backend.disposeIntermediateTensorInfo(i);\n    }\n    return out;\n}\n// Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\nexport function conv2dWithIm2Row({ x, filter, convInfo, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {\n    // Rearranges conv2d input so each block to be convolved over forms the\n    // column of a new matrix with shape [filterWidth * filterHeight *\n    // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n    // output channel forms a row of a new matrix with shape [outChannels,\n    // filterWidth * filterHeight * inChannels]. The convolution is then\n    // computed by multiplying these matrices and reshaping the result.\n    const { filterWidth, filterHeight, inChannels, outWidth, outHeight, dataFormat } = convInfo;\n    const isChannelsLast = dataFormat === 'channelsLast';\n    const sharedDim = filterWidth * filterHeight * inChannels;\n    const numCols = outHeight * outWidth;\n    const x2ColShape = [sharedDim, numCols];\n    const transposeA = true;\n    const transposeB = false;\n    const intermediates = [];\n    const xSqueezed = reshape({ inputs: { x }, backend, attrs: { shape: x.shape.slice(1) } });\n    const w2Row = reshape({\n        inputs: { x: filter },\n        backend,\n        attrs: { shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim] }\n    });\n    intermediates.push(xSqueezed);\n    intermediates.push(w2Row);\n    const im2ColProgram = new Im2ColPackedProgram(x2ColShape, xSqueezed.shape, convInfo);\n    const im2Col = backend.runWebGLProgram(im2ColProgram, [xSqueezed], 'float32');\n    const im2ColReshaped = reshape({\n        inputs: { x: im2Col },\n        backend,\n        attrs: { shape: [1, x2ColShape[0], x2ColShape[1]] }\n    });\n    intermediates.push(im2Col);\n    intermediates.push(im2ColReshaped);\n    const hasBias = bias != null;\n    const hasPreluActivationWeights = preluActivationWeights != null;\n    const hasLeakyreluAlpha = activation === 'leakyrelu';\n    const fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n    const matmulProgram = new MatMulPackedProgram(im2ColReshaped.shape, w2Row.shape, [1, numCols, convInfo.outChannels], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n    const inputs = [im2ColReshaped, w2Row];\n    if (bias) {\n        inputs.push(bias);\n    }\n    if (hasPreluActivationWeights) {\n        inputs.push(preluActivationWeights);\n    }\n    if (hasLeakyreluAlpha) {\n        const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n        inputs.push($leakyreluAlpha);\n        intermediates.push($leakyreluAlpha);\n    }\n    const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n    const outShape = isChannelsLast ?\n        [1, outHeight, outWidth, convInfo.outChannels] :\n        [1, convInfo.outChannels, outHeight, outWidth];\n    const out = reshape({ inputs: { x: product }, backend, attrs: { shape: outShape } });\n    intermediates.push(product);\n    for (const i of intermediates) {\n        backend.disposeIntermediateTensorInfo(i);\n    }\n    return out;\n}\n//# sourceMappingURL=Conv2D_impl.js.map"]},"metadata":{},"sourceType":"module"}
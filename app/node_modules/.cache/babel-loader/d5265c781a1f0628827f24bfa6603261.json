{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedDepthwiseConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport * as conv_util from '../conv_util';\nimport { depthwiseConv2d as unfusedDepthwiseConv2d } from '../depthwise_conv2d';\nimport { depthwiseConv2dNativeBackpropFilter } from '../depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from '../depthwise_conv2d_native_backprop_input';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedDepthwiseConv2d_(_ref) {\n  let {\n    x,\n    filter,\n    strides,\n    pad,\n    dataFormat = 'NHWC',\n    dilations = [1, 1],\n    dimRoundingMode,\n    bias,\n    activation = 'linear',\n    preluActivationWeights,\n    leakyreluAlpha\n  } = _ref;\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedDepthwiseConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n    if (bias != null) {\n      result = add(result, bias);\n    }\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n  const $x = convertToTensor(x, 'x', 'depthwiseConv2d');\n  const $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d');\n  let x4D = $x;\n  let reshapedTo4D = false;\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n  util.assert(x4D.rank === 4, () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` + `rank ${x4D.rank}.`);\n  util.assert($filter.rank === 4, () => `Error in fused depthwiseConv2d: filter must be rank 4, ` + `but got rank ${$filter.rank}.`);\n  util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in fused depthwiseConv2d: number of input channels ` + `(${x4D.shape[3]}) must match the inChannels dimension in ` + `filter ${$filter.shape[2]}.`);\n  if (dilations == null) {\n    dilations = [1, 1];\n  }\n  util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in fused depthwiseConv2d: Either strides or dilations must ' + `be 1. Got strides ${strides} and dilations '${dilations}'`);\n  if (dimRoundingMode != null) {\n    util.assert(util.isInt(pad), () => `Error in fused depthwiseConv2d: pad must be an integer when ` + `using dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n  }\n  const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode, true /* depthwise */);\n  let $bias;\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n  let $preluActivationWeights;\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n  }\n  const grad = (dy, saved) => {\n    util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' + `greater than 1 are not yet supported. Got dilations ` + `'${dilations}'`);\n    const [$filter, x4D, y, bias] = saved;\n    const dyActivation = getFusedDyActivation(dy, y, activation);\n    const xDer = depthwiseConv2dNativeBackpropInput(x4D.shape, dyActivation, $filter, strides, pad, dilations, dimRoundingMode);\n    const filterDer = depthwiseConv2dNativeBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad, dilations, dimRoundingMode);\n    if (bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [xDer, filterDer, biasDer];\n    }\n    return [xDer, filterDer];\n  };\n  const inputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  };\n  // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n  if (bias == null) {\n    const customOp = customGrad((x4D, filter, save) => {\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n      save([filter, x4D, res]);\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOp(x4D, $filter);\n  } else {\n    const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n      save([filter, x4D, res, bias]);\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(x4D, $filter, $bias);\n  }\n}\nexport const depthwiseConv2d = op({\n  fusedDepthwiseConv2d_\n});","map":{"version":3,"sources":["../../../src/ops/fused/depthwise_conv2d.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAM,QAAO,cAAc;AACnC,SAAQ,UAAU,QAAO,iBAAiB;AAC1C,SAAQ,oBAAoB,QAA8D,oBAAoB;AAI9G,SAAQ,cAAc,QAAO,mBAAmB;AAChD,SAAQ,eAAe,QAAO,uBAAuB;AAErD,OAAO,KAAK,IAAI,MAAM,YAAY;AAElC,SAAQ,GAAG,QAAO,QAAQ;AAC1B,OAAO,KAAK,cAAc,MAAM,mBAAmB;AACnD,OAAO,KAAK,SAAS,MAAM,cAAc;AACzC,SAAQ,eAAe,IAAI,sBAAsB,QAAO,qBAAqB;AAC7E,SAAQ,mCAAmC,QAAO,4CAA4C;AAC9F,SAAQ,kCAAkC,QAAO,2CAA2C;AAE5F,SAAQ,eAAe,EAAE,oBAAoB,EAAE,oBAAoB,EAAE,UAAU,QAAO,eAAe;AACrG,SAAQ,EAAE,QAAO,cAAc;AAC/B,SAAQ,OAAO,QAAO,YAAY;AAElC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAkDG;AACH,SAAS,qBAAqB,OAwB7B;EAAA,IAxB2D;IAC1D,CAAC;IACD,MAAM;IACN,OAAO;IACP,GAAG;IACH,UAAU,GAAG,MAAM;IACnB,SAAS,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC;IAClB,eAAe;IACf,IAAI;IACJ,UAAU,GAAG,QAAQ;IACrB,sBAAsB;IACtB;EAAc,CAaf;EACC,IAAI,UAAU,CAAC,MAAM,CAAC,KAAK,CAAC,aAAa,EAAE,UAAU,CAAC,KAAK,KAAK,EAAE;IAChE,IAAI,MAAM,GAAG,sBAAsB,CAC/B,CAAC,EAAE,MAAM,EAAE,OAAO,EAAE,GAAG,EAAE,UAAU,EAAE,SAAS,EAAE,eAAe,CAAC;IACpE,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,MAAM,GAAG,GAAG,CAAC,MAAM,EAAE,IAAI,CAAC;IAC3B;IAED,OAAO,eAAe,CACX,MAAM,EAAE,UAAU,EAAE,sBAAsB,EAAE,cAAc,CAAM;EAC5E;EAED,MAAM,EAAE,GAAG,eAAe,CAAC,CAAC,EAAE,GAAG,EAAE,iBAAiB,CAAC;EACrD,MAAM,OAAO,GAAG,eAAe,CAAC,MAAM,EAAE,QAAQ,EAAE,iBAAiB,CAAC;EAEpE,IAAI,GAAG,GAAG,EAAc;EACxB,IAAI,YAAY,GAAG,KAAK;EACxB,IAAI,EAAE,CAAC,IAAI,KAAK,CAAC,EAAE;IACjB,YAAY,GAAG,IAAI;IACnB,GAAG,GAAG,OAAO,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;EAC9D;EACD,IAAI,CAAC,MAAM,CACP,GAAG,CAAC,IAAI,KAAK,CAAC,EACd,MAAM,gEAAgE,GAClE,QAAQ,GAAG,CAAC,IAAI,GAAG,CAAC;EAC5B,IAAI,CAAC,MAAM,CACP,OAAO,CAAC,IAAI,KAAK,CAAC,EAClB,MAAM,yDAAyD,GAC3D,gBAAgB,OAAO,CAAC,IAAI,GAAG,CAAC;EACxC,IAAI,CAAC,MAAM,CACP,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,EACjC,MAAM,2DAA2D,GAC7D,IAAI,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,2CAA2C,GAC3D,UAAU,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,GAAG,CAAC;EACtC,IAAI,SAAS,IAAI,IAAI,EAAE;IACrB,SAAS,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC;EACnB;EACD,IAAI,CAAC,MAAM,CACP,SAAS,CAAC,8BAA8B,CAAC,OAAO,EAAE,SAAS,CAAC,EAC5D,MACI,mEAAmE,GACnE,qBAAqB,OAAO,mBAAmB,SAAS,GAAG,CAAC;EAEpE,IAAI,eAAe,IAAI,IAAI,EAAE;IAC3B,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,KAAK,CAAC,GAAa,CAAC,EACzB,MAAM,8DAA8D,GAChE,yBAAyB,eAAe,gBAAgB,GAAG,GAAG,CAAC;EACxE;EAED,MAAM,QAAQ,GAAG,SAAS,CAAC,iBAAiB,CACxC,GAAG,CAAC,KAAK,EAAE,OAAO,CAAC,KAAK,EAAE,OAAO,EAAE,SAAS,EAAE,GAAG,EAAE,eAAe,EAClE,IAAI,CAAC,gBAAgB;EAEzB,IAAI,KAAa;EACjB,IAAI,IAAI,IAAI,IAAI,EAAE;IAChB,KAAK,GAAG,eAAe,CAAC,IAAI,EAAE,MAAM,EAAE,cAAc,CAAC;IACrD,CAAC,KAAK,CAAC,GAAG,cAAc,CAAC,KAAK,EAAE,EAAE,CAAC;IAEnC,cAAc,CAAC,0BAA0B,CAAC,QAAQ,CAAC,QAAQ,EAAE,KAAK,CAAC,KAAK,CAAC;EAC1E;EAED,IAAI,uBAA+B;EACnC,IAAI,sBAAsB,IAAI,IAAI,EAAE;IAClC,uBAAuB,GAAG,eAAe,CACrC,sBAAsB,EAAE,eAAe,EAAE,uBAAuB,CAAC;EACtE;EAED,MAAM,IAAI,GAAG,CAAC,EAAY,EAAE,KAAe,KAAI;IAC7C,IAAI,CAAC,MAAM,CACP,SAAS,CAAC,iBAAiB,CAAC,SAAS,CAAC,EACtC,MAAM,6DAA6D,GAC/D,sDAAsD,GACtD,IAAI,SAAS,GAAG,CAAC;IACzB,MAAM,CAAC,OAAO,EAAE,GAAG,EAAE,CAAC,EAAE,IAAI,CAAC,GAAG,KAAK;IAErC,MAAM,YAAY,GAAG,oBAAoB,CAAC,EAAE,EAAE,CAAC,EAAE,UAAU,CAAa;IAExE,MAAM,IAAI,GAAG,kCAAkC,CAC1C,GAAgB,CAAC,KAAK,EAAE,YAAY,EAAE,OAAmB,EAAE,OAAO,EACnE,GAAG,EAAE,SAAS,EAAE,eAAe,CAAC;IACpC,MAAM,SAAS,GAAG,mCAAmC,CACjD,GAAe,EAAE,YAAY,EAAG,OAAoB,CAAC,KAAK,EAAE,OAAO,EACnE,GAAG,EAAE,SAAS,EAAE,eAAe,CAAC;IAEpC,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,MAAM,OAAO,GAAG,oBAAoB,CAAC,KAAK,EAAE,YAAY,CAAC;MACzD,OAAO,CAAC,IAAI,EAAE,SAAS,EAAE,OAAO,CAAC;IAClC;IACD,OAAO,CAAC,IAAI,EAAE,SAAS,CAAC;EAC1B,CAAC;EAED,MAAM,MAAM,GAA+B;IACzC,CAAC,EAAE,GAAG;IACN,MAAM,EAAE,OAAO;IACf,IAAI,EAAE,KAAK;IACX,sBAAsB,EAAE;GACzB;EACD,MAAM,KAAK,GAA8B;IACvC,OAAO;IACP,GAAG;IACH,UAAU;IACV,SAAS;IACT,eAAe;IACf,UAAU;IACV;GACD;EAED;EACA;EACA,IAAI,IAAI,IAAI,IAAI,EAAE;IAChB,MAAM,QAAQ,GACV,UAAU,CAAC,CAAC,GAAa,EAAE,MAAgB,EAAE,IAAkB,KAAI;MACjE;MACA,IAAI,GAAG,GAAsB,MAAM,CAAC,SAAS,CACzC,oBAAoB,EAAE,MAA8B,EACpD,KAA2B,CAAC;MAEhC,IAAI,CAAC,CAAC,MAAM,EAAE,GAAG,EAAE,GAAG,CAAC,CAAC;MAExB,IAAI,YAAY,EAAE;QAChB;QACA,GAAG,GAAG,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CACjD;MACb;MAED,OAAO;QAAC,KAAK,EAAE,GAAG;QAAE,QAAQ,EAAE;MAAI,CAAC;IACrC,CAAC,CAAC;IACN,OAAO,QAAQ,CAAC,GAAG,EAAE,OAAO,CAAM;GACnC,MAAM;IACL,MAAM,gBAAgB,GAAG,UAAU,CAC/B,CAAC,GAAa,EAAE,MAAgB,EAAE,IAAY,EAAE,IAAkB,KAAI;MACpE;MACA,IAAI,GAAG,GAAsB,MAAM,CAAC,SAAS,CACzC,oBAAoB,EAAE,MAA8B,EACpD,KAA2B,CAAC;MAEhC,IAAI,CAAC,CAAC,MAAM,EAAE,GAAG,EAAE,GAAG,EAAE,IAAI,CAAC,CAAC;MAE9B,IAAI,YAAY,EAAE;QAChB;QACA,GAAG,GAAG,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CACjD;MACb;MAED,OAAO;QAAC,KAAK,EAAE,GAAG;QAAE,QAAQ,EAAE;MAAI,CAAC;IACrC,CAAC,CAAC;IAEN,OAAO,gBAAgB,CAAC,GAAG,EAAE,OAAO,EAAE,KAAK,CAAM;EAClD;AACH;AACA,OAAO,MAAM,eAAe,GAAG,EAAE,CAAC;EAAC;AAAqB,CAAC,CAAC","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedDepthwiseConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport * as conv_util from '../conv_util';\nimport { depthwiseConv2d as unfusedDepthwiseConv2d } from '../depthwise_conv2d';\nimport { depthwiseConv2dNativeBackpropFilter } from '../depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from '../depthwise_conv2d_native_backprop_input';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedDepthwiseConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedDepthwiseConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    const $x = convertToTensor(x, 'x', 'depthwiseConv2d');\n    const $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d');\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` +\n        `rank ${x4D.rank}.`);\n    util.assert($filter.rank === 4, () => `Error in fused depthwiseConv2d: filter must be rank 4, ` +\n        `but got rank ${$filter.rank}.`);\n    util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in fused depthwiseConv2d: number of input channels ` +\n        `(${x4D.shape[3]}) must match the inChannels dimension in ` +\n        `filter ${$filter.shape[2]}.`);\n    if (dilations == null) {\n        dilations = [1, 1];\n    }\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in fused depthwiseConv2d: Either strides or dilations must ' +\n        `be 1. Got strides ${strides} and dilations '${dilations}'`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in fused depthwiseConv2d: pad must be an integer when ` +\n            `using dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode, true /* depthwise */);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n        [$bias] = makeTypesMatch($bias, $x);\n        broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n    }\n    const grad = (dy, saved) => {\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' +\n            `greater than 1 are not yet supported. Got dilations ` +\n            `'${dilations}'`);\n        const [$filter, x4D, y, bias] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        const xDer = depthwiseConv2dNativeBackpropInput(x4D.shape, dyActivation, $filter, strides, pad, dilations, dimRoundingMode);\n        const filterDer = depthwiseConv2dNativeBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad, dilations, dimRoundingMode);\n        if (bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            return [xDer, filterDer, biasDer];\n        }\n        return [xDer, filterDer];\n    };\n    const inputs = {\n        x: x4D,\n        filter: $filter,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = {\n        strides,\n        pad,\n        dataFormat,\n        dilations,\n        dimRoundingMode,\n        activation,\n        leakyreluAlpha\n    };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((x4D, filter, save) => {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n            save([filter, x4D, res]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOp(x4D, $filter);\n    }\n    else {\n        const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n            save([filter, x4D, res, bias]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOpWithBias(x4D, $filter, $bias);\n    }\n}\nexport const depthwiseConv2d = op({ fusedDepthwiseConv2d_ });\n//# sourceMappingURL=depthwise_conv2d.js.map"]},"metadata":{},"sourceType":"module"}
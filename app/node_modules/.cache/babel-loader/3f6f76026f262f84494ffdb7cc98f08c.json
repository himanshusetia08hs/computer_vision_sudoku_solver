{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Noise Layers.\n */\nimport { greaterEqual, randomUniform, serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { getExactlyOneTensor } from '../utils/types_utils';\nexport class GaussianNoise extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.stddev = args.stddev;\n  }\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      stddev: this.stddev\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      const noised = () => K.randomNormal(input.shape, 0, this.stddev).add(input);\n      const output = K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n      return output;\n    });\n  }\n}\n/** @nocollapse */\nGaussianNoise.className = 'GaussianNoise';\nserialization.registerClass(GaussianNoise);\nexport class GaussianDropout extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.rate = args.rate;\n  }\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      rate: this.rate\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      if (this.rate > 0 && this.rate < 1) {\n        const noised = () => {\n          const stddev = Math.sqrt(this.rate / (1 - this.rate));\n          return input.mul(K.randomNormal(input.shape, 1, stddev));\n        };\n        return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n      }\n      return input;\n    });\n  }\n}\n/** @nocollapse */\nGaussianDropout.className = 'GaussianDropout';\nserialization.registerClass(GaussianDropout);\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\nexport class AlphaDropout extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.rate = args.rate;\n    this.noiseShape = args.noiseShape;\n  }\n  _getNoiseShape(inputs) {\n    return this.noiseShape || getExactlyOneTensor(inputs).shape;\n  }\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      rate: this.rate\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.rate < 1 && this.rate > 0) {\n        const noiseShape = this._getNoiseShape(inputs);\n        const droppedInputs = () => {\n          const input = getExactlyOneTensor(inputs);\n          const alpha = 1.6732632423543772848170429916717;\n          const scale = 1.0507009873554804934193349852946;\n          const alphaP = -alpha * scale;\n          let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\n          keptIdx = K.cast(keptIdx, 'float32'); // get default dtype.\n          // Get affine transformation params.\n          const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\n          const b = -a * alphaP * this.rate;\n          // Apply mask.\n          const x = input.mul(keptIdx).add(keptIdx.add(-1).mul(alphaP));\n          return x.mul(a).add(b);\n        };\n        return K.inTrainPhase(droppedInputs, () => getExactlyOneTensor(inputs), kwargs['training'] || false);\n      }\n      return inputs;\n    });\n  }\n}\n/** @nocollapse */\nAlphaDropout.className = 'AlphaDropout';\nserialization.registerClass(AlphaDropout);","map":{"version":3,"sources":["../../src/layers/noise.ts"],"names":[],"mappings":"AAAA;;;;;;;;AAQG;AAEH;;AAEG;AAEH,SAAQ,YAAY,EAAE,aAAa,EAAE,aAAa,EAAU,IAAI,QAAO,uBAAuB;AAE9F,OAAO,KAAK,CAAC,MAAM,yBAAyB;AAC5C,SAAQ,KAAK,QAAkB,oBAAoB;AAGnD,SAAQ,mBAAmB,QAAO,sBAAsB;AAOxD,OAAM,MAAO,aAAc,SAAQ,KAAK,CAAA;EAKtC,WAAA,CAAY,IAAuB,EAAA;IACjC,KAAK,CAAC,IAAI,CAAC;IACX,IAAI,CAAC,eAAe,GAAG,IAAI;IAC3B,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM;EAC3B;EAEA,kBAAkB,CAAC,UAAyB,EAAA;IAC1C,OAAO,UAAU;EACnB;EAEA,SAAS,GAAA;IACP,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,MAAM,GAAG;MAAC,MAAM,EAAE,IAAI,CAAC;IAAM,CAAC;IACpC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,IAAI,CAAC,cAAc,CAAC,MAAM,EAAE,MAAM,CAAC;MACnC,MAAM,KAAK,GAAG,mBAAmB,CAAC,MAAM,CAAC;MACzC,MAAM,MAAM,GAAG,MACX,CAAC,CAAC,YAAY,CAAC,KAAK,CAAC,KAAK,EAAE,CAAC,EAAE,IAAI,CAAC,MAAM,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC;MAC1D,MAAM,MAAM,GACR,CAAC,CAAC,YAAY,CAAC,MAAM,EAAE,MAAM,KAAK,EAAE,MAAM,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC;MACpE,OAAO,MAAM;IACf,CAAC,CAAC;EACJ;;AA/BA;AACO,aAAA,CAAA,SAAS,GAAG,eAAe;AAgCpC,aAAa,CAAC,aAAa,CAAC,aAAa,CAAC;AAO1C,OAAM,MAAO,eAAgB,SAAQ,KAAK,CAAA;EAKxC,WAAA,CAAY,IAAyB,EAAA;IACnC,KAAK,CAAC,IAAI,CAAC;IACX,IAAI,CAAC,eAAe,GAAG,IAAI;IAC3B,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI;EACvB;EAEA,kBAAkB,CAAC,UAAyB,EAAA;IAC1C,OAAO,UAAU;EACnB;EAEA,SAAS,GAAA;IACP,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,MAAM,GAAG;MAAC,IAAI,EAAE,IAAI,CAAC;IAAI,CAAC;IAChC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,IAAI,CAAC,cAAc,CAAC,MAAM,EAAE,MAAM,CAAC;MACnC,MAAM,KAAK,GAAG,mBAAmB,CAAC,MAAM,CAAC;MACzC,IAAI,IAAI,CAAC,IAAI,GAAG,CAAC,IAAI,IAAI,CAAC,IAAI,GAAG,CAAC,EAAE;QAClC,MAAM,MAAM,GAAG,MAAK;UAClB,MAAM,MAAM,GAAG,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC,IAAI,IAAI,CAAC,GAAG,IAAI,CAAC,IAAI,CAAC,CAAC;UACrD,OAAO,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC,YAAY,CAAC,KAAK,CAAC,KAAK,EAAE,CAAC,EAAE,MAAM,CAAC,CAAC;QAC1D,CAAC;QACD,OAAO,CAAC,CAAC,YAAY,CAAC,MAAM,EAAE,MAAM,KAAK,EAAE,MAAM,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC;MACxE;MACD,OAAO,KAAK;IACd,CAAC,CAAC;EACJ;;AAlCA;AACO,eAAA,CAAA,SAAS,GAAG,iBAAiB;AAmCtC,aAAa,CAAC,aAAa,CAAC,eAAe,CAAC;AAY5C;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA4BG;AACH,OAAM,MAAO,YAAa,SAAQ,KAAK,CAAA;EAMrC,WAAA,CAAY,IAAsB,EAAA;IAChC,KAAK,CAAC,IAAI,CAAC;IACX,IAAI,CAAC,eAAe,GAAG,IAAI;IAC3B,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI;IACrB,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,UAAU;EACnC;EAEA,cAAc,CAAC,MAAuB,EAAA;IACpC,OAAO,IAAI,CAAC,UAAU,IAAI,mBAAmB,CAAC,MAAM,CAAC,CAAC,KAAK;EAC7D;EAEA,kBAAkB,CAAC,UAAyB,EAAA;IAC1C,OAAO,UAAU;EACnB;EAEA,SAAS,GAAA;IACP,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,MAAM,GAAG;MAAC,IAAI,EAAE,IAAI,CAAC;IAAI,CAAC;IAChC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,IAAI,IAAI,CAAC,IAAI,GAAG,CAAC,IAAI,IAAI,CAAC,IAAI,GAAG,CAAC,EAAE;QAClC,MAAM,UAAU,GAAG,IAAI,CAAC,cAAc,CAAC,MAAM,CAAC;QAE9C,MAAM,aAAa,GAAG,MAAK;UACzB,MAAM,KAAK,GAAG,mBAAmB,CAAC,MAAM,CAAC;UAEzC,MAAM,KAAK,GAAG,iCAAiC;UAC/C,MAAM,KAAK,GAAG,iCAAiC;UAE/C,MAAM,MAAM,GAAG,CAAC,KAAK,GAAG,KAAK;UAE7B,IAAI,OAAO,GAAG,YAAY,CAAC,aAAa,CAAC,UAAU,CAAC,EAAE,IAAI,CAAC,IAAI,CAAC;UAEhE,OAAO,GAAG,CAAC,CAAC,IAAI,CAAC,OAAO,EAAE,SAAS,CAAC,CAAC,CAAE;UAEvC;UACA,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,IAAI,CAAC,IAAI,KAAK,CAAC,GAAG,IAAI,CAAC,IAAI,GAAG,MAAM,IAAI,CAAC,CAAC,KAAK,CAAC,GAAG;UACnE,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM,GAAG,IAAI,CAAC,IAAI;UAEjC;UACA,MAAM,CAAC,GAAG,KAAK,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,GAAG,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC;UAE7D,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;QACxB,CAAC;QACD,OAAO,CAAC,CAAC,YAAY,CACjB,aAAa,EAAE,MAAM,mBAAmB,CAAC,MAAM,CAAC,EAChD,MAAM,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC;MACjC;MACD,OAAO,MAAM;IACf,CAAC,CAAC;EACJ;;AA3DA;AACO,YAAA,CAAA,SAAS,GAAG,cAAc;AA4DnC,aAAa,CAAC,aAAa,CAAC,YAAY,CAAC","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Noise Layers.\n */\nimport { greaterEqual, randomUniform, serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { getExactlyOneTensor } from '../utils/types_utils';\nexport class GaussianNoise extends Layer {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n        this.stddev = args.stddev;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = { stddev: this.stddev };\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            const input = getExactlyOneTensor(inputs);\n            const noised = () => K.randomNormal(input.shape, 0, this.stddev).add(input);\n            const output = K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n            return output;\n        });\n    }\n}\n/** @nocollapse */\nGaussianNoise.className = 'GaussianNoise';\nserialization.registerClass(GaussianNoise);\nexport class GaussianDropout extends Layer {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n        this.rate = args.rate;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = { rate: this.rate };\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            const input = getExactlyOneTensor(inputs);\n            if (this.rate > 0 && this.rate < 1) {\n                const noised = () => {\n                    const stddev = Math.sqrt(this.rate / (1 - this.rate));\n                    return input.mul(K.randomNormal(input.shape, 1, stddev));\n                };\n                return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n            }\n            return input;\n        });\n    }\n}\n/** @nocollapse */\nGaussianDropout.className = 'GaussianDropout';\nserialization.registerClass(GaussianDropout);\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\nexport class AlphaDropout extends Layer {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n        this.rate = args.rate;\n        this.noiseShape = args.noiseShape;\n    }\n    _getNoiseShape(inputs) {\n        return this.noiseShape || getExactlyOneTensor(inputs).shape;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = { rate: this.rate };\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.rate < 1 && this.rate > 0) {\n                const noiseShape = this._getNoiseShape(inputs);\n                const droppedInputs = () => {\n                    const input = getExactlyOneTensor(inputs);\n                    const alpha = 1.6732632423543772848170429916717;\n                    const scale = 1.0507009873554804934193349852946;\n                    const alphaP = -alpha * scale;\n                    let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\n                    keptIdx = K.cast(keptIdx, 'float32'); // get default dtype.\n                    // Get affine transformation params.\n                    const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\n                    const b = -a * alphaP * this.rate;\n                    // Apply mask.\n                    const x = input.mul(keptIdx).add(keptIdx.add(-1).mul(alphaP));\n                    return x.mul(a).add(b);\n                };\n                return K.inTrainPhase(droppedInputs, () => getExactlyOneTensor(inputs), kwargs['training'] || false);\n            }\n            return inputs;\n        });\n    }\n}\n/** @nocollapse */\nAlphaDropout.className = 'AlphaDropout';\nserialization.registerClass(AlphaDropout);\n//# sourceMappingURL=noise.js.map"]},"metadata":{},"sourceType":"module"}
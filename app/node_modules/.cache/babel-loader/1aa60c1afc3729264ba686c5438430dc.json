{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { _FusedMatMul } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { matMul as unfusedMatMul } from '../mat_mul';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n * - `leakyreluAlpha` Alpha of leakyrelu.\n */\nfunction fusedMatMul_(_ref) {\n  let {\n    a,\n    b,\n    transposeA = false,\n    transposeB = false,\n    bias,\n    activation = 'linear',\n    preluActivationWeights,\n    leakyreluAlpha\n  } = _ref;\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedMatMul(a, b, transposeA, transposeB);\n    if (bias != null) {\n      result = add(result, bias);\n    }\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n  let $a = convertToTensor(a, 'a', 'fused matMul');\n  let $b = convertToTensor(b, 'b', 'fused matMul');\n  [$a, $b] = makeTypesMatch($a, $b);\n  const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n  const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n  const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n  const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n  const outerDimsA = $a.shape.slice(0, -2);\n  const outerDimsB = $b.shape.slice(0, -2);\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n  util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at ` + `least 2, got ranks ${$a.rank} and ${$b.rank}.`);\n  util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (` + `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` + `${$b.shape} must match.`);\n  util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` + `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` + `${$b.shape} and transposeA=${transposeA}` + ` and transposeB=${transposeB} must match.`);\n  const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n  const a3D = transposeA ? reshape($a, [batchDimA, innerShapeA, outerShapeA]) : reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n  const b3D = transposeB ? reshape($b, [batchDimB, outerShapeB, innerShapeB]) : reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n  let $bias;\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused matMul');\n    [$bias] = makeTypesMatch($bias, $a);\n    broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n  }\n  let $preluActivationWeights;\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n  }\n  const grad = (dy, saved) => {\n    const [a3D, b3D, y, $bias] = saved;\n    // we reshape dy because the result of the forward is not\n    // necessarily going to be a 3d tensor due to a reshape done at the end of\n    // the customOp.\n    const dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n    let aDer;\n    let bDer;\n    if (!transposeA && !transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, true, false);\n    } else if (!transposeA && transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, false);\n      bDer = unfusedMatMul(dyActivation, a3D, true, false);\n    } else if (transposeA && !transposeB) {\n      aDer = unfusedMatMul(b3D, dyActivation, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, false, false);\n    } else {\n      aDer = unfusedMatMul(b3D, dyActivation, true, true);\n      bDer = unfusedMatMul(dyActivation, a3D, true, true);\n    }\n    if (bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [aDer, bDer, biasDer];\n    } else {\n      return [aDer, bDer];\n    }\n  };\n  const inputs = {\n    a: a3D,\n    b: b3D,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs = {\n    transposeA,\n    transposeB,\n    activation,\n    leakyreluAlpha\n  };\n  // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n  if (bias == null) {\n    const customOp = customGrad((a3D, b3D, save) => {\n      const res =\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n      save([a3D, b3D, res]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOp(a3D, b3D);\n  } else {\n    const customOpWithBias = customGrad((a3D, b3D, $bias, save) => {\n      const res =\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n      save([a3D, b3D, res, $bias]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(a3D, b3D, $bias);\n  }\n}\nexport const matMul = op({\n  fusedMatMul_\n});","map":{"version":3,"sources":["../../../src/ops/fused/mat_mul.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAM,QAAO,cAAc;AACnC,SAAQ,UAAU,QAAO,iBAAiB;AAC1C,SAAQ,YAAY,QAA8C,oBAAoB;AAItF,SAAQ,cAAc,QAAO,mBAAmB;AAChD,SAAQ,eAAe,QAAO,uBAAuB;AAErD,OAAO,KAAK,IAAI,MAAM,YAAY;AAElC,SAAQ,GAAG,QAAO,QAAQ;AAC1B,OAAO,KAAK,cAAc,MAAM,mBAAmB;AAEnD,SAAQ,eAAe,EAAE,oBAAoB,EAAE,oBAAoB,EAAE,UAAU,QAAO,eAAe;AACrG,SAAQ,MAAM,IAAI,aAAa,QAAO,YAAY;AAClD,SAAQ,EAAE,QAAO,cAAc;AAC/B,SAAQ,OAAO,QAAO,YAAY;AAElC;;;;;;;;;;;;;;;;;;;;AAoBG;AACH,SAAS,YAAY,OAkBpB;EAAA,IAlBuC;IACtC,CAAC;IACD,CAAC;IACD,UAAU,GAAG,KAAK;IAClB,UAAU,GAAG,KAAK;IAClB,IAAI;IACJ,UAAU,GAAG,QAAQ;IACrB,sBAAsB;IACtB;EAAc,CAUf;EACG,IAAI,UAAU,CAAC,MAAM,CAAC,KAAK,CAAC,aAAa,EAAE,UAAU,CAAC,KAAK,KAAK,EAAE;IAChE,IAAI,MAAM,GAAG,aAAa,CAAC,CAAC,EAAE,CAAC,EAAE,UAAU,EAAE,UAAU,CAAC;IACxD,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,MAAM,GAAG,GAAG,CAAC,MAAM,EAAE,IAAI,CAAC;IAC3B;IAED,OAAO,eAAe,CACX,MAAM,EAAE,UAAU,EAAE,sBAAsB,EAAE,cAAc,CAChE;EACN;EAED,IAAI,EAAE,GAAG,eAAe,CAAC,CAAC,EAAE,GAAG,EAAE,cAAc,CAAC;EAChD,IAAI,EAAE,GAAG,eAAe,CAAC,CAAC,EAAE,GAAG,EAAE,cAAc,CAAC;EAChD,CAAC,EAAE,EAAE,EAAE,CAAC,GAAG,cAAc,CAAC,EAAE,EAAE,EAAE,CAAC;EAEjC,MAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC,GAAG,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC;EAC9D,MAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC,GAAG,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC;EAE9D,MAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC,GAAG,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC;EAC9D,MAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC,GAAG,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC;EAE9D,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;EACxC,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;EACxC,MAAM,SAAS,GAAG,IAAI,CAAC,aAAa,CAAC,UAAU,CAAC;EAChD,MAAM,SAAS,GAAG,IAAI,CAAC,aAAa,CAAC,UAAU,CAAC;EAEhD,IAAI,CAAC,MAAM,CACP,EAAE,CAAC,IAAI,IAAI,CAAC,IAAI,EAAE,CAAC,IAAI,IAAI,CAAC,IAAI,EAAE,CAAC,IAAI,KAAK,EAAE,CAAC,IAAI,EACnD,MAAM,8DAA8D,GAChE,sBAAsB,EAAE,CAAC,IAAI,QAAQ,EAAE,CAAC,IAAI,GAAG,CAAC;EAExD,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,WAAW,CAAC,UAAU,EAAE,UAAU,CAAC,EACxC,MAAM,4CAA4C,UAAU,SAAS,GACjE,GAAG,UAAU,4BAA4B,EAAE,CAAC,KAAK,OAAO,GACxD,GAAG,EAAE,CAAC,KAAK,cAAc,CAAC;EAElC,IAAI,CAAC,MAAM,CACP,WAAW,KAAK,WAAW,EAC3B,MAAM,wCAAwC,WAAW,SAAS,GAC9D,GAAG,WAAW,4BAA4B,EAAE,CAAC,KAAK,OAAO,GACzD,GAAG,EAAE,CAAC,KAAK,mBAAmB,UAAU,EAAE,GAC1C,mBAAmB,UAAU,cAAc,CAAC;EAEpD,MAAM,QAAQ,GAAG,EAAE,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,WAAW,EAAE,WAAW,CAAC,CAAC;EAEzE,MAAM,GAAG,GAAa,UAAU,GAC5B,OAAO,CAAC,EAAE,EAAE,CAAC,SAAS,EAAE,WAAW,EAAE,WAAW,CAAC,CAAC,GAClD,OAAO,CAAC,EAAE,EAAE,CAAC,SAAS,EAAE,WAAW,EAAE,WAAW,CAAC,CAAC;EACtD,MAAM,GAAG,GAAa,UAAU,GAC5B,OAAO,CAAC,EAAE,EAAE,CAAC,SAAS,EAAE,WAAW,EAAE,WAAW,CAAC,CAAC,GAClD,OAAO,CAAC,EAAE,EAAE,CAAC,SAAS,EAAE,WAAW,EAAE,WAAW,CAAC,CAAC;EAEtD,IAAI,KAAa;EACjB,IAAI,IAAI,IAAI,IAAI,EAAE;IAChB,KAAK,GAAG,eAAe,CAAC,IAAI,EAAE,MAAM,EAAE,cAAc,CAAC;IACrD,CAAC,KAAK,CAAC,GAAG,cAAc,CAAC,KAAK,EAAE,EAAE,CAAC;IAEnC,cAAc,CAAC,0BAA0B,CAAC,QAAQ,EAAE,KAAK,CAAC,KAAK,CAAC;EACjE;EAED,IAAI,uBAA+B;EACnC,IAAI,sBAAsB,IAAI,IAAI,EAAE;IAClC,uBAAuB,GAAG,eAAe,CACrC,sBAAsB,EAAE,eAAe,EAAE,cAAc,CAAC;EAC7D;EAED,MAAM,IAAI,GAAG,CAAC,EAAY,EAAE,KAAe,KAAI;IAC7C,MAAM,CAAC,GAAG,EAAE,GAAG,EAAE,CAAC,EAAE,KAAK,CAAC,GAAG,KAAK;IAClC;IACA;IACA;IACA,MAAM,YAAY,GACd,oBAAoB,CAAC,OAAO,CAAC,EAAE,EAAE,CAAC,CAAC,KAAK,CAAC,EAAE,CAAC,EAAE,UAAU,CAAC;IAC7D,IAAI,IAAY;IAChB,IAAI,IAAY;IAEhB,IAAI,CAAC,UAAU,IAAI,CAAC,UAAU,EAAE;MAC9B,IAAI,GAAG,aAAa,CAAC,YAAY,EAAE,GAAG,EAAE,KAAK,EAAE,IAAI,CAAC;MACpD,IAAI,GAAG,aAAa,CAAC,GAAG,EAAE,YAAY,EAAE,IAAI,EAAE,KAAK,CAAC;KACrD,MAAM,IAAI,CAAC,UAAU,IAAI,UAAU,EAAE;MACpC,IAAI,GAAG,aAAa,CAAC,YAAY,EAAE,GAAG,EAAE,KAAK,EAAE,KAAK,CAAC;MACrD,IAAI,GAAG,aAAa,CAAC,YAAY,EAAE,GAAG,EAAE,IAAI,EAAE,KAAK,CAAC;KACrD,MAAM,IAAI,UAAU,IAAI,CAAC,UAAU,EAAE;MACpC,IAAI,GAAG,aAAa,CAAC,GAAG,EAAE,YAAY,EAAE,KAAK,EAAE,IAAI,CAAC;MACpD,IAAI,GAAG,aAAa,CAAC,GAAG,EAAE,YAAY,EAAE,KAAK,EAAE,KAAK,CAAC;KACtD,MAAM;MACL,IAAI,GAAG,aAAa,CAAC,GAAG,EAAE,YAAY,EAAE,IAAI,EAAE,IAAI,CAAC;MACnD,IAAI,GAAG,aAAa,CAAC,YAAY,EAAE,GAAG,EAAE,IAAI,EAAE,IAAI,CAAC;IACpD;IAED,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,MAAM,OAAO,GAAG,oBAAoB,CAAC,KAAK,EAAE,YAAY,CAAC;MACzD,OAAO,CAAC,IAAI,EAAE,IAAI,EAAE,OAAO,CAAC;KAC7B,MAAM;MACL,OAAO,CAAC,IAAI,EAAE,IAAI,CAAC;IACpB;EACH,CAAC;EAED,MAAM,MAAM,GAAuB;IACjC,CAAC,EAAE,GAAG;IACN,CAAC,EAAE,GAAG;IACN,IAAI,EAAE,KAAK;IACX,sBAAsB,EAAE;GACzB;EACD,MAAM,KAAK,GACP;IAAC,UAAU;IAAE,UAAU;IAAE,UAAU;IAAE;EAAc,CAAC;EAExD;EACA;EACA,IAAI,IAAI,IAAI,IAAI,EAAE;IAChB,MAAM,QAAQ,GACV,UAAU,CAAC,CAAC,GAAa,EAAE,GAAa,EAAE,IAAkB,KAAI;MAC9D,MAAM,GAAG;MACL;MACA,MAAM,CAAC,SAAS,CACZ,YAAY,EAAE,MAA8B,EAC5C,KAA2B,CAAM;MAEzC,IAAI,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,CAAC,CAAC;MAErB,OAAO;QAAC,KAAK,EAAE,OAAO,CAAC,GAAG,EAAE,QAAQ,CAAC;QAAE,QAAQ,EAAE;MAAI,CAAC;IACxD,CAAC,CAAC;IACN,OAAO,QAAQ,CAAC,GAAG,EAAE,GAAG,CAAM;GAC/B,MAAM;IACL,MAAM,gBAAgB,GAAG,UAAU,CAC/B,CAAC,GAAa,EAAE,GAAa,EAAE,KAAa,EAAE,IAAkB,KAAI;MAClE,MAAM,GAAG;MACL;MACA,MAAM,CAAC,SAAS,CACZ,YAAY,EAAE,MAA8B,EAC5C,KAA2B,CAAM;MAEzC,IAAI,CAAC,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,KAAK,CAAC,CAAC;MAE5B,OAAO;QAAC,KAAK,EAAE,OAAO,CAAC,GAAG,EAAE,QAAQ,CAAC;QAAE,QAAQ,EAAE;MAAI,CAAC;IACxD,CAAC,CAAC;IAEN,OAAO,gBAAgB,CAAC,GAAG,EAAE,GAAG,EAAE,KAAK,CAAM;EAC9C;AACH;AAEA,OAAO,MAAM,MAAM,GAAG,EAAE,CAAC;EAAC;AAAY,CAAC,CAAC","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { _FusedMatMul } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { matMul as unfusedMatMul } from '../mat_mul';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n * - `leakyreluAlpha` Alpha of leakyrelu.\n */\nfunction fusedMatMul_({ a, b, transposeA = false, transposeB = false, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha, }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedMatMul(a, b, transposeA, transposeB);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    let $a = convertToTensor(a, 'a', 'fused matMul');\n    let $b = convertToTensor(b, 'b', 'fused matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n    const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n    const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n    const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n    const outerDimsA = $a.shape.slice(0, -2);\n    const outerDimsB = $b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n    util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at ` +\n        `least 2, got ranks ${$a.rank} and ${$b.rank}.`);\n    util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (` +\n        `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} must match.`);\n    util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` +\n        `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} and transposeA=${transposeA}` +\n        ` and transposeB=${transposeB} must match.`);\n    const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n    const a3D = transposeA ?\n        reshape($a, [batchDimA, innerShapeA, outerShapeA]) :\n        reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n    const b3D = transposeB ?\n        reshape($b, [batchDimB, outerShapeB, innerShapeB]) :\n        reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused matMul');\n        [$bias] = makeTypesMatch($bias, $a);\n        broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n    }\n    const grad = (dy, saved) => {\n        const [a3D, b3D, y, $bias] = saved;\n        // we reshape dy because the result of the forward is not\n        // necessarily going to be a 3d tensor due to a reshape done at the end of\n        // the customOp.\n        const dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n        let aDer;\n        let bDer;\n        if (!transposeA && !transposeB) {\n            aDer = unfusedMatMul(dyActivation, b3D, false, true);\n            bDer = unfusedMatMul(a3D, dyActivation, true, false);\n        }\n        else if (!transposeA && transposeB) {\n            aDer = unfusedMatMul(dyActivation, b3D, false, false);\n            bDer = unfusedMatMul(dyActivation, a3D, true, false);\n        }\n        else if (transposeA && !transposeB) {\n            aDer = unfusedMatMul(b3D, dyActivation, false, true);\n            bDer = unfusedMatMul(a3D, dyActivation, false, false);\n        }\n        else {\n            aDer = unfusedMatMul(b3D, dyActivation, true, true);\n            bDer = unfusedMatMul(dyActivation, a3D, true, true);\n        }\n        if (bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            return [aDer, bDer, biasDer];\n        }\n        else {\n            return [aDer, bDer];\n        }\n    };\n    const inputs = {\n        a: a3D,\n        b: b3D,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = { transposeA, transposeB, activation, leakyreluAlpha };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((a3D, b3D, save) => {\n            const res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n            save([a3D, b3D, res]);\n            return { value: reshape(res, outShape), gradFunc: grad };\n        });\n        return customOp(a3D, b3D);\n    }\n    else {\n        const customOpWithBias = customGrad((a3D, b3D, $bias, save) => {\n            const res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n            save([a3D, b3D, res, $bias]);\n            return { value: reshape(res, outShape), gradFunc: grad };\n        });\n        return customOpWithBias(a3D, b3D, $bias);\n    }\n}\nexport const matMul = op({ fusedMatMul_ });\n//# sourceMappingURL=mat_mul.js.map"]},"metadata":{},"sourceType":"module"}
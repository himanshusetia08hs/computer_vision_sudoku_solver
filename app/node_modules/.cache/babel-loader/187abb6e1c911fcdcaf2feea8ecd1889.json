{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n *  Advanced activation layers.\n */\nimport { clipByValue, elu, leakyRelu, prelu, relu, serialization } from '@tensorflow/tfjs-core';\nimport { Softmax as softmaxActivation } from '../activations';\nimport { cast } from '../backend/tfjs_backend';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nexport class ReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.supportsMasking = true;\n    if (args != null) {\n      this.maxValue = args.maxValue;\n    }\n  }\n  call(inputs, kwargs) {\n    inputs = getExactlyOneTensor(inputs);\n    let output = relu(inputs);\n    if (this.maxValue != null) {\n      output = clipByValue(output, 0, this.maxValue);\n    }\n    return output;\n  }\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  getConfig() {\n    const config = {\n      maxValue: this.maxValue\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nReLU.className = 'ReLU';\nserialization.registerClass(ReLU);\nexport class LeakyReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_ALPHA = 0.3;\n    if (args == null) {\n      args = {};\n    }\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return leakyRelu(x, this.alpha);\n  }\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  getConfig() {\n    const config = {\n      alpha: this.alpha\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nLeakyReLU.className = 'LeakyReLU';\nserialization.registerClass(LeakyReLU);\nexport class PReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_ALPHA_INITIALIZER = 'zeros';\n    if (args == null) {\n      args = {};\n    }\n    this.supportsMasking = true;\n    this.alphaInitializer = getInitializer(args.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER);\n    this.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n    this.alphaConstraint = getConstraint(args.alphaConstraint);\n    if (args.sharedAxes == null) {\n      this.sharedAxes = null;\n    } else if (Array.isArray(args.sharedAxes)) {\n      this.sharedAxes = args.sharedAxes;\n    } else if (typeof args.sharedAxes === 'number') {\n      this.sharedAxes = [args.sharedAxes];\n    } else {\n      throw new ValueError(`Expected sharedAxes to be a number or an array of numbers, ` + `but got ${args.sharedAxes}`);\n    }\n  }\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const paramShape = inputShape.slice(1);\n    if (this.sharedAxes != null) {\n      for (const i of this.sharedAxes) {\n        paramShape[i - 1] = 1;\n      }\n    }\n    this.alpha = this.addWeight('alpha', paramShape, 'float32', this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint);\n    // Set input spec.\n    const axes = {};\n    if (this.sharedAxes != null) {\n      for (let i = 1; i < inputShape.length; ++i) {\n        axes[i] = inputShape[i];\n      }\n    }\n    this.inputSpec = [new InputSpec({\n      ndim: inputShape.length,\n      axes\n    })];\n    this.built = true;\n  }\n  call(inputs, kwargs) {\n    inputs = getExactlyOneTensor(inputs);\n    return prelu(inputs, this.alpha.read());\n  }\n  getConfig() {\n    const config = {\n      alphaInitializer: serializeInitializer(this.alphaInitializer),\n      alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n      alphaConstraint: serializeConstraint(this.alphaConstraint),\n      sharedAxes: this.sharedAxes\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nPReLU.className = 'PReLU';\nserialization.registerClass(PReLU);\nexport class ELU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_ALPHA = 1.0;\n    if (args == null) {\n      args = {};\n    }\n    if (args.alpha != null && args.alpha !== this.DEFAULT_ALPHA) {\n      throw new NotImplementedError(`Non-default alpha value (${args.alpha}) is not supported by the ` + `ELU layer yet.`);\n    }\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return elu(x);\n  }\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  getConfig() {\n    const config = {\n      alpha: this.alpha\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nELU.className = 'ELU';\nserialization.registerClass(ELU);\nexport class ThresholdedReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_THETA = 1.0;\n    if (args == null) {\n      args = {};\n    }\n    this.theta = args.theta == null ? this.DEFAULT_THETA : args.theta;\n  }\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return x.mul(cast(x.greater(this.theta), 'float32'));\n  }\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  getConfig() {\n    const config = {\n      theta: this.theta\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nThresholdedReLU.className = 'ThresholdedReLU';\nserialization.registerClass(ThresholdedReLU);\nexport class Softmax extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_AXIS = 1.0;\n    if (args == null) {\n      args = {};\n    }\n    this.softmax = new softmaxActivation().apply;\n    this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n  }\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return this.softmax(x, this.axis);\n  }\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  getConfig() {\n    const config = {\n      axis: this.axis\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nSoftmax.className = 'Softmax';\nserialization.registerClass(Softmax);","map":{"version":3,"sources":["../../src/layers/advanced_activations.ts"],"names":[],"mappings":"AAAA;;;;;;;;AAQG;AAEH;;AAEG;AAEH,SAAQ,WAAW,EAAE,GAAG,EAAE,SAAS,EAAE,KAAK,EAAE,IAAI,EAAE,aAAa,QAAe,uBAAuB;AAErG,SAAQ,OAAO,IAAI,iBAAiB,QAAO,gBAAgB;AAC3D,SAAQ,IAAI,QAAO,yBAAyB;AAC5C,SAAoB,aAAa,EAAE,mBAAmB,QAAO,gBAAgB;AAC7E,SAAQ,SAAS,EAAE,KAAK,QAAkB,oBAAoB;AAC9D,SAAQ,mBAAmB,EAAE,UAAU,QAAO,WAAW;AACzD,SAAQ,cAAc,EAAsC,oBAAoB,QAAO,iBAAiB;AAExG,SAAQ,cAAc,EAAe,oBAAoB,QAAO,iBAAiB;AAEjF,SAAQ,kBAAkB,EAAE,mBAAmB,QAAO,sBAAsB;AAU5E,OAAM,MAAO,IAAK,SAAQ,KAAK,CAAA;EAK7B,WAAA,CAAY,IAAoB,EAAA;IAC9B,KAAK,CAAC,IAAI,IAAI,IAAI,GAAG,CAAA,CAAE,GAAG,IAAI,CAAC;IAC/B,IAAI,CAAC,eAAe,GAAG,IAAI;IAC3B,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,IAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,QAAQ;IAC9B;EACH;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,MAAM,GAAG,mBAAmB,CAAC,MAAM,CAAC;IACpC,IAAI,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC;IACzB,IAAI,IAAI,CAAC,QAAQ,IAAI,IAAI,EAAE;MACzB,MAAM,GAAG,WAAW,CAAC,MAAM,EAAE,CAAC,EAAE,IAAI,CAAC,QAAQ,CAAC;IAC/C;IACD,OAAO,MAAM;EACf;EAEA,kBAAkB,CAAC,UAAyB,EAAA;IAC1C,OAAO,UAAU;EACnB;EAEA,SAAS,GAAA;IACP,MAAM,MAAM,GAA6B;MAAC,QAAQ,EAAE,IAAI,CAAC;IAAQ,CAAC;IAClE,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;;AA9BA;AACO,IAAA,CAAA,SAAS,GAAG,MAAM;AA+B3B,aAAa,CAAC,aAAa,CAAC,IAAI,CAAC;AASjC,OAAM,MAAO,SAAU,SAAQ,KAAK,CAAA;EAOlC,WAAA,CAAY,IAAyB,EAAA;IACnC,KAAK,CAAC,IAAI,IAAI,IAAI,GAAG,CAAA,CAAE,GAAG,IAAI,CAAC;IAHxB,IAAA,CAAA,aAAa,GAAG,GAAG;IAI1B,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,IAAI,GAAG,CAAA,CAAE;IACV;IACD,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,IAAI,IAAI,GAAG,IAAI,CAAC,aAAa,GAAG,IAAI,CAAC,KAAK;EACnE;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,MAAM,CAAC,GAAG,mBAAmB,CAAC,MAAM,CAAC;IACrC,OAAO,SAAS,CAAC,CAAC,EAAE,IAAI,CAAC,KAAK,CAAC;EACjC;EAEA,kBAAkB,CAAC,UAAyB,EAAA;IAC1C,OAAO,UAAU;EACnB;EAEA,SAAS,GAAA;IACP,MAAM,MAAM,GAA6B;MAAC,KAAK,EAAE,IAAI,CAAC;IAAK,CAAC;IAC5D,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;;AA5BA;AACO,SAAA,CAAA,SAAS,GAAG,WAAW;AA6BhC,aAAa,CAAC,aAAa,CAAC,SAAS,CAAC;AA6BtC,OAAM,MAAO,KAAM,SAAQ,KAAK,CAAA;EAW9B,WAAA,CAAY,IAAqB,EAAA;IAC/B,KAAK,CAAC,IAAI,IAAI,IAAI,GAAG,CAAA,CAAE,GAAG,IAAI,CAAC;IAHxB,IAAA,CAAA,yBAAyB,GAA0B,OAAO;IAIjE,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,IAAI,GAAG,CAAA,CAAE;IACV;IAED,IAAI,CAAC,eAAe,GAAG,IAAI;IAC3B,IAAI,CAAC,gBAAgB,GACjB,cAAc,CAAC,IAAI,CAAC,gBAAgB,IAAI,IAAI,CAAC,yBAAyB,CAAC;IAC3E,IAAI,CAAC,gBAAgB,GAAG,cAAc,CAAC,IAAI,CAAC,gBAAgB,CAAC;IAC7D,IAAI,CAAC,eAAe,GAAG,aAAa,CAAC,IAAI,CAAC,eAAe,CAAC;IAC1D,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;MAC3B,IAAI,CAAC,UAAU,GAAG,IAAI;KACvB,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,UAAU,CAAC,EAAE;MACzC,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,UAAU;KAClC,MAAM,IAAI,OAAO,IAAI,CAAC,UAAU,KAAK,QAAQ,EAAE;MAC9C,IAAI,CAAC,UAAU,GAAG,CAAC,IAAI,CAAC,UAAU,CAAC;KACpC,MAAM;MACL,MAAM,IAAI,UAAU,CAChB,6DAA6D,GAC7D,WAAW,IAAI,CAAC,UAAU,EAAE,CAAC;IAClC;EACH;EAEA,KAAK,CAAC,UAAyB,EAAA;IAC7B,UAAU,GAAG,kBAAkB,CAAC,UAAU,CAAC;IAC3C,MAAM,UAAU,GAAU,UAAU,CAAC,KAAK,CAAC,CAAC,CAAC;IAC7C,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;MAC3B,KAAK,MAAM,CAAC,IAAI,IAAI,CAAC,UAAU,EAAE;QAC/B,UAAU,CAAC,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC;MACtB;IACF;IACD,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,SAAS,CACvB,OAAO,EAAE,UAAU,EAAE,SAAS,EAAE,IAAI,CAAC,gBAAgB,EACrD,IAAI,CAAC,gBAAgB,EAAE,IAAI,EAAE,IAAI,CAAC,eAAe,CAAC;IACtD;IACA,MAAM,IAAI,GAA6B,CAAA,CAAE;IACzC,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;MAC3B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,UAAU,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;QAC1C,IAAI,CAAC,CAAC,CAAC,GAAG,UAAU,CAAC,CAAC,CAAC;MACxB;IACF;IACD,IAAI,CAAC,SAAS,GAAG,CAAC,IAAI,SAAS,CAAC;MAC9B,IAAI,EAAE,UAAU,CAAC,MAAM;MACvB;KACD,CAAC,CAAC;IACH,IAAI,CAAC,KAAK,GAAG,IAAI;EACnB;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,MAAM,GAAG,mBAAmB,CAAC,MAAM,CAAC;IACpC,OAAO,KAAK,CAAC,MAAM,EAAE,IAAI,CAAC,KAAK,CAAC,IAAI,EAAE,CAAC;EACzC;EAEA,SAAS,GAAA;IACP,MAAM,MAAM,GAA6B;MACvC,gBAAgB,EAAE,oBAAoB,CAAC,IAAI,CAAC,gBAAgB,CAAC;MAC7D,gBAAgB,EAAE,oBAAoB,CAAC,IAAI,CAAC,gBAAgB,CAAC;MAC7D,eAAe,EAAE,mBAAmB,CAAC,IAAI,CAAC,eAAe,CAAC;MAC1D,UAAU,EAAE,IAAI,CAAC;KAClB;IACD,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;;AA1EA;AACO,KAAA,CAAA,SAAS,GAAG,OAAO;AA2E5B,aAAa,CAAC,aAAa,CAAC,KAAK,CAAC;AASlC,OAAM,MAAO,GAAI,SAAQ,KAAK,CAAA;EAO5B,WAAA,CAAY,IAAmB,EAAA;IAC7B,KAAK,CAAC,IAAI,IAAI,IAAI,GAAG,CAAA,CAAE,GAAG,IAAI,CAAC;IAHxB,IAAA,CAAA,aAAa,GAAG,GAAG;IAI1B,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,IAAI,GAAG,CAAA,CAAE;IACV;IAED,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,IAAI,IAAI,CAAC,KAAK,KAAK,IAAI,CAAC,aAAa,EAAE;MAC3D,MAAM,IAAI,mBAAmB,CACzB,4BAA4B,IAAI,CAAC,KAAK,4BAA4B,GAClE,gBAAgB,CAAC;IACtB;IAED,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,IAAI,IAAI,GAAG,IAAI,CAAC,aAAa,GAAG,IAAI,CAAC,KAAK;EACnE;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,MAAM,CAAC,GAAG,mBAAmB,CAAC,MAAM,CAAC;IACrC,OAAO,GAAG,CAAC,CAAC,CAAC;EACf;EAEA,kBAAkB,CAAC,UAAyB,EAAA;IAC1C,OAAO,UAAU;EACnB;EAEA,SAAS,GAAA;IACP,MAAM,MAAM,GAA6B;MAAC,KAAK,EAAE,IAAI,CAAC;IAAK,CAAC;IAC5D,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;;AAnCA;AACO,GAAA,CAAA,SAAS,GAAG,KAAK;AAoC1B,aAAa,CAAC,aAAa,CAAC,GAAG,CAAC;AAShC,OAAM,MAAO,eAAgB,SAAQ,KAAK,CAAA;EAOxC,WAAA,CAAY,IAA+B,EAAA;IACzC,KAAK,CAAC,IAAI,IAAI,IAAI,GAAG,CAAA,CAAE,GAAG,IAAI,CAAC;IAHxB,IAAA,CAAA,aAAa,GAAG,GAAG;IAI1B,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,IAAI,GAAG,CAAA,CAAE;IACV;IAED,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,IAAI,IAAI,GAAG,IAAI,CAAC,aAAa,GAAG,IAAI,CAAC,KAAK;EACnE;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,MAAM,CAAC,GAAG,mBAAmB,CAAC,MAAM,CAAC;IACrC,OAAO,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,EAAE,SAAS,CAAC,CAAC;EACtD;EAEA,kBAAkB,CAAC,UAAyB,EAAA;IAC1C,OAAO,UAAU;EACnB;EAEA,SAAS,GAAA;IACP,MAAM,MAAM,GAA6B;MAAC,KAAK,EAAE,IAAI,CAAC;IAAK,CAAC;IAC5D,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;;AA7BA;AACO,eAAA,CAAA,SAAS,GAAG,iBAAiB;AA8BtC,aAAa,CAAC,aAAa,CAAC,eAAe,CAAC;AAU5C,OAAM,MAAO,OAAQ,SAAQ,KAAK,CAAA;EAOhC,WAAA,CAAY,IAAuB,EAAA;IACjC,KAAK,CAAC,IAAI,IAAI,IAAI,GAAG,CAAA,CAAE,GAAG,IAAI,CAAC;IAHxB,IAAA,CAAA,YAAY,GAAG,GAAG;IAIzB,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,IAAI,GAAG,CAAA,CAAE;IACV;IACD,IAAI,CAAC,OAAO,GAAG,IAAI,iBAAiB,EAAE,CAAC,KAAK;IAC5C,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,GAAG,IAAI,CAAC,YAAY,GAAG,IAAI,CAAC,IAAI;EAC/D;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,MAAM,CAAC,GAAG,mBAAmB,CAAC,MAAM,CAAC;IACrC,OAAO,IAAI,CAAC,OAAO,CAAC,CAAC,EAAE,IAAI,CAAC,IAAI,CAAC;EACnC;EAEA,kBAAkB,CAAC,UAAyB,EAAA;IAC1C,OAAO,UAAU;EACnB;EAEA,SAAS,GAAA;IACP,MAAM,MAAM,GAA6B;MAAC,IAAI,EAAE,IAAI,CAAC;IAAI,CAAC;IAC1D,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;;AA7BA;AACO,OAAA,CAAA,SAAS,GAAG,SAAS;AA8B9B,aAAa,CAAC,aAAa,CAAC,OAAO,CAAC","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n *  Advanced activation layers.\n */\nimport { clipByValue, elu, leakyRelu, prelu, relu, serialization } from '@tensorflow/tfjs-core';\nimport { Softmax as softmaxActivation } from '../activations';\nimport { cast } from '../backend/tfjs_backend';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nexport class ReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.supportsMasking = true;\n        if (args != null) {\n            this.maxValue = args.maxValue;\n        }\n    }\n    call(inputs, kwargs) {\n        inputs = getExactlyOneTensor(inputs);\n        let output = relu(inputs);\n        if (this.maxValue != null) {\n            output = clipByValue(output, 0, this.maxValue);\n        }\n        return output;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { maxValue: this.maxValue };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nReLU.className = 'ReLU';\nserialization.registerClass(ReLU);\nexport class LeakyReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_ALPHA = 0.3;\n        if (args == null) {\n            args = {};\n        }\n        this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return leakyRelu(x, this.alpha);\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { alpha: this.alpha };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nLeakyReLU.className = 'LeakyReLU';\nserialization.registerClass(LeakyReLU);\nexport class PReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_ALPHA_INITIALIZER = 'zeros';\n        if (args == null) {\n            args = {};\n        }\n        this.supportsMasking = true;\n        this.alphaInitializer =\n            getInitializer(args.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER);\n        this.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n        this.alphaConstraint = getConstraint(args.alphaConstraint);\n        if (args.sharedAxes == null) {\n            this.sharedAxes = null;\n        }\n        else if (Array.isArray(args.sharedAxes)) {\n            this.sharedAxes = args.sharedAxes;\n        }\n        else if (typeof args.sharedAxes === 'number') {\n            this.sharedAxes = [args.sharedAxes];\n        }\n        else {\n            throw new ValueError(`Expected sharedAxes to be a number or an array of numbers, ` +\n                `but got ${args.sharedAxes}`);\n        }\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const paramShape = inputShape.slice(1);\n        if (this.sharedAxes != null) {\n            for (const i of this.sharedAxes) {\n                paramShape[i - 1] = 1;\n            }\n        }\n        this.alpha = this.addWeight('alpha', paramShape, 'float32', this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint);\n        // Set input spec.\n        const axes = {};\n        if (this.sharedAxes != null) {\n            for (let i = 1; i < inputShape.length; ++i) {\n                axes[i] = inputShape[i];\n            }\n        }\n        this.inputSpec = [new InputSpec({\n                ndim: inputShape.length,\n                axes,\n            })];\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        inputs = getExactlyOneTensor(inputs);\n        return prelu(inputs, this.alpha.read());\n    }\n    getConfig() {\n        const config = {\n            alphaInitializer: serializeInitializer(this.alphaInitializer),\n            alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n            alphaConstraint: serializeConstraint(this.alphaConstraint),\n            sharedAxes: this.sharedAxes\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nPReLU.className = 'PReLU';\nserialization.registerClass(PReLU);\nexport class ELU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_ALPHA = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        if (args.alpha != null && args.alpha !== this.DEFAULT_ALPHA) {\n            throw new NotImplementedError(`Non-default alpha value (${args.alpha}) is not supported by the ` +\n                `ELU layer yet.`);\n        }\n        this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return elu(x);\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { alpha: this.alpha };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nELU.className = 'ELU';\nserialization.registerClass(ELU);\nexport class ThresholdedReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_THETA = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        this.theta = args.theta == null ? this.DEFAULT_THETA : args.theta;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return x.mul(cast(x.greater(this.theta), 'float32'));\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { theta: this.theta };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nThresholdedReLU.className = 'ThresholdedReLU';\nserialization.registerClass(ThresholdedReLU);\nexport class Softmax extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_AXIS = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        this.softmax = new softmaxActivation().apply;\n        this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return this.softmax(x, this.axis);\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { axis: this.axis };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nSoftmax.className = 'Softmax';\nserialization.registerClass(Softmax);\n//# sourceMappingURL=advanced_activations.js.map"]},"metadata":{},"sourceType":"module"}
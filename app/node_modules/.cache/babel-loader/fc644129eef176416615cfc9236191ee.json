{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Normalization layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { moments, serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nexport function batchNormalization(x, mean, variance, beta, gamma) {\n  let epsilon = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 1e-3;\n  let out;\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 3) {\n    // TODO(cais): Check rank; give proper error message.\n    out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n  } else {\n    throw new NotImplementedError(`batchNormalization is not implemented for array of rank ${x.rank} ` + `yet`);\n  }\n  return out;\n}\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  let epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(() => {\n    const meanAndVariance = tfc.moments(x, reductionAxes);\n    const mean = meanAndVariance.mean;\n    const variance = meanAndVariance.variance;\n    const normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  let epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(() => {\n    const meanAndVariance = tfc.moments(x, reductionAxes);\n    const mean = meanAndVariance.mean;\n    const variance = meanAndVariance.variance;\n    const targetShape = [];\n    for (const axis of math_utils.range(0, x.rank)) {\n      if (reductionAxes.indexOf(axis) !== -1) {\n        targetShape.push(1);\n      } else {\n        targetShape.push(x.shape[axis]);\n      }\n    }\n    const broadcastMean = mean.reshape(targetShape);\n    const broadcastVariance = variance.reshape(targetShape);\n    const broadcastGamma = gamma == null ? null : gamma.reshape(targetShape);\n    const broadcastBeta = beta == null ? null : beta.reshape(targetShape);\n    const normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nexport function normalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  let epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  if (util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  }\n}\nexport class BatchNormalization extends Layer {\n  constructor(args) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n    this.supportsMasking = true;\n    this.axis = args.axis == null ? -1 : args.axis;\n    this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.movingMeanInitializer = getInitializer(args.movingMeanInitializer || 'zeros');\n    this.movingVarianceInitializer = getInitializer(args.movingVarianceInitializer || 'ones');\n    this.betaConstraint = getConstraint(args.betaConstraint);\n    this.gammaConstraint = getConstraint(args.gammaConstraint);\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n  }\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const axis = this.axis >= 0 ? this.axis : this.axis + inputShape.length;\n    const dim = inputShape[axis];\n    if (dim == null) {\n      throw new ValueError(`Axis ${axis} of input tensor should have a defined dimension but ` + `the layer received an input with shape ` + `${JSON.stringify(inputShape)}.`);\n    }\n    this.inputSpec = [new InputSpec({\n      ndim: inputShape.length,\n      axes: {\n        [axis]: dim\n      }\n    })];\n    const shape = [dim];\n    if (this.scale) {\n      this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n    }\n    if (this.center) {\n      this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n    }\n    this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n    this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n    this.built = true;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      const input = getExactlyOneTensor(inputs);\n      const inputShape = input.shape;\n      const ndim = inputShape.length;\n      const reductionAxes = math_utils.range(0, ndim);\n      const axis = this.axis >= 0 ? this.axis : this.axis + ndim;\n      reductionAxes.splice(axis, 1);\n      const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n      broadcastShape[axis] = inputShape[axis];\n      const sortedReductionAxes = reductionAxes.slice();\n      sortedReductionAxes.sort();\n      const needsBroadcasting = !util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n      const normalizeInference = () => {\n        if (needsBroadcasting) {\n          const broadcastMovingMean = this.movingMean.read().reshape(broadcastShape);\n          const broadcastMovingVariance = this.movingVariance.read().reshape(broadcastShape);\n          const broadcastBeta = this.center ? this.beta.read().reshape(broadcastShape) : null;\n          const broadcastGamma = this.scale ? this.gamma.read().reshape(broadcastShape) : null;\n          return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, this.epsilon);\n        } else {\n          return batchNormalization(input, this.movingMean.read(), this.movingVariance.read(), this.beta == null ? null : this.beta.read(), this.gamma == null ? null : this.gamma.read(), this.epsilon);\n        }\n      };\n      if (!training) {\n        return normalizeInference();\n      }\n      const [normedTraining, mean, variance] = normalizeBatchInTraining(input, this.gamma.read(), this.beta.read(), reductionAxes, this.epsilon);\n      const doMovingAverage = (variable, value, momentum) => {\n        tfc.tidy(() => {\n          const decay = 1 - momentum;\n          const origValue = variable.read();\n          const updateDelta = origValue.sub(value).mul(decay);\n          variable.write(origValue.sub(updateDelta));\n        });\n      };\n      // Perform updates to moving mean and moving variance for training.\n      // Porting Note: In PyKeras, these updates to `movingMean` and\n      //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n      //   `update`s using the `add_update()` method. Here we do it imperatively\n      //   and encapsulate the updates in a function that is invoked\n      //   immediately.\n      const updateMovingMeanAndVariance = () => {\n        doMovingAverage(this.movingMean, mean, this.momentum);\n        doMovingAverage(this.movingVariance, variance, this.momentum);\n      };\n      updateMovingMeanAndVariance();\n      return normedTraining;\n    });\n  }\n  getConfig() {\n    const config = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n      movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n      betaConstraint: serializeConstraint(this.betaConstraint),\n      gammaConstraint: serializeConstraint(this.gammaConstraint)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nBatchNormalization.className = 'BatchNormalization';\nserialization.registerClass(BatchNormalization);\nexport class LayerNormalization extends Layer {\n  constructor(args) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n    this.axis = args.axis == null ? -1 : args.axis;\n    if (typeof this.axis === 'number') {\n      if (!Number.isInteger(this.axis)) {\n        throw new Error(`Expected axis to be an integer, but received ${this.axis}`);\n      }\n    } else if (Array.isArray(this.axis)) {\n      for (const axis of this.axis) {\n        if (!Number.isInteger(axis)) {\n          throw new Error(`Expected axis to be an array of integers, ` + `but received ${JSON.stringify(this.axis)}`);\n        }\n      }\n    } else {\n      throw new Error(`Expected axis to be an integer or an array of integers, ` + `but received ${JSON.stringify(this.axis)}`);\n    }\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    this.supportsMasking = true;\n  }\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const nDims = inputShape.length;\n    // Convert axis to array and resolve negatives.\n    if (typeof this.axis === 'number') {\n      this.axis = [this.axis];\n    }\n    for (let i = 0; i < this.axis.length; ++i) {\n      if (this.axis[i] < 0) {\n        this.axis[i] += nDims;\n      }\n    }\n    // Further validate axes.\n    for (const axis of this.axis) {\n      if (axis < 0 || axis >= nDims) {\n        throw new Error(`Invalid axis: ${axis}`);\n      }\n    }\n    if (this.axis.length !== generic_utils.unique(this.axis).length) {\n      throw new Error(`Found duplicate axes in: ${this.axis}`);\n    }\n    const paramShape = this.axis.map(axis => inputShape[axis]);\n    const trainable = true;\n    if (this.scale) {\n      this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n    } else {\n      this.gamma = null;\n    }\n    if (this.center) {\n      this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n    } else {\n      this.beta = null;\n    }\n    this.built = true;\n  }\n  call(inputs, kwargs) {\n    const input = getExactlyOneTensor(inputs);\n    const inputShape = input.shape;\n    const nDims = inputShape.length;\n    return tidy(() => {\n      const keepDims = true;\n      let {\n        mean,\n        variance\n      } = moments(input, this.axis, keepDims);\n      const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n      for (const dim of this.axis) {\n        broadcastShape[dim] = inputShape[dim];\n      }\n      const broadcast = v => {\n        if (v != null && v.shape.length !== nDims && this.axis !== [nDims - 1]) {\n          return v.reshape(broadcastShape);\n        } else {\n          return v;\n        }\n      };\n      let scale = broadcast(this.gamma.read());\n      let offset = broadcast(this.beta.read());\n      // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n      // is a workaround for the limitation of core's batchNormalization?d don't\n      // support broadcasting in their gradients. In addition, the tiling is\n      // necessary to ensure correctness on the browser CPU backend regardless\n      // of forward or backward computation. Remove this workaround once the\n      // limitation is addressed. See .\n      const momentsTiling = [];\n      const scaleOffsetTiling = [];\n      for (let i = 0; i < nDims; ++i) {\n        if (this.axis.indexOf(i) !== -1) {\n          momentsTiling.push(inputShape[i]);\n          scaleOffsetTiling.push(1);\n        } else {\n          momentsTiling.push(1);\n          scaleOffsetTiling.push(inputShape[i]);\n        }\n      }\n      mean = mean.tile(momentsTiling);\n      variance = variance.tile(momentsTiling);\n      scale = scale.tile(scaleOffsetTiling);\n      offset = offset.tile(scaleOffsetTiling);\n      return batchNormalization(input, mean, variance, offset, scale, this.epsilon);\n    });\n  }\n  getConfig() {\n    const config = {\n      axis: this.axis,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nLayerNormalization.className = 'LayerNormalization';\nserialization.registerClass(LayerNormalization);","map":{"version":3,"sources":["../../src/layers/normalization.ts"],"names":[],"mappings":"AAAA;;;;;;;;AAQG;AAEH;;AAEG;AAEH,OAAO,KAAK,GAAG,MAAM,uBAAuB;AAC5C,SAAQ,OAAO,EAAE,aAAa,EAAkD,IAAI,EAAE,IAAI,QAAO,uBAAuB;AAExH,SAA0C,aAAa,EAAE,mBAAmB,QAAO,gBAAgB;AACnG,SAAQ,SAAS,EAAE,KAAK,QAAkB,oBAAoB;AAC9D,SAAQ,mBAAmB,EAAE,UAAU,QAAO,WAAW;AACzD,SAAQ,cAAc,EAAsC,oBAAoB,QAAO,iBAAiB;AAExG,SAAQ,cAAc,EAAsC,oBAAoB,QAAO,iBAAiB;AAExG,OAAO,KAAK,aAAa,MAAM,wBAAwB;AACvD,OAAO,KAAK,UAAU,MAAM,qBAAqB;AACjD,SAAQ,kBAAkB,EAAE,mBAAmB,QAAO,sBAAsB;AAG5E;;;;;;;;;;;;;AAaG;AACH,OAAM,SAAU,kBAAkB,CAC9B,CAAS,EAAE,IAAY,EAAE,QAAgB,EAAE,IAAa,EAAE,KAAc,EAC1D;EAAA,IAAd,OAAO,uEAAG,IAAI;EAChB,IAAI,GAAW;EACf,IAAI,CAAC,CAAC,IAAI,KAAK,CAAC,EAAE;IAChB,GAAG,GAAG,GAAG,CAAC,WAAW,CACjB,CAAa,EAAE,IAA2B,EAC1C,QAA+B,EAAE,IAA2B,EAC5D,KAA4B,EAAE,OAAO,CAAC;GAC3C,MAAM,IAAI,CAAC,CAAC,IAAI,KAAK,CAAC,EAAE;IACvB;IACA,GAAG,GAAG,GAAG,CAAC,WAAW,CACjB,CAAa,EAAE,IAA2B,EAC1C,QAA+B,EAAE,IAA2B,EAC5D,KAA4B,EAAE,OAAO,CAAC;GAC3C,MAAM,IAAI,CAAC,CAAC,IAAI,KAAK,CAAC,EAAE;IACvB,GAAG,GAAG,GAAG,CAAC,WAAW,CACjB,CAAa,EAAE,IAA2B,EAC1C,QAA+B,EAAE,IAA2B,EAC5D,KAA4B,EAAE,OAAO,CAAC;GAC3C,MAAM;IACL,MAAM,IAAI,mBAAmB,CACzB,2DAA2D,CAAC,CAAC,IAAI,GAAG,GACpE,KAAK,CAAC;EACX;EACD,OAAO,GAAG;AACZ;AAEA;;;;;;;;;;;;;;;;AAgBG;AACH,SAAS,+BAA+B,CACpC,CAAS,EAAE,KAAa,EAAE,IAAY,EAAE,aAAuB,EACjD;EAAA,IAAd,OAAO,uEAAG,IAAI;EAChB,OAAO,IAAI,CAAC,MAAK;IACR,MAAM,eAAe,GAAG,GAAG,CAAC,OAAO,CAAC,CAAC,EAAE,aAAa,CAAC;IACrD,MAAM,IAAI,GAAG,eAAe,CAAC,IAAI;IACjC,MAAM,QAAQ,GAAG,eAAe,CAAC,QAAQ;IACzC,MAAM,MAAM,GACR,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,KAAK,EAAE,OAAO,CAAC;IAC/D,OAAO,CAAC,MAAM,EAAE,IAAI,EAAE,QAAQ,CAAC;EACjC,CAAC,CAA6B;AACvC;AAEA;;;;;;;;;;;;;;;;AAgBG;AACH,SAAS,iCAAiC,CACtC,CAAS,EAAE,KAAa,EAAE,IAAY,EAAE,aAAuB,EACjD;EAAA,IAAd,OAAO,uEAAG,IAAI;EAChB,OAAO,IAAI,CAAC,MAAK;IACR,MAAM,eAAe,GAAG,GAAG,CAAC,OAAO,CAAC,CAAC,EAAE,aAAa,CAAC;IACrD,MAAM,IAAI,GAAG,eAAe,CAAC,IAAI;IACjC,MAAM,QAAQ,GAAG,eAAe,CAAC,QAAQ;IACzC,MAAM,WAAW,GAAa,EAAE;IAChC,KAAK,MAAM,IAAI,IAAI,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,IAAI,CAAC,EAAE;MAC9C,IAAI,aAAa,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;QACtC,WAAW,CAAC,IAAI,CAAC,CAAC,CAAC;OACpB,MAAM;QACL,WAAW,CAAC,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;MAChC;IACF;IACD,MAAM,aAAa,GAAG,IAAI,CAAC,OAAO,CAAC,WAAW,CAAC;IAC/C,MAAM,iBAAiB,GAAG,QAAQ,CAAC,OAAO,CAAC,WAAW,CAAC;IACvD,MAAM,cAAc,GAChB,KAAK,IAAI,IAAI,GAAG,IAAI,GAAG,KAAK,CAAC,OAAO,CAAC,WAAW,CAAC;IACrD,MAAM,aAAa,GACf,IAAI,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,OAAO,CAAC,WAAW,CAAC;IACnD,MAAM,MAAM,GAAG,kBAAkB,CAC7B,CAAC,EAAE,aAAa,EAAE,iBAAiB,EAAE,aAAa,EAClD,cAAc,EAAE,OAAO,CAAC;IAC5B,OAAO,CAAC,MAAM,EAAE,IAAI,EAAE,QAAQ,CAAC;EACjC,CAAC,CAA6B;AACvC;AAEA;;;;;;;;;;AAUG;AACH,OAAM,SAAU,wBAAwB,CACpC,CAAS,EAAE,KAAa,EAAE,IAAY,EAAE,aAAuB,EACjD;EAAA,IAAd,OAAO,uEAAG,IAAI;EAChB,IAAI,IAAI,CAAC,WAAW,CACZ,aAAa,CAAC,KAAK,EAAE,CAAC,IAAI,EAAE,EAAE,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,IAAI,GAAG,CAAC,CAAC,CAAC,EAAE;IACtE,OAAO,+BAA+B,CAClC,CAAC,EAAE,KAAK,EAAE,IAAI,EAAE,aAAa,EAAE,OAAO,CAAC;GAC5C,MAAM;IACL,OAAO,iCAAiC,CACpC,CAAC,EAAE,KAAK,EAAE,IAAI,EAAE,aAAa,EAAE,OAAO,CAAC;EAC5C;AACH;AAoFA,OAAM,MAAO,kBAAmB,SAAQ,KAAK,CAAA;EAqB3C,WAAA,CAAY,IAAkC,EAAA;IAC5C,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,IAAI,GAAG,CAAA,CAAE;IACV;IACD,KAAK,CAAC,IAAI,CAAC;IAEX,IAAI,CAAC,eAAe,GAAG,IAAI;IAC3B,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,GAAG,CAAC,CAAC,GAAG,IAAI,CAAC,IAAI;IAC9C,IAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,QAAQ,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,QAAQ;IAC5D,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,OAAO;IACzD,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,MAAM;IACtD,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,KAAK;IACnD,IAAI,CAAC,eAAe,GAAG,cAAc,CAAC,IAAI,CAAC,eAAe,IAAI,OAAO,CAAC;IACtE,IAAI,CAAC,gBAAgB,GAAG,cAAc,CAAC,IAAI,CAAC,gBAAgB,IAAI,MAAM,CAAC;IACvE,IAAI,CAAC,qBAAqB,GACtB,cAAc,CAAC,IAAI,CAAC,qBAAqB,IAAI,OAAO,CAAC;IACzD,IAAI,CAAC,yBAAyB,GAC1B,cAAc,CAAC,IAAI,CAAC,yBAAyB,IAAI,MAAM,CAAC;IAC5D,IAAI,CAAC,cAAc,GAAG,aAAa,CAAC,IAAI,CAAC,cAAc,CAAC;IACxD,IAAI,CAAC,eAAe,GAAG,aAAa,CAAC,IAAI,CAAC,eAAe,CAAC;IAC1D,IAAI,CAAC,eAAe,GAAG,cAAc,CAAC,IAAI,CAAC,eAAe,CAAC;IAC3D,IAAI,CAAC,gBAAgB,GAAG,cAAc,CAAC,IAAI,CAAC,gBAAgB,CAAC;EAC/D;EAEO,KAAK,CAAC,UAAyB,EAAA;IACpC,UAAU,GAAG,kBAAkB,CAAC,UAAU,CAAC;IAC3C,MAAM,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,CAAC,GAAG,IAAI,CAAC,IAAI,GAAI,IAAI,CAAC,IAAI,GAAG,UAAU,CAAC,MAAO;IACzE,MAAM,GAAG,GAAG,UAAU,CAAC,IAAI,CAAC;IAC5B,IAAI,GAAG,IAAI,IAAI,EAAE;MACf,MAAM,IAAI,UAAU,CAChB,QAAQ,IAAI,uDAAuD,GACnE,yCAAyC,GACzC,GAAG,IAAI,CAAC,SAAS,CAAC,UAAU,CAAC,GAAG,CAAC;IACtC;IACD,IAAI,CAAC,SAAS,GACV,CAAC,IAAI,SAAS,CAAC;MAAC,IAAI,EAAE,UAAU,CAAC,MAAM;MAAE,IAAI,EAAE;QAAC,CAAC,IAAI,GAAG;MAAG;IAAC,CAAC,CAAC,CAAC;IACnE,MAAM,KAAK,GAAG,CAAC,GAAG,CAAC;IACnB,IAAI,IAAI,CAAC,KAAK,EAAE;MACd,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,SAAS,CACvB,OAAO,EAAE,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC,gBAAgB,EAAE,IAAI,CAAC,gBAAgB,EAClE,IAAI,EAAE,IAAI,CAAC,eAAe,CAAC;IAChC;IACD,IAAI,IAAI,CAAC,MAAM,EAAE;MACf,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,SAAS,CACtB,MAAM,EAAE,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC,eAAe,EAAE,IAAI,CAAC,eAAe,EAAE,IAAI,EACrE,IAAI,CAAC,cAAc,CAAC;IACzB;IACD,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,SAAS,CAC5B,aAAa,EAAE,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC,qBAAqB,EAAE,IAAI,EAAE,KAAK,CAAC;IACxE,IAAI,CAAC,cAAc,GAAG,IAAI,CAAC,SAAS,CAChC,iBAAiB,EAAE,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC,yBAAyB,EAAE,IAAI,EACpE,KAAK,CAAC;IACV,IAAI,CAAC,KAAK,GAAG,IAAI;EACnB;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,MAAM,QAAQ,GAAG,MAAM,CAAC,UAAU,CAAC,IAAI,IAAI,GAAG,KAAK,GAAG,MAAM,CAAC,UAAU,CAAC;MACxE,MAAM,KAAK,GAAG,mBAAmB,CAAC,MAAM,CAAC;MACzC,MAAM,UAAU,GAAG,KAAK,CAAC,KAAK;MAC9B,MAAM,IAAI,GAAG,UAAU,CAAC,MAAM;MAC9B,MAAM,aAAa,GAAG,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,IAAI,CAAC;MAC/C,MAAM,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,CAAC,GAAG,IAAI,CAAC,IAAI,GAAI,IAAI,CAAC,IAAI,GAAG,IAAK;MAC5D,aAAa,CAAC,MAAM,CAAC,IAAI,EAAE,CAAC,CAAC;MAC7B,MAAM,cAAc,GAAG,aAAa,CAAC,YAAY,CAAC,CAAC,EAAE,IAAI,CAAC;MAC1D,cAAc,CAAC,IAAI,CAAC,GAAG,UAAU,CAAC,IAAI,CAAC;MAEvC,MAAM,mBAAmB,GAAG,aAAa,CAAC,KAAK,EAAE;MACjD,mBAAmB,CAAC,IAAI,EAAE;MAC1B,MAAM,iBAAiB,GAAG,CAAC,IAAI,CAAC,WAAW,CACvC,mBAAmB,EAAE,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE,IAAI,GAAG,CAAC,CAAC,CAAC;MAEtE,MAAM,kBAAkB,GAAiB,MAAK;QAC5C,IAAI,iBAAiB,EAAE;UACrB,MAAM,mBAAmB,GACrB,IAAI,CAAC,UAAU,CAAC,IAAI,EAAE,CAAC,OAAO,CAAC,cAAc,CAAC;UAClD,MAAM,uBAAuB,GACzB,IAAI,CAAC,cAAc,CAAC,IAAI,EAAE,CAAC,OAAO,CAAC,cAAc,CAAC;UACtD,MAAM,aAAa,GACf,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,IAAI,CAAC,IAAI,EAAE,CAAC,OAAO,CAAC,cAAc,CAAC,GAAG,IAAI;UACjE,MAAM,cAAc,GAChB,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,CAAC,IAAI,EAAE,CAAC,OAAO,CAAC,cAAc,CAAC,GAAG,IAAI;UACjE,OAAO,kBAAkB,CACrB,KAAK,EAAE,mBAAmB,EAAE,uBAAuB,EACnD,aAAa,EAAE,cAAc,EAAE,IAAI,CAAC,OAAO,CAAC;SACjD,MAAM;UACL,OAAO,kBAAkB,CACrB,KAAK,EAAE,IAAI,CAAC,UAAU,CAAC,IAAI,EAAE,EAAE,IAAI,CAAC,cAAc,CAAC,IAAI,EAAE,EACzD,IAAI,CAAC,IAAI,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC,IAAI,EAAE,EAC3C,IAAI,CAAC,KAAK,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,KAAK,CAAC,IAAI,EAAE,EAAE,IAAI,CAAC,OAAO,CAAC;QACjE;MACH,CAAC;MAED,IAAI,CAAC,QAAQ,EAAE;QACb,OAAO,kBAAkB,EAAE;MAC5B;MAED,MAAM,CAAC,cAAc,EAAE,IAAI,EAAE,QAAQ,CAAC,GAAG,wBAAwB,CAC7D,KAAK,EAAE,IAAI,CAAC,KAAK,CAAC,IAAI,EAAE,EAAE,IAAI,CAAC,IAAI,CAAC,IAAI,EAAE,EAAE,aAAa,EACzD,IAAI,CAAC,OAAO,CAAC;MAEjB,MAAM,eAAe,GACjB,CAAC,QAAuB,EAAE,KAAa,EAAE,QAAgB,KAAU;QACjE,GAAG,CAAC,IAAI,CAAC,MAAK;UACZ,MAAM,KAAK,GAAG,CAAC,GAAG,QAAQ;UAC1B,MAAM,SAAS,GAAG,QAAQ,CAAC,IAAI,EAAE;UACjC,MAAM,WAAW,GAAG,SAAS,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC;UACnD,QAAQ,CAAC,KAAK,CAAC,SAAS,CAAC,GAAG,CAAC,WAAW,CAAC,CAAC;QAC5C,CAAC,CAAC;MACJ,CAAC;MAEL;MACA;MACA;MACA;MACA;MACA;MACA,MAAM,2BAA2B,GAAG,MAAK;QACvC,eAAe,CAAC,IAAI,CAAC,UAAU,EAAE,IAAI,EAAE,IAAI,CAAC,QAAQ,CAAC;QACrD,eAAe,CAAC,IAAI,CAAC,cAAc,EAAE,QAAQ,EAAE,IAAI,CAAC,QAAQ,CAAC;MAC/D,CAAC;MACD,2BAA2B,EAAE;MAE7B,OAAO,cAAc;IACvB,CAAC,CAAC;EACJ;EAEA,SAAS,GAAA;IACP,MAAM,MAAM,GAA6B;MACvC,IAAI,EAAE,IAAI,CAAC,IAAI;MACf,QAAQ,EAAE,IAAI,CAAC,QAAQ;MACvB,OAAO,EAAE,IAAI,CAAC,OAAO;MACrB,MAAM,EAAE,IAAI,CAAC,MAAM;MACnB,KAAK,EAAE,IAAI,CAAC,KAAK;MACjB,eAAe,EAAE,oBAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;MAC3D,gBAAgB,EAAE,oBAAoB,CAAC,IAAI,CAAC,gBAAgB,CAAC;MAC7D,qBAAqB,EAAE,oBAAoB,CAAC,IAAI,CAAC,qBAAqB,CAAC;MACvE,yBAAyB,EACrB,oBAAoB,CAAC,IAAI,CAAC,yBAAyB,CAAC;MACxD,eAAe,EAAE,oBAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;MAC3D,gBAAgB,EAAE,oBAAoB,CAAC,IAAI,CAAC,gBAAgB,CAAC;MAC7D,cAAc,EAAE,mBAAmB,CAAC,IAAI,CAAC,cAAc,CAAC;MACxD,eAAe,EAAE,mBAAmB,CAAC,IAAI,CAAC,eAAe;KAC1D;IACD,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;;AAvKA;AACO,kBAAA,CAAA,SAAS,GAAG,oBAAoB;AAwKzC,aAAa,CAAC,aAAa,CAAC,kBAAkB,CAAC;AAkD/C,OAAM,MAAO,kBAAmB,SAAQ,KAAK,CAAA;EAgB3C,WAAA,CAAY,IAAkC,EAAA;IAC5C,IAAI,IAAI,IAAI,IAAI,EAAE;MAChB,IAAI,GAAG,CAAA,CAAE;IACV;IACD,KAAK,CAAC,IAAI,CAAC;IAEX,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,GAAG,CAAC,CAAC,GAAG,IAAI,CAAC,IAAI;IAC9C,IAAI,OAAO,IAAI,CAAC,IAAI,KAAK,QAAQ,EAAE;MACjC,IAAI,CAAC,MAAM,CAAC,SAAS,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE;QAChC,MAAM,IAAI,KAAK,CACX,gDAAgD,IAAI,CAAC,IAAI,EAAE,CAAC;MACjE;KACF,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE;MACnC,KAAK,MAAM,IAAI,IAAI,IAAI,CAAC,IAAI,EAAE;QAC5B,IAAI,CAAC,MAAM,CAAC,SAAS,CAAC,IAAI,CAAC,EAAE;UAC3B,MAAM,IAAI,KAAK,CACX,4CAA4C,GAC5C,gBAAgB,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE,CAAC;QACjD;MACF;KACF,MAAM;MACL,MAAM,IAAI,KAAK,CACX,0DAA0D,GAC1D,gBAAgB,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE,CAAC;IACjD;IAED,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,OAAO;IACzD,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,MAAM;IACtD,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAAC,KAAK;IACnD,IAAI,CAAC,eAAe,GAAG,cAAc,CAAC,IAAI,CAAC,eAAe,IAAI,OAAO,CAAC;IACtE,IAAI,CAAC,gBAAgB,GAAG,cAAc,CAAC,IAAI,CAAC,gBAAgB,IAAI,MAAM,CAAC;IACvE,IAAI,CAAC,eAAe,GAAG,cAAc,CAAC,IAAI,CAAC,eAAe,CAAC;IAC3D,IAAI,CAAC,gBAAgB,GAAG,cAAc,CAAC,IAAI,CAAC,gBAAgB,CAAC;IAE7D,IAAI,CAAC,eAAe,GAAG,IAAI;EAC7B;EAEO,KAAK,CAAC,UAAyB,EAAA;IACpC,UAAU,GAAG,kBAAkB,CAAC,UAAU,CAAC;IAC3C,MAAM,KAAK,GAAG,UAAU,CAAC,MAAM;IAE/B;IACA,IAAI,OAAO,IAAI,CAAC,IAAI,KAAK,QAAQ,EAAE;MACjC,IAAI,CAAC,IAAI,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC;IACxB;IACD,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,IAAI,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;MACzC,IAAI,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC,EAAE;QACpB,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,KAAK;MACtB;IACF;IAED;IACA,KAAK,MAAM,IAAI,IAAI,IAAI,CAAC,IAAI,EAAE;MAC5B,IAAI,IAAI,GAAG,CAAC,IAAI,IAAI,IAAI,KAAK,EAAE;QAC7B,MAAM,IAAI,KAAK,CAAC,iBAAiB,IAAI,EAAE,CAAC;MACzC;IACF;IACD,IAAI,IAAI,CAAC,IAAI,CAAC,MAAM,KAAK,aAAa,CAAC,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,MAAM,EAAE;MAC/D,MAAM,IAAI,KAAK,CAAC,4BAA4B,IAAI,CAAC,IAAI,EAAE,CAAC;IACzD;IAED,MAAM,UAAU,GAAG,IAAI,CAAC,IAAI,CAAC,GAAG,CAAC,IAAI,IAAI,UAAU,CAAC,IAAI,CAAC,CAAa;IAEtE,MAAM,SAAS,GAAG,IAAI;IACtB,IAAI,IAAI,CAAC,KAAK,EAAE;MACd,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,SAAS,CACvB,OAAO,EAAE,UAAU,EAAE,SAAS,EAAE,IAAI,CAAC,gBAAgB,EACrD,IAAI,CAAC,gBAAgB,EAAE,SAAS,CAAC;KACtC,MAAM;MACL,IAAI,CAAC,KAAK,GAAG,IAAI;IAClB;IACD,IAAI,IAAI,CAAC,MAAM,EAAE;MACf,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,SAAS,CACtB,MAAM,EAAE,UAAU,EAAE,SAAS,EAAE,IAAI,CAAC,eAAe,EACnD,IAAI,CAAC,eAAe,EAAE,SAAS,CAAC;KACrC,MAAM;MACL,IAAI,CAAC,IAAI,GAAG,IAAI;IACjB;IAED,IAAI,CAAC,KAAK,GAAG,IAAI;EACnB;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,MAAM,KAAK,GAAG,mBAAmB,CAAC,MAAM,CAAC;IACzC,MAAM,UAAU,GAAG,KAAK,CAAC,KAAK;IAC9B,MAAM,KAAK,GAAG,UAAU,CAAC,MAAM;IAE/B,OAAO,IAAI,CAAC,MAAK;MACf,MAAM,QAAQ,GAAG,IAAI;MACrB,IAAI;QAAC,IAAI;QAAE;MAAQ,CAAC,GAAG,OAAO,CAAC,KAAK,EAAE,IAAI,CAAC,IAAI,EAAE,QAAQ,CAAC;MAC1D,MAAM,cAAc,GAAG,aAAa,CAAC,YAAY,CAAC,CAAC,EAAE,KAAK,CAAC;MAC3D,KAAK,MAAM,GAAG,IAAI,IAAI,CAAC,IAAgB,EAAE;QACvC,cAAc,CAAC,GAAG,CAAC,GAAG,UAAU,CAAC,GAAG,CAAC;MACtC;MAED,MAAM,SAAS,GAAI,CAAS,IAAI;QAC9B,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,KAAK,CAAC,MAAM,KAAK,KAAK,IACrC,IAAI,CAAC,IAAI,KAAK,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE;UAC7B,OAAO,CAAC,CAAC,OAAO,CAAC,cAAc,CAAC;SACjC,MAAM;UACL,OAAO,CAAC;QACT;MACH,CAAC;MAED,IAAI,KAAK,GAAG,SAAS,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,EAAE,CAAC;MACxC,IAAI,MAAM,GAAG,SAAS,CAAC,IAAI,CAAC,IAAI,CAAC,IAAI,EAAE,CAAC;MAExC;MACA;MACA;MACA;MACA;MACA;MACA,MAAM,aAAa,GAAa,EAAE;MAClC,MAAM,iBAAiB,GAAa,EAAE;MACtC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,EAAE,EAAE,CAAC,EAAE;QAC9B,IAAK,IAAI,CAAC,IAAiB,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;UAC7C,aAAa,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC;UACjC,iBAAiB,CAAC,IAAI,CAAC,CAAC,CAAC;SAC1B,MAAM;UACL,aAAa,CAAC,IAAI,CAAC,CAAC,CAAC;UACrB,iBAAiB,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC;QACtC;MACF;MACD,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC,aAAa,CAAC;MAC/B,QAAQ,GAAG,QAAQ,CAAC,IAAI,CAAC,aAAa,CAAC;MACvC,KAAK,GAAG,KAAK,CAAC,IAAI,CAAC,iBAAiB,CAAC;MACrC,MAAM,GAAG,MAAM,CAAC,IAAI,CAAC,iBAAiB,CAAC;MAEvC,OAAO,kBAAkB,CACrB,KAAK,EAAE,IAAI,EAAE,QAAQ,EAAE,MAAM,EAAE,KAAK,EAAE,IAAI,CAAC,OAAO,CAAC;IACzD,CAAC,CAAC;EACJ;EAEA,SAAS,GAAA;IACP,MAAM,MAAM,GAA6B;MACvC,IAAI,EAAE,IAAI,CAAC,IAAI;MACf,OAAO,EAAE,IAAI,CAAC,OAAO;MACrB,MAAM,EAAE,IAAI,CAAC,MAAM;MACnB,KAAK,EAAE,IAAI,CAAC,KAAK;MACjB,eAAe,EAAE,oBAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;MAC3D,gBAAgB,EAAE,oBAAoB,CAAC,IAAI,CAAC,gBAAgB,CAAC;MAC7D,eAAe,EAAE,oBAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;MAC3D,gBAAgB,EAAE,oBAAoB,CAAC,IAAI,CAAC,gBAAgB;KAC7D;IACD,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;;AAnKA;AACO,kBAAA,CAAA,SAAS,GAAG,oBAAoB;AAoKzC,aAAa,CAAC,aAAa,CAAC,kBAAkB,CAAC","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Normalization layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { moments, serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nexport function batchNormalization(x, mean, variance, beta, gamma, epsilon = 1e-3) {\n    let out;\n    if (x.rank === 2) {\n        out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else if (x.rank === 3) {\n        // TODO(cais): Check rank; give proper error message.\n        out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else if (x.rank === 4) {\n        out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else {\n        throw new NotImplementedError(`batchNormalization is not implemented for array of rank ${x.rank} ` +\n            `yet`);\n    }\n    return out;\n}\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n    return tidy(() => {\n        const meanAndVariance = tfc.moments(x, reductionAxes);\n        const mean = meanAndVariance.mean;\n        const variance = meanAndVariance.variance;\n        const normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n        return [normed, mean, variance];\n    });\n}\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n    return tidy(() => {\n        const meanAndVariance = tfc.moments(x, reductionAxes);\n        const mean = meanAndVariance.mean;\n        const variance = meanAndVariance.variance;\n        const targetShape = [];\n        for (const axis of math_utils.range(0, x.rank)) {\n            if (reductionAxes.indexOf(axis) !== -1) {\n                targetShape.push(1);\n            }\n            else {\n                targetShape.push(x.shape[axis]);\n            }\n        }\n        const broadcastMean = mean.reshape(targetShape);\n        const broadcastVariance = variance.reshape(targetShape);\n        const broadcastGamma = gamma == null ? null : gamma.reshape(targetShape);\n        const broadcastBeta = beta == null ? null : beta.reshape(targetShape);\n        const normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n        return [normed, mean, variance];\n    });\n}\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nexport function normalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n    if (util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n        return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n    }\n    else {\n        return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n    }\n}\nexport class BatchNormalization extends Layer {\n    constructor(args) {\n        if (args == null) {\n            args = {};\n        }\n        super(args);\n        this.supportsMasking = true;\n        this.axis = args.axis == null ? -1 : args.axis;\n        this.momentum = args.momentum == null ? 0.99 : args.momentum;\n        this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n        this.center = args.center == null ? true : args.center;\n        this.scale = args.scale == null ? true : args.scale;\n        this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n        this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n        this.movingMeanInitializer =\n            getInitializer(args.movingMeanInitializer || 'zeros');\n        this.movingVarianceInitializer =\n            getInitializer(args.movingVarianceInitializer || 'ones');\n        this.betaConstraint = getConstraint(args.betaConstraint);\n        this.gammaConstraint = getConstraint(args.gammaConstraint);\n        this.betaRegularizer = getRegularizer(args.betaRegularizer);\n        this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const axis = this.axis >= 0 ? this.axis : (this.axis + inputShape.length);\n        const dim = inputShape[axis];\n        if (dim == null) {\n            throw new ValueError(`Axis ${axis} of input tensor should have a defined dimension but ` +\n                `the layer received an input with shape ` +\n                `${JSON.stringify(inputShape)}.`);\n        }\n        this.inputSpec =\n            [new InputSpec({ ndim: inputShape.length, axes: { [axis]: dim } })];\n        const shape = [dim];\n        if (this.scale) {\n            this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n        }\n        if (this.center) {\n            this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n        }\n        this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n        this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            const input = getExactlyOneTensor(inputs);\n            const inputShape = input.shape;\n            const ndim = inputShape.length;\n            const reductionAxes = math_utils.range(0, ndim);\n            const axis = this.axis >= 0 ? this.axis : (this.axis + ndim);\n            reductionAxes.splice(axis, 1);\n            const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n            broadcastShape[axis] = inputShape[axis];\n            const sortedReductionAxes = reductionAxes.slice();\n            sortedReductionAxes.sort();\n            const needsBroadcasting = !util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n            const normalizeInference = () => {\n                if (needsBroadcasting) {\n                    const broadcastMovingMean = this.movingMean.read().reshape(broadcastShape);\n                    const broadcastMovingVariance = this.movingVariance.read().reshape(broadcastShape);\n                    const broadcastBeta = this.center ? this.beta.read().reshape(broadcastShape) : null;\n                    const broadcastGamma = this.scale ? this.gamma.read().reshape(broadcastShape) : null;\n                    return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, this.epsilon);\n                }\n                else {\n                    return batchNormalization(input, this.movingMean.read(), this.movingVariance.read(), this.beta == null ? null : this.beta.read(), this.gamma == null ? null : this.gamma.read(), this.epsilon);\n                }\n            };\n            if (!training) {\n                return normalizeInference();\n            }\n            const [normedTraining, mean, variance] = normalizeBatchInTraining(input, this.gamma.read(), this.beta.read(), reductionAxes, this.epsilon);\n            const doMovingAverage = (variable, value, momentum) => {\n                tfc.tidy(() => {\n                    const decay = 1 - momentum;\n                    const origValue = variable.read();\n                    const updateDelta = origValue.sub(value).mul(decay);\n                    variable.write(origValue.sub(updateDelta));\n                });\n            };\n            // Perform updates to moving mean and moving variance for training.\n            // Porting Note: In PyKeras, these updates to `movingMean` and\n            //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n            //   `update`s using the `add_update()` method. Here we do it imperatively\n            //   and encapsulate the updates in a function that is invoked\n            //   immediately.\n            const updateMovingMeanAndVariance = () => {\n                doMovingAverage(this.movingMean, mean, this.momentum);\n                doMovingAverage(this.movingVariance, variance, this.momentum);\n            };\n            updateMovingMeanAndVariance();\n            return normedTraining;\n        });\n    }\n    getConfig() {\n        const config = {\n            axis: this.axis,\n            momentum: this.momentum,\n            epsilon: this.epsilon,\n            center: this.center,\n            scale: this.scale,\n            betaInitializer: serializeInitializer(this.betaInitializer),\n            gammaInitializer: serializeInitializer(this.gammaInitializer),\n            movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n            movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n            betaRegularizer: serializeRegularizer(this.betaRegularizer),\n            gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n            betaConstraint: serializeConstraint(this.betaConstraint),\n            gammaConstraint: serializeConstraint(this.gammaConstraint)\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nBatchNormalization.className = 'BatchNormalization';\nserialization.registerClass(BatchNormalization);\nexport class LayerNormalization extends Layer {\n    constructor(args) {\n        if (args == null) {\n            args = {};\n        }\n        super(args);\n        this.axis = args.axis == null ? -1 : args.axis;\n        if (typeof this.axis === 'number') {\n            if (!Number.isInteger(this.axis)) {\n                throw new Error(`Expected axis to be an integer, but received ${this.axis}`);\n            }\n        }\n        else if (Array.isArray(this.axis)) {\n            for (const axis of this.axis) {\n                if (!Number.isInteger(axis)) {\n                    throw new Error(`Expected axis to be an array of integers, ` +\n                        `but received ${JSON.stringify(this.axis)}`);\n                }\n            }\n        }\n        else {\n            throw new Error(`Expected axis to be an integer or an array of integers, ` +\n                `but received ${JSON.stringify(this.axis)}`);\n        }\n        this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n        this.center = args.center == null ? true : args.center;\n        this.scale = args.scale == null ? true : args.scale;\n        this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n        this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n        this.betaRegularizer = getRegularizer(args.betaRegularizer);\n        this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n        this.supportsMasking = true;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const nDims = inputShape.length;\n        // Convert axis to array and resolve negatives.\n        if (typeof this.axis === 'number') {\n            this.axis = [this.axis];\n        }\n        for (let i = 0; i < this.axis.length; ++i) {\n            if (this.axis[i] < 0) {\n                this.axis[i] += nDims;\n            }\n        }\n        // Further validate axes.\n        for (const axis of this.axis) {\n            if (axis < 0 || axis >= nDims) {\n                throw new Error(`Invalid axis: ${axis}`);\n            }\n        }\n        if (this.axis.length !== generic_utils.unique(this.axis).length) {\n            throw new Error(`Found duplicate axes in: ${this.axis}`);\n        }\n        const paramShape = this.axis.map(axis => inputShape[axis]);\n        const trainable = true;\n        if (this.scale) {\n            this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n        }\n        else {\n            this.gamma = null;\n        }\n        if (this.center) {\n            this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n        }\n        else {\n            this.beta = null;\n        }\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        const input = getExactlyOneTensor(inputs);\n        const inputShape = input.shape;\n        const nDims = inputShape.length;\n        return tidy(() => {\n            const keepDims = true;\n            let { mean, variance } = moments(input, this.axis, keepDims);\n            const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n            for (const dim of this.axis) {\n                broadcastShape[dim] = inputShape[dim];\n            }\n            const broadcast = (v) => {\n                if (v != null && v.shape.length !== nDims &&\n                    this.axis !== [nDims - 1]) {\n                    return v.reshape(broadcastShape);\n                }\n                else {\n                    return v;\n                }\n            };\n            let scale = broadcast(this.gamma.read());\n            let offset = broadcast(this.beta.read());\n            // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n            // is a workaround for the limitation of core's batchNormalization?d don't\n            // support broadcasting in their gradients. In addition, the tiling is\n            // necessary to ensure correctness on the browser CPU backend regardless\n            // of forward or backward computation. Remove this workaround once the\n            // limitation is addressed. See .\n            const momentsTiling = [];\n            const scaleOffsetTiling = [];\n            for (let i = 0; i < nDims; ++i) {\n                if (this.axis.indexOf(i) !== -1) {\n                    momentsTiling.push(inputShape[i]);\n                    scaleOffsetTiling.push(1);\n                }\n                else {\n                    momentsTiling.push(1);\n                    scaleOffsetTiling.push(inputShape[i]);\n                }\n            }\n            mean = mean.tile(momentsTiling);\n            variance = variance.tile(momentsTiling);\n            scale = scale.tile(scaleOffsetTiling);\n            offset = offset.tile(scaleOffsetTiling);\n            return batchNormalization(input, mean, variance, offset, scale, this.epsilon);\n        });\n    }\n    getConfig() {\n        const config = {\n            axis: this.axis,\n            epsilon: this.epsilon,\n            center: this.center,\n            scale: this.scale,\n            betaInitializer: serializeInitializer(this.betaInitializer),\n            gammaInitializer: serializeInitializer(this.gammaInitializer),\n            betaRegularizer: serializeRegularizer(this.betaRegularizer),\n            gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nLayerNormalization.className = 'LayerNormalization';\nserialization.registerClass(LayerNormalization);\n//# sourceMappingURL=normalization.js.map"]},"metadata":{},"sourceType":"module"}
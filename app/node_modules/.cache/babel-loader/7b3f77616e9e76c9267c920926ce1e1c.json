{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport class Wrapper extends Layer {\n  constructor(args) {\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    super(args);\n    this.layer = args.layer;\n  }\n  build(inputShape) {\n    this.built = true;\n  }\n  // TODO(cais): Implement activityRegularizer getter.\n  get trainable() {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      return this.layer.trainable;\n    } else {\n      return false;\n    }\n  }\n  set trainable(value) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      this.layer.trainable = value;\n    }\n  }\n  get trainableWeights() {\n    return this.layer.trainableWeights;\n  }\n  // TODO(cais): Implement setter for trainableWeights.\n  get nonTrainableWeights() {\n    return this.layer.nonTrainableWeights;\n  }\n  // TODO(cais): Implement setter for nonTrainableWeights.\n  get updates() {\n    // tslint:disable-next-line:no-any\n    return this.layer._updates;\n  }\n  // TODO(cais): Implement getUpdatesFor().\n  get losses() {\n    return this.layer.losses;\n  }\n  // TODO(cais): Implement getLossesFor().\n  getWeights() {\n    return this.layer.getWeights();\n  }\n  setWeights(weights) {\n    this.layer.setWeights(weights);\n  }\n  getConfig() {\n    const config = {\n      'layer': {\n        'className': this.layer.getClassName(),\n        'config': this.layer.getConfig()\n      }\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.layer != null) {\n      this.layer.setFastWeightInitDuringBuild(value);\n    }\n  }\n  /** @nocollapse */\n  static fromConfig(cls, config) {\n    let customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    const layerConfig = config['layer'];\n    const layer = deserialize(layerConfig, customObjects);\n    delete config['layer'];\n    const newConfig = {\n      layer\n    };\n    Object.assign(newConfig, config);\n    return new cls(newConfig);\n  }\n}\nexport class TimeDistributed extends Wrapper {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n  }\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    if (inputShape.length < 3) {\n      throw new ValueError(`TimeDistributed layer expects an input shape >= 3D, but received ` + `input shape ${JSON.stringify(inputShape)}`);\n    }\n    this.inputSpec = [{\n      shape: inputShape\n    }];\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    if (!this.layer.built) {\n      this.layer.build(childInputShape);\n      this.layer.built = true;\n    }\n    super.build(inputShape);\n  }\n  computeOutputShape(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    const childOutputShape = this.layer.computeOutputShape(childInputShape);\n    const timesteps = inputShape[1];\n    return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n      inputs = getExactlyOneTensor(inputs);\n      // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n      // values. Hence the inputs can't have an undetermined first (batch)\n      // dimension, which is why we always use the K.rnn approach here.\n      const step = (inputs, states) => {\n        // TODO(cais): Add useLearningPhase.\n        // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n        //   some cases (e.g., `layer` is a `Sequential` instance), which is\n        //   why `getExactlyOneTensor` is used below.\n        const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n        return [output, []];\n      };\n      const rnnOutputs = rnn(step, inputs, [], false /* goBackwards */, null /* mask */, null /* constants */, false /* unroll */, true /* needPerStepOutputs */);\n      const y = rnnOutputs[1];\n      // TODO(cais): Add activity regularization.\n      // TODO(cais): Add useLearningPhase.\n      return y;\n    });\n  }\n}\n/** @nocollapse */\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n  generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport class Bidirectional extends Wrapper {\n  constructor(args) {\n    super(args);\n    // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n    const layerConfig = args.layer.getConfig();\n    const forwDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    this.forwardLayer = deserialize(forwDict);\n    layerConfig['goBackwards'] = layerConfig['goBackwards'] === true ? false : true;\n    const backDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    this.backwardLayer = deserialize(backDict);\n    this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n    this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n    this.mergeMode = args.mergeMode === undefined ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : args.mergeMode;\n    checkBidirectionalMergeMode(this.mergeMode);\n    if (args.weights) {\n      throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n    }\n    this._stateful = args.layer.stateful;\n    this.returnSequences = args.layer.returnSequences;\n    this.returnState = args.layer.returnState;\n    this.supportsMasking = true;\n    this._trainable = true;\n    this.inputSpec = args.layer.inputSpec;\n    this.numConstants = null;\n  }\n  get trainable() {\n    return this._trainable;\n  }\n  set trainable(value) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    this._trainable = value;\n    if (this.forwardLayer != null) {\n      this.forwardLayer.trainable = value;\n    }\n    if (this.backwardLayer != null) {\n      this.backwardLayer.trainable = value;\n    }\n  }\n  getWeights() {\n    return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n  }\n  setWeights(weights) {\n    const numWeights = weights.length;\n    const numeightsOver2 = Math.floor(numWeights / 2);\n    this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n    this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n  }\n  computeOutputShape(inputShape) {\n    let layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n    if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n      layerShapes = [layerShapes];\n    }\n    layerShapes = layerShapes;\n    let outputShape;\n    let outputShapes;\n    let stateShape;\n    if (this.returnState) {\n      stateShape = layerShapes.slice(1);\n      outputShape = layerShapes[0];\n    } else {\n      outputShape = layerShapes[0];\n    }\n    outputShape = outputShape;\n    if (this.mergeMode === 'concat') {\n      outputShape[outputShape.length - 1] *= 2;\n      outputShapes = [outputShape];\n    } else if (this.mergeMode == null) {\n      outputShapes = [outputShape, outputShape.slice()];\n    } else {\n      outputShapes = [outputShape];\n    }\n    if (this.returnState) {\n      if (this.mergeMode == null) {\n        return outputShapes.concat(stateShape).concat(stateShape.slice());\n      }\n      return [outputShape].concat(stateShape).concat(stateShape.slice());\n    }\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n  apply(inputs, kwargs) {\n    let initialState = kwargs == null ? null : kwargs['initialState'];\n    let constants = kwargs == null ? null : kwargs['constants'];\n    if (kwargs == null) {\n      kwargs = {};\n    }\n    const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n    if (Array.isArray(inputs)) {\n      initialState = inputs.slice(1);\n      inputs = inputs[0];\n    }\n    if ((initialState == null || initialState.length === 0) && constants == null) {\n      return super.apply(inputs, kwargs);\n    }\n    const additionalInputs = [];\n    const additionalSpecs = [];\n    if (initialState != null) {\n      const numStates = initialState.length;\n      if (numStates % 2 > 0) {\n        throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' + 'the state should be an Array containing the states of ' + 'the underlying RNNs.');\n      }\n      kwargs['initialState'] = initialState;\n      additionalInputs.push(...initialState);\n      const stateSpecs = initialState.map(state => new InputSpec({\n        shape: state.shape\n      }));\n      this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n      this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n      additionalSpecs.push(...stateSpecs);\n    }\n    if (constants != null) {\n      throw new NotImplementedError('Support for constants in Bidirectional layers is not ' + 'implemented yet.');\n    }\n    const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n    for (const tensor of additionalInputs) {\n      if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n        throw new ValueError('The initial state of a Bidirectional layer cannot be ' + 'specified as a mix of symbolic and non-symbolic tensors');\n      }\n    }\n    if (isSymbolicTensor) {\n      // Compute the full input and specs, including the states.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n      // Perform the call temporarily and replace inputSpec.\n      // Note: with initial states symbolic calls and non-symbolic calls to\n      // this method differ in how the initial states are passed. For\n      // symbolic calls, the initial states are passed in the first arg, as\n      // an Array of SymbolicTensors; for non-symbolic calls, they are\n      // passed in the second arg as a part of the kwargs. Hence the need to\n      // temporarily modify inputSpec here.\n      // TODO(cais): Make refactoring so that this hacky code below is no\n      // longer needed.\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output = super.apply(fullInput, kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const initialState = kwargs['initialState'];\n      let y;\n      let yRev;\n      if (initialState == null) {\n        y = this.forwardLayer.call(inputs, kwargs);\n        yRev = this.backwardLayer.call(inputs, kwargs);\n      } else {\n        const forwardState = initialState.slice(0, initialState.length / 2);\n        const backwardState = initialState.slice(initialState.length / 2);\n        y = this.forwardLayer.call(inputs, Object.assign(kwargs, {\n          initialState: forwardState\n        }));\n        yRev = this.backwardLayer.call(inputs, Object.assign(kwargs, {\n          initialState: backwardState\n        }));\n      }\n      let states;\n      if (this.returnState) {\n        if (Array.isArray(y)) {\n          states = y.slice(1).concat(yRev.slice(1));\n        } else {}\n        y = y[0];\n        yRev = yRev[0];\n      }\n      if (this.returnSequences) {\n        yRev = tfc.reverse(yRev, 1);\n      }\n      let output;\n      if (this.mergeMode === 'concat') {\n        output = K.concatenate([y, yRev]);\n      } else if (this.mergeMode === 'sum') {\n        output = tfc.add(y, yRev);\n      } else if (this.mergeMode === 'ave') {\n        output = tfc.mul(.5, tfc.add(y, yRev));\n      } else if (this.mergeMode === 'mul') {\n        output = tfc.mul(y, yRev);\n      } else if (this.mergeMode == null) {\n        output = [y, yRev];\n      }\n      // TODO(cais): Properly set learning phase.\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return output.concat(states);\n        }\n        return [output].concat(states);\n      }\n      return output;\n    });\n  }\n  resetStates(states) {\n    this.forwardLayer.resetStates();\n    this.backwardLayer.resetStates();\n  }\n  build(inputShape) {\n    nameScope(this.forwardLayer.name, () => {\n      this.forwardLayer.build(inputShape);\n    });\n    nameScope(this.backwardLayer.name, () => {\n      this.backwardLayer.build(inputShape);\n    });\n    this.built = true;\n  }\n  computeMask(inputs, mask) {\n    if (Array.isArray(mask)) {\n      mask = mask[0];\n    }\n    let outputMask;\n    if (this.returnSequences) {\n      if (this.mergeMode == null) {\n        outputMask = [mask, mask];\n      } else {\n        outputMask = mask;\n      }\n    } else {\n      if (this.mergeMode == null) {\n        outputMask = [null, null];\n      } else {\n        outputMask = null;\n      }\n    }\n    if (this.returnState) {\n      const states = this.forwardLayer.states;\n      const stateMask = states.map(state => null);\n      if (Array.isArray(outputMask)) {\n        return outputMask.concat(stateMask).concat(stateMask);\n      } else {\n        return [outputMask].concat(stateMask).concat(stateMask);\n      }\n    } else {\n      return outputMask;\n    }\n  }\n  get trainableWeights() {\n    return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n  }\n  get nonTrainableWeights() {\n    return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n  }\n  // TODO(cais): Implement constraints().\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.forwardLayer != null) {\n      this.forwardLayer.setFastWeightInitDuringBuild(value);\n    }\n    if (this.backwardLayer != null) {\n      this.backwardLayer.setFastWeightInitDuringBuild(value);\n    }\n  }\n  getConfig() {\n    const config = {\n      'mergeMode': this.mergeMode\n    };\n    // TODO(cais): Add logic for `numConstants` once the property is added.\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  /** @nocollapse */\n  static fromConfig(cls, config) {\n    const rnnLayer = deserialize(config['layer']);\n    delete config['layer'];\n    // TODO(cais): Add logic for `numConstants` once the property is added.\n    if (config['numConstants'] != null) {\n      throw new NotImplementedError(`Deserialization of a Bidirectional layer with numConstants ` + `present is not supported yet.`);\n    }\n    // tslint:disable-next-line:no-any\n    const newConfig = config;\n    newConfig['layer'] = rnnLayer;\n    return new cls(newConfig);\n  }\n}\n/** @nocollapse */\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);","map":{"version":3,"sources":["../../src/layers/wrappers.ts"],"names":[],"mappings":"AAAA;;;;;;;;AAQG;AAEH;;AAEG;AAEH,OAAO,KAAK,GAAG,MAAM,uBAAuB;AAC5C,SAAQ,aAAa,EAAU,IAAI,QAAO,uBAAuB;AACjE,OAAO,KAAK,CAAC,MAAM,yBAAyB;AAC5C,SAAQ,SAAS,QAAO,WAAW;AACnC,SAAQ,SAAS,EAAE,KAAK,EAAa,cAAc,QAAO,oBAAoB;AAC9E,SAAQ,mBAAmB,EAAE,UAAU,QAAO,WAAW;AACzD,SAAuC,+BAA+B,QAAO,wBAAwB;AAGrG,OAAO,KAAK,aAAa,MAAM,wBAAwB;AACvD,SAAQ,kBAAkB,EAAE,mBAAmB,QAAO,sBAAsB;AAG5E,SAAQ,GAAG,EAAO,eAAe,QAAO,aAAa;AACrD,SAAQ,WAAW,QAAO,iBAAiB;AAS3C;;;;;;AAMG;AACH,OAAM,MAAgB,OAAQ,SAAQ,KAAK,CAAA;EAGzC,WAAA,CAAY,IAAsB,EAAA;IAChC;IACA;IACA;IACA;IACA;IACA;IACA;IACA,KAAK,CAAC,IAAI,CAAC;IACX,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK;EACzB;EAEA,KAAK,CAAC,UAAyB,EAAA;IAC7B,IAAI,CAAC,KAAK,GAAG,IAAI;EACnB;EAEA;EAEA,IAAI,SAAS,GAAA;IACX;IACA;IACA;IACA,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;MACtB,OAAO,IAAI,CAAC,KAAK,CAAC,SAAS;KAC5B,MAAM;MACL,OAAO,KAAK;IACb;EACH;EAEA,IAAI,SAAS,CAAC,KAAc,EAAA;IAC1B;IACA;IACA;IACA,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;MACtB,IAAI,CAAC,KAAK,CAAC,SAAS,GAAG,KAAK;IAC7B;EACH;EAEA,IAAI,gBAAgB,GAAA;IAClB,OAAO,IAAI,CAAC,KAAK,CAAC,gBAAgB;EACpC;EACA;EAEA,IAAI,mBAAmB,GAAA;IACrB,OAAO,IAAI,CAAC,KAAK,CAAC,mBAAmB;EACvC;EACA;EAEA,IAAI,OAAO,GAAA;IACT;IACA,OAAQ,IAAI,CAAC,KAAa,CAAC,QAAQ;EACrC;EAEA;EAEA,IAAI,MAAM,GAAA;IACR,OAAO,IAAI,CAAC,KAAK,CAAC,MAAM;EAC1B;EAEA;EAEA,UAAU,GAAA;IACR,OAAO,IAAI,CAAC,KAAK,CAAC,UAAU,EAAE;EAChC;EAEA,UAAU,CAAC,OAAiB,EAAA;IAC1B,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC,OAAO,CAAC;EAChC;EAEA,SAAS,GAAA;IACP,MAAM,MAAM,GAA6B;MACvC,OAAO,EAAE;QACP,WAAW,EAAE,IAAI,CAAC,KAAK,CAAC,YAAY,EAAE;QACtC,QAAQ,EAAE,IAAI,CAAC,KAAK,CAAC,SAAS;MAC/B;KACF;IACD,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;EAEA,4BAA4B,CAAC,KAAc,EAAA;IACzC,KAAK,CAAC,4BAA4B,CAAC,KAAK,CAAC;IACzC,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;MACtB,IAAI,CAAC,KAAK,CAAC,4BAA4B,CAAC,KAAK,CAAC;IAC/C;EACH;EAEA;EACA,OAAO,UAAU,CACb,GAA6C,EAC7C,MAAgC,EACc;IAAA,IAA9C,aAAA,uEAAgB,CAAA,CAA8B;IAChD,MAAM,WAAW,GAAG,MAAM,CAAC,OAAO,CAA6B;IAC/D,MAAM,KAAK,GAAG,WAAW,CAAC,WAAW,EAAE,aAAa,CAAU;IAC9D,OAAO,MAAM,CAAC,OAAO,CAAC;IACtB,MAAM,SAAS,GAAG;MAAC;IAAK,CAAC;IACzB,MAAM,CAAC,MAAM,CAAC,SAAS,EAAE,MAAM,CAAC;IAChC,OAAO,IAAI,GAAG,CAAC,SAAS,CAAC;EAC3B;AACD;AAED,OAAM,MAAO,eAAgB,SAAQ,OAAO,CAAA;EAG1C,WAAA,CAAY,IAAsB,EAAA;IAChC,KAAK,CAAC,IAAI,CAAC;IACX,IAAI,CAAC,eAAe,GAAG,IAAI;EAC7B;EAEA,KAAK,CAAC,UAAyB,EAAA;IAC7B,UAAU,GAAG,kBAAkB,CAAC,UAAU,CAAC;IAC3C,IAAI,UAAU,CAAC,MAAM,GAAG,CAAC,EAAE;MACzB,MAAM,IAAI,UAAU,CAChB,mEAAmE,GACnE,eAAe,IAAI,CAAC,SAAS,CAAC,UAAU,CAAC,EAAE,CAAC;IACjD;IACD,IAAI,CAAC,SAAS,GAAG,CAAC;MAAC,KAAK,EAAE;IAAU,CAAC,CAAC;IACtC,MAAM,eAAe,GAAG,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;IACnE,IAAI,CAAC,IAAI,CAAC,KAAK,CAAC,KAAK,EAAE;MACrB,IAAI,CAAC,KAAK,CAAC,KAAK,CAAC,eAAe,CAAC;MACjC,IAAI,CAAC,KAAK,CAAC,KAAK,GAAG,IAAI;IACxB;IACD,KAAK,CAAC,KAAK,CAAC,UAAU,CAAC;EACzB;EAEA,kBAAkB,CAAC,UAAyB,EAAA;IAC1C,UAAU,GAAG,kBAAkB,CAAC,UAAU,CAAC;IAC3C,MAAM,eAAe,GAAG,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;IACnE,MAAM,gBAAgB,GAClB,IAAI,CAAC,KAAK,CAAC,kBAAkB,CAAC,eAAe,CAAU;IAC3D,MAAM,SAAS,GAAG,UAAU,CAAC,CAAC,CAAC;IAC/B,OAAO,CAAC,gBAAgB,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC,CAAC,MAAM,CAAC,gBAAgB,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;EAC3E;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf;MACA,MAAM,GAAG,mBAAmB,CAAC,MAAM,CAAC;MACpC;MACA;MACA;MACA,MAAM,IAAI,GAAoB,CAAC,MAAc,EAAE,MAAgB,KAAI;QACjE;QACA;QACA;QACA;QACA,MAAM,MAAM,GAAG,mBAAmB,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;QACnE,OAAO,CAAC,MAAM,EAAE,EAAE,CAAC;MACrB,CAAC;MACD,MAAM,UAAU,GACZ,GAAG,CAAC,IAAI,EAAE,MAAM,EAAE,EAAE,EAAE,KAAK,CAAC,mBAAmB,IAAI,CAAC,YAChD,IAAI,CAAC,iBAAiB,KAAK,CAAC,cAC5B,IAAI,CAAC,yBAAyB;MACtC,MAAM,CAAC,GAAG,UAAU,CAAC,CAAC,CAAC;MACvB;MACA;MACA,OAAO,CAAC;IACV,CAAC,CAAC;EACJ;;AAxDA;AACO,eAAA,CAAA,SAAS,GAAG,iBAAiB;AA2DtC,aAAa,CAAC,aAAa,CAAC,eAAe,CAAC;AAE5C,OAAM,SAAU,2BAA2B,CAAC,KAAc,EAAA;EACxD,aAAa,CAAC,yBAAyB,CACnC,+BAA+B,EAAE,wBAAwB,EAAE,KAAK,CAAC;AACvE;AAkBA,MAAM,gCAAgC,GAA2B,QAAQ;AAEzE,OAAM,MAAO,aAAc,SAAQ,OAAO,CAAA;EAWxC,WAAA,CAAY,IAA4B,EAAA;IACtC,KAAK,CAAC,IAAI,CAAC;IAEX;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA,MAAM,WAAW,GAAG,IAAI,CAAC,KAAK,CAAC,SAAS,EAAE;IAC1C,MAAM,QAAQ,GAA6B,CAAA,CAAE;IAC7C,QAAQ,CAAC,WAAW,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,YAAY,EAAE;IACjD,QAAQ,CAAC,QAAQ,CAAC,GAAG,WAAW;IAChC,IAAI,CAAC,YAAY,GAAG,WAAW,CAAC,QAAQ,CAAQ;IAChD,WAAW,CAAC,aAAa,CAAC,GACtB,WAAW,CAAC,aAAa,CAAC,KAAK,IAAI,GAAG,KAAK,GAAG,IAAI;IACtD,MAAM,QAAQ,GAA6B,CAAA,CAAE;IAC7C,QAAQ,CAAC,WAAW,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,YAAY,EAAE;IACjD,QAAQ,CAAC,QAAQ,CAAC,GAAG,WAAW;IAChC,IAAI,CAAC,aAAa,GAAG,WAAW,CAAC,QAAQ,CAAQ;IACjD,IAAI,CAAC,YAAY,CAAC,IAAI,GAAG,UAAU,GAAG,IAAI,CAAC,YAAY,CAAC,IAAI;IAC5D,IAAI,CAAC,aAAa,CAAC,IAAI,GAAG,WAAW,GAAG,IAAI,CAAC,aAAa,CAAC,IAAI;IAE/D,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC,SAAS,KAAK,SAAS,GACzC,gCAAgC,GAChC,IAAI,CAAC,SAAS;IAClB,2BAA2B,CAAC,IAAI,CAAC,SAAS,CAAC;IAC3C,IAAI,IAAI,CAAC,OAAO,EAAE;MAChB,MAAM,IAAI,mBAAmB,CACzB,iEAAiE,CAAC;IACvE;IACD,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC,KAAK,CAAC,QAAQ;IACpC,IAAI,CAAC,eAAe,GAAG,IAAI,CAAC,KAAK,CAAC,eAAe;IACjD,IAAI,CAAC,WAAW,GAAG,IAAI,CAAC,KAAK,CAAC,WAAW;IACzC,IAAI,CAAC,eAAe,GAAG,IAAI;IAC3B,IAAI,CAAC,UAAU,GAAG,IAAI;IACtB,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC,KAAK,CAAC,SAAS;IACrC,IAAI,CAAC,YAAY,GAAG,IAAI;EAC1B;EAEA,IAAI,SAAS,GAAA;IACX,OAAO,IAAI,CAAC,UAAU;EACxB;EAEA,IAAI,SAAS,CAAC,KAAc,EAAA;IAC1B;IACA;IACA;IACA,IAAI,CAAC,UAAU,GAAG,KAAK;IACvB,IAAI,IAAI,CAAC,YAAY,IAAI,IAAI,EAAE;MAC7B,IAAI,CAAC,YAAY,CAAC,SAAS,GAAG,KAAK;IACpC;IACD,IAAI,IAAI,CAAC,aAAa,IAAI,IAAI,EAAE;MAC9B,IAAI,CAAC,aAAa,CAAC,SAAS,GAAG,KAAK;IACrC;EACH;EAEA,UAAU,GAAA;IACR,OAAO,IAAI,CAAC,YAAY,CAAC,UAAU,EAAE,CAAC,MAAM,CACxC,IAAI,CAAC,aAAa,CAAC,UAAU,EAAE,CAAC;EACtC;EAEA,UAAU,CAAC,OAAiB,EAAA;IAC1B,MAAM,UAAU,GAAG,OAAO,CAAC,MAAM;IACjC,MAAM,cAAc,GAAG,IAAI,CAAC,KAAK,CAAC,UAAU,GAAG,CAAC,CAAC;IACjD,IAAI,CAAC,YAAY,CAAC,UAAU,CAAC,OAAO,CAAC,KAAK,CAAC,CAAC,EAAE,cAAc,CAAC,CAAC;IAC9D,IAAI,CAAC,aAAa,CAAC,UAAU,CAAC,OAAO,CAAC,KAAK,CAAC,cAAc,CAAC,CAAC;EAC9D;EAEA,kBAAkB,CAAC,UAAyB,EAAA;IAC1C,IAAI,WAAW,GACX,IAAI,CAAC,YAAY,CAAC,kBAAkB,CAAC,UAAU,CAAC;IACpD,IAAI,EAAE,KAAK,CAAC,OAAO,CAAC,WAAW,CAAC,IAAI,KAAK,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE;MAClE,WAAW,GAAG,CAAC,WAAoB,CAAC;IACrC;IACD,WAAW,GAAG,WAAsB;IAEpC,IAAI,WAAkB;IACtB,IAAI,YAAqB;IACzB,IAAI,UAAmB;IACvB,IAAI,IAAI,CAAC,WAAW,EAAE;MACpB,UAAU,GAAG,WAAW,CAAC,KAAK,CAAC,CAAC,CAAC;MACjC,WAAW,GAAG,WAAW,CAAC,CAAC,CAAC;KAC7B,MAAM;MACL,WAAW,GAAG,WAAW,CAAC,CAAC,CAAC;IAC7B;IACD,WAAW,GAAG,WAAW;IACzB,IAAI,IAAI,CAAC,SAAS,KAAK,QAAQ,EAAE;MAC/B,WAAW,CAAC,WAAW,CAAC,MAAM,GAAG,CAAC,CAAC,IAAI,CAAC;MACxC,YAAY,GAAG,CAAC,WAAW,CAAC;KAC7B,MAAM,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;MACjC,YAAY,GAAG,CAAC,WAAW,EAAE,WAAW,CAAC,KAAK,EAAE,CAAC;KAClD,MAAM;MACL,YAAY,GAAG,CAAC,WAAW,CAAC;IAC7B;IAED,IAAI,IAAI,CAAC,WAAW,EAAE;MACpB,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;QAC1B,OAAO,YAAY,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,KAAK,EAAE,CAAC;MAClE;MACD,OAAO,CAAC,WAAW,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,KAAK,EAAE,CAAC;IACnE;IACD,OAAO,aAAa,CAAC,gBAAgB,CAAC,YAAY,CAAC;EACrD;EAEA,KAAK,CACD,MAAuD,EACvD,MAAe,EAAA;IACjB,IAAI,YAAY,GACZ,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,cAAc,CAAC;IAClD,IAAI,SAAS,GACT,MAAM,IAAI,IAAI,GAAG,IAAI,GAAG,MAAM,CAAC,WAAW,CAAC;IAC/C,IAAI,MAAM,IAAI,IAAI,EAAE;MAClB,MAAM,GAAG,CAAA,CAAE;IACZ;IACD,MAAM,YAAY,GACd,eAAe,CAAC,MAAM,EAAE,YAAY,EAAE,SAAS,EAAE,IAAI,CAAC,YAAY,CAAC;IACvE,MAAM,GAAG,YAAY,CAAC,MAAM;IAC5B,YAAY,GAAG,YAAY,CAAC,YAAY;IACxC,SAAS,GAAG,YAAY,CAAC,SAAS;IAElC,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;MACzB,YAAY,GAAI,MAAsC,CAAC,KAAK,CAAC,CAAC,CAAC;MAC/D,MAAM,GAAI,MAAsC,CAAC,CAAC,CAAC;IACpD;IAED,IAAI,CAAC,YAAY,IAAI,IAAI,IAAI,YAAY,CAAC,MAAM,KAAK,CAAC,KAClD,SAAS,IAAI,IAAI,EAAE;MACrB,OAAO,KAAK,CAAC,KAAK,CAAC,MAAM,EAAE,MAAM,CAAC;IACnC;IACD,MAAM,gBAAgB,GAAiC,EAAE;IACzD,MAAM,eAAe,GAAgB,EAAE;IACvC,IAAI,YAAY,IAAI,IAAI,EAAE;MACxB,MAAM,SAAS,GAAG,YAAY,CAAC,MAAM;MACrC,IAAI,SAAS,GAAG,CAAC,GAAG,CAAC,EAAE;QACrB,MAAM,IAAI,UAAU,CAChB,qDAAqD,GACrD,wDAAwD,GACxD,sBAAsB,CAAC;MAC5B;MACD,MAAM,CAAC,cAAc,CAAC,GAAG,YAAY;MACrC,gBAAgB,CAAC,IAAI,CAAC,GAAG,YAAY,CAAC;MACtC,MAAM,UAAU,GAAI,YAA6C,CACzC,GAAG,CAAC,KAAK,IAAI,IAAI,SAAS,CAAC;QAAC,KAAK,EAAE,KAAK,CAAC;MAAK,CAAC,CAAC,CAAC;MACzE,IAAI,CAAC,YAAY,CAAC,SAAS,GAAG,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,SAAS,GAAG,CAAC,CAAC;MAChE,IAAI,CAAC,aAAa,CAAC,SAAS,GAAG,UAAU,CAAC,KAAK,CAAC,SAAS,GAAG,CAAC,CAAC;MAC9D,eAAe,CAAC,IAAI,CAAC,GAAG,UAAU,CAAC;IACpC;IACD,IAAI,SAAS,IAAI,IAAI,EAAE;MACrB,MAAM,IAAI,mBAAmB,CACzB,uDAAuD,GACvD,kBAAkB,CAAC;IACxB;IAED,MAAM,gBAAgB,GAAG,gBAAgB,CAAC,CAAC,CAAC,YAAY,cAAc;IACtE,KAAK,MAAM,MAAM,IAAI,gBAAgB,EAAE;MACrC,IAAI,MAAM,YAAY,cAAc,KAAK,gBAAgB,EAAE;QACzD,MAAM,IAAI,UAAU,CAChB,uDAAuD,GACvD,yDAAyD,CAAC;MAC/D;IACF;IAED,IAAI,gBAAgB,EAAE;MACpB;MACA,MAAM,SAAS,GAAG,CAAC,MAAM,CAAC,CAAC,MAAM,CAAC,gBAAgB,CAAC;MACnD,MAAM,aAAa,GAAG,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,eAAe,CAAC;MAC5D;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA,MAAM,iBAAiB,GAAG,IAAI,CAAC,SAAS;MACxC,IAAI,CAAC,SAAS,GAAG,aAAa;MAC9B,MAAM,MAAM,GACR,KAAK,CAAC,KAAK,CAAC,SAAwC,EAAE,MAAM,CAAC;MACjE,IAAI,CAAC,SAAS,GAAG,iBAAiB;MAClC,OAAO,MAAM;KACd,MAAM;MACL,OAAO,KAAK,CAAC,KAAK,CAAC,MAAM,EAAE,MAAM,CAAC;IACnC;EACH;EAEA,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,MAAM,YAAY,GAAG,MAAM,CAAC,cAAc,CAAC;MAE3C,IAAI,CAAkB;MACtB,IAAI,IAAqB;MACzB,IAAI,YAAY,IAAI,IAAI,EAAE;QACxB,CAAC,GAAG,IAAI,CAAC,YAAY,CAAC,IAAI,CAAC,MAAM,EAAE,MAAM,CAAC;QAC1C,IAAI,GAAG,IAAI,CAAC,aAAa,CAAC,IAAI,CAAC,MAAM,EAAE,MAAM,CAAC;OAC/C,MAAM;QACL,MAAM,YAAY,GAAG,YAAY,CAAC,KAAK,CAAC,CAAC,EAAE,YAAY,CAAC,MAAM,GAAG,CAAC,CAAC;QACnE,MAAM,aAAa,GAAG,YAAY,CAAC,KAAK,CAAC,YAAY,CAAC,MAAM,GAAG,CAAC,CAAC;QACjE,CAAC,GAAG,IAAI,CAAC,YAAY,CAAC,IAAI,CACtB,MAAM,EAAE,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE;UAAC,YAAY,EAAE;QAAY,CAAC,CAAC,CAAC;QAChE,IAAI,GAAG,IAAI,CAAC,aAAa,CAAC,IAAI,CAC1B,MAAM,EAAE,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE;UAAC,YAAY,EAAE;QAAa,CAAC,CAAC,CAAC;MAClE;MAED,IAAI,MAAgB;MACpB,IAAI,IAAI,CAAC,WAAW,EAAE;QACpB,IAAI,KAAK,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE;UACpB,MAAM,GAAG,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,MAAM,CAAE,IAAiB,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;SACxD,MAAM,CACN;QACD,CAAC,GAAI,CAAc,CAAC,CAAC,CAAC;QACtB,IAAI,GAAI,IAAiB,CAAC,CAAC,CAAC;MAC7B;MAED,IAAI,IAAI,CAAC,eAAe,EAAE;QACxB,IAAI,GAAG,GAAG,CAAC,OAAO,CAAC,IAAc,EAAE,CAAC,CAAC;MACtC;MAED,IAAI,MAAuB;MAC3B,IAAI,IAAI,CAAC,SAAS,KAAK,QAAQ,EAAE;QAC/B,MAAM,GAAG,CAAC,CAAC,WAAW,CAAC,CAAC,CAAW,EAAE,IAAc,CAAC,CAAC;OACtD,MAAM,IAAI,IAAI,CAAC,SAAS,KAAK,KAAK,EAAE;QACnC,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,CAAW,EAAE,IAAc,CAAC;OAC9C,MAAM,IAAI,IAAI,CAAC,SAAS,KAAK,KAAK,EAAE;QACnC,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,EAAE,EAAE,GAAG,CAAC,GAAG,CAAC,CAAW,EAAE,IAAc,CAAC,CAAC;OAC3D,MAAM,IAAI,IAAI,CAAC,SAAS,KAAK,KAAK,EAAE;QACnC,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,CAAW,EAAE,IAAc,CAAC;OAC9C,MAAM,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;QACjC,MAAM,GAAG,CAAC,CAAW,EAAE,IAAc,CAAC;MACvC;MAED;MACA,IAAI,IAAI,CAAC,WAAW,EAAE;QACpB,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;UAC1B,OAAQ,MAAmB,CAAC,MAAM,CAAC,MAAM,CAAC;QAC3C;QACD,OAAO,CAAC,MAAgB,CAAC,CAAC,MAAM,CAAC,MAAM,CAAC;MACzC;MACD,OAAO,MAAM;IACf,CAAC,CAAC;EACJ;EAEA,WAAW,CAAC,MAAwB,EAAA;IAClC,IAAI,CAAC,YAAY,CAAC,WAAW,EAAE;IAC/B,IAAI,CAAC,aAAa,CAAC,WAAW,EAAE;EAClC;EAEA,KAAK,CAAC,UAAyB,EAAA;IAC7B,SAAS,CAAC,IAAI,CAAC,YAAY,CAAC,IAAI,EAAE,MAAK;MACrC,IAAI,CAAC,YAAY,CAAC,KAAK,CAAC,UAAU,CAAC;IACrC,CAAC,CAAC;IACF,SAAS,CAAC,IAAI,CAAC,aAAa,CAAC,IAAI,EAAE,MAAK;MACtC,IAAI,CAAC,aAAa,CAAC,KAAK,CAAC,UAAU,CAAC;IACtC,CAAC,CAAC;IACF,IAAI,CAAC,KAAK,GAAG,IAAI;EACnB;EAEA,WAAW,CAAC,MAAuB,EAAE,IAAsB,EAAA;IAEzD,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE;MACvB,IAAI,GAAG,IAAI,CAAC,CAAC,CAAC;IACf;IACD,IAAI,UAA2B;IAC/B,IAAI,IAAI,CAAC,eAAe,EAAE;MACxB,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;QAC1B,UAAU,GAAG,CAAC,IAAI,EAAE,IAAI,CAAC;OAC1B,MAAM;QACL,UAAU,GAAG,IAAI;MAClB;KACF,MAAM;MACL,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;QAC1B,UAAU,GAAG,CAAC,IAAI,EAAE,IAAI,CAAC;OAC1B,MAAM;QACL,UAAU,GAAG,IAAI;MAClB;IACF;IACD,IAAI,IAAI,CAAC,WAAW,EAAE;MACpB,MAAM,MAAM,GAAG,IAAI,CAAC,YAAY,CAAC,MAAM;MACvC,MAAM,SAAS,GAAa,MAAM,CAAC,GAAG,CAAC,KAAK,IAAI,IAAI,CAAC;MACrD,IAAI,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,EAAE;QAC7B,OAAO,UAAU,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC,MAAM,CAAC,SAAS,CAAC;OACtD,MAAM;QACL,OAAO,CAAC,UAAU,CAAC,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC,MAAM,CAAC,SAAS,CAAC;MACxD;KACF,MAAM;MACL,OAAO,UAAU;IAClB;EACH;EAEA,IAAI,gBAAgB,GAAA;IAClB,OAAO,IAAI,CAAC,YAAY,CAAC,gBAAgB,CAAC,MAAM,CAC5C,IAAI,CAAC,aAAa,CAAC,gBAAgB,CAAC;EAC1C;EAEA,IAAI,mBAAmB,GAAA;IACrB,OAAO,IAAI,CAAC,YAAY,CAAC,mBAAmB,CAAC,MAAM,CAC/C,IAAI,CAAC,aAAa,CAAC,mBAAmB,CAAC;EAC7C;EAEA;EAEA,4BAA4B,CAAC,KAAc,EAAA;IACzC,KAAK,CAAC,4BAA4B,CAAC,KAAK,CAAC;IACzC,IAAI,IAAI,CAAC,YAAY,IAAI,IAAI,EAAE;MAC7B,IAAI,CAAC,YAAY,CAAC,4BAA4B,CAAC,KAAK,CAAC;IACtD;IACD,IAAI,IAAI,CAAC,aAAa,IAAI,IAAI,EAAE;MAC9B,IAAI,CAAC,aAAa,CAAC,4BAA4B,CAAC,KAAK,CAAC;IACvD;EACH;EAEA,SAAS,GAAA;IACP,MAAM,MAAM,GAA6B;MACvC,WAAW,EAAE,IAAI,CAAC;KACnB;IACD;IACA,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE;IACpC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC;IACjC,OAAO,MAAM;EACf;EAEA;EACA,OAAO,UAAU,CACb,GAA6C,EAC7C,MAAgC,EAAA;IAClC,MAAM,QAAQ,GACV,WAAW,CAAC,MAAM,CAAC,OAAO,CAA6B,CAAQ;IACnE,OAAO,MAAM,CAAC,OAAO,CAAC;IACtB;IACA,IAAI,MAAM,CAAC,cAAc,CAAC,IAAI,IAAI,EAAE;MAClC,MAAM,IAAI,mBAAmB,CACzB,6DAA6D,GAC7D,+BAA+B,CAAC;IACrC;IACD;IACA,MAAM,SAAS,GAAyB,MAAM;IAC9C,SAAS,CAAC,OAAO,CAAC,GAAG,QAAQ;IAC7B,OAAO,IAAI,GAAG,CAAC,SAAS,CAAC;EAC3B;;AA/VA;AACO,aAAA,CAAA,SAAS,GAAG,eAAe;AAgWpC,aAAa,CAAC,aAAa,CAAC,aAAa,CAAC","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport class Wrapper extends Layer {\n    constructor(args) {\n        // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n        //   `super()`. But we can't do that here due to TypeScript's restriction.\n        //   See: https://github.com/Microsoft/TypeScript/issues/8277\n        //   As a result, we have to add checks in `get trainable()` and\n        //   `set trainable()` below in order to prevent using `this.layer` when\n        //   its value is `undefined`. The super constructor does use the getter\n        //   and the setter of `this.layer`.\n        super(args);\n        this.layer = args.layer;\n    }\n    build(inputShape) {\n        this.built = true;\n    }\n    // TODO(cais): Implement activityRegularizer getter.\n    get trainable() {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        if (this.layer != null) {\n            return this.layer.trainable;\n        }\n        else {\n            return false;\n        }\n    }\n    set trainable(value) {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        if (this.layer != null) {\n            this.layer.trainable = value;\n        }\n    }\n    get trainableWeights() {\n        return this.layer.trainableWeights;\n    }\n    // TODO(cais): Implement setter for trainableWeights.\n    get nonTrainableWeights() {\n        return this.layer.nonTrainableWeights;\n    }\n    // TODO(cais): Implement setter for nonTrainableWeights.\n    get updates() {\n        // tslint:disable-next-line:no-any\n        return this.layer._updates;\n    }\n    // TODO(cais): Implement getUpdatesFor().\n    get losses() {\n        return this.layer.losses;\n    }\n    // TODO(cais): Implement getLossesFor().\n    getWeights() {\n        return this.layer.getWeights();\n    }\n    setWeights(weights) {\n        this.layer.setWeights(weights);\n    }\n    getConfig() {\n        const config = {\n            'layer': {\n                'className': this.layer.getClassName(),\n                'config': this.layer.getConfig(),\n            }\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.layer != null) {\n            this.layer.setFastWeightInitDuringBuild(value);\n        }\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config, customObjects = {}) {\n        const layerConfig = config['layer'];\n        const layer = deserialize(layerConfig, customObjects);\n        delete config['layer'];\n        const newConfig = { layer };\n        Object.assign(newConfig, config);\n        return new cls(newConfig);\n    }\n}\nexport class TimeDistributed extends Wrapper {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        if (inputShape.length < 3) {\n            throw new ValueError(`TimeDistributed layer expects an input shape >= 3D, but received ` +\n                `input shape ${JSON.stringify(inputShape)}`);\n        }\n        this.inputSpec = [{ shape: inputShape }];\n        const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        if (!this.layer.built) {\n            this.layer.build(childInputShape);\n            this.layer.built = true;\n        }\n        super.build(inputShape);\n    }\n    computeOutputShape(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        const childOutputShape = this.layer.computeOutputShape(childInputShape);\n        const timesteps = inputShape[1];\n        return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n            inputs = getExactlyOneTensor(inputs);\n            // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n            // values. Hence the inputs can't have an undetermined first (batch)\n            // dimension, which is why we always use the K.rnn approach here.\n            const step = (inputs, states) => {\n                // TODO(cais): Add useLearningPhase.\n                // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n                //   some cases (e.g., `layer` is a `Sequential` instance), which is\n                //   why `getExactlyOneTensor` is used below.\n                const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n                return [output, []];\n            };\n            const rnnOutputs = rnn(step, inputs, [], false /* goBackwards */, null /* mask */, null /* constants */, false /* unroll */, true /* needPerStepOutputs */);\n            const y = rnnOutputs[1];\n            // TODO(cais): Add activity regularization.\n            // TODO(cais): Add useLearningPhase.\n            return y;\n        });\n    }\n}\n/** @nocollapse */\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n    generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport class Bidirectional extends Wrapper {\n    constructor(args) {\n        super(args);\n        // Note: When creating `this.forwardLayer`, the original Layer object\n        //   (`config.layer`) ought to be cloned. This is why we call\n        //   `getConfig()` followed by `deserialize()`. Without this cloning,\n        //   the layer names saved during serialization will incorrectly contain\n        //   the 'forward_' prefix. In Python Keras, this is done using\n        //   `copy.copy` (shallow copy), which does not have a simple equivalent\n        //   in JavaScript. JavaScript's `Object.assign()` does not copy\n        //   methods.\n        const layerConfig = args.layer.getConfig();\n        const forwDict = {};\n        forwDict['className'] = args.layer.getClassName();\n        forwDict['config'] = layerConfig;\n        this.forwardLayer = deserialize(forwDict);\n        layerConfig['goBackwards'] =\n            layerConfig['goBackwards'] === true ? false : true;\n        const backDict = {};\n        backDict['className'] = args.layer.getClassName();\n        backDict['config'] = layerConfig;\n        this.backwardLayer = deserialize(backDict);\n        this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n        this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n        this.mergeMode = args.mergeMode === undefined ?\n            DEFAULT_BIDIRECTIONAL_MERGE_MODE :\n            args.mergeMode;\n        checkBidirectionalMergeMode(this.mergeMode);\n        if (args.weights) {\n            throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n        }\n        this._stateful = args.layer.stateful;\n        this.returnSequences = args.layer.returnSequences;\n        this.returnState = args.layer.returnState;\n        this.supportsMasking = true;\n        this._trainable = true;\n        this.inputSpec = args.layer.inputSpec;\n        this.numConstants = null;\n    }\n    get trainable() {\n        return this._trainable;\n    }\n    set trainable(value) {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        this._trainable = value;\n        if (this.forwardLayer != null) {\n            this.forwardLayer.trainable = value;\n        }\n        if (this.backwardLayer != null) {\n            this.backwardLayer.trainable = value;\n        }\n    }\n    getWeights() {\n        return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    }\n    setWeights(weights) {\n        const numWeights = weights.length;\n        const numeightsOver2 = Math.floor(numWeights / 2);\n        this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n        this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    }\n    computeOutputShape(inputShape) {\n        let layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n        if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n            layerShapes = [layerShapes];\n        }\n        layerShapes = layerShapes;\n        let outputShape;\n        let outputShapes;\n        let stateShape;\n        if (this.returnState) {\n            stateShape = layerShapes.slice(1);\n            outputShape = layerShapes[0];\n        }\n        else {\n            outputShape = layerShapes[0];\n        }\n        outputShape = outputShape;\n        if (this.mergeMode === 'concat') {\n            outputShape[outputShape.length - 1] *= 2;\n            outputShapes = [outputShape];\n        }\n        else if (this.mergeMode == null) {\n            outputShapes = [outputShape, outputShape.slice()];\n        }\n        else {\n            outputShapes = [outputShape];\n        }\n        if (this.returnState) {\n            if (this.mergeMode == null) {\n                return outputShapes.concat(stateShape).concat(stateShape.slice());\n            }\n            return [outputShape].concat(stateShape).concat(stateShape.slice());\n        }\n        return generic_utils.singletonOrArray(outputShapes);\n    }\n    apply(inputs, kwargs) {\n        let initialState = kwargs == null ? null : kwargs['initialState'];\n        let constants = kwargs == null ? null : kwargs['constants'];\n        if (kwargs == null) {\n            kwargs = {};\n        }\n        const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n        inputs = standardized.inputs;\n        initialState = standardized.initialState;\n        constants = standardized.constants;\n        if (Array.isArray(inputs)) {\n            initialState = inputs.slice(1);\n            inputs = inputs[0];\n        }\n        if ((initialState == null || initialState.length === 0) &&\n            constants == null) {\n            return super.apply(inputs, kwargs);\n        }\n        const additionalInputs = [];\n        const additionalSpecs = [];\n        if (initialState != null) {\n            const numStates = initialState.length;\n            if (numStates % 2 > 0) {\n                throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' +\n                    'the state should be an Array containing the states of ' +\n                    'the underlying RNNs.');\n            }\n            kwargs['initialState'] = initialState;\n            additionalInputs.push(...initialState);\n            const stateSpecs = initialState\n                .map(state => new InputSpec({ shape: state.shape }));\n            this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n            this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n            additionalSpecs.push(...stateSpecs);\n        }\n        if (constants != null) {\n            throw new NotImplementedError('Support for constants in Bidirectional layers is not ' +\n                'implemented yet.');\n        }\n        const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n        for (const tensor of additionalInputs) {\n            if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n                throw new ValueError('The initial state of a Bidirectional layer cannot be ' +\n                    'specified as a mix of symbolic and non-symbolic tensors');\n            }\n        }\n        if (isSymbolicTensor) {\n            // Compute the full input and specs, including the states.\n            const fullInput = [inputs].concat(additionalInputs);\n            const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n            // Perform the call temporarily and replace inputSpec.\n            // Note: with initial states symbolic calls and non-symbolic calls to\n            // this method differ in how the initial states are passed. For\n            // symbolic calls, the initial states are passed in the first arg, as\n            // an Array of SymbolicTensors; for non-symbolic calls, they are\n            // passed in the second arg as a part of the kwargs. Hence the need to\n            // temporarily modify inputSpec here.\n            // TODO(cais): Make refactoring so that this hacky code below is no\n            // longer needed.\n            const originalInputSpec = this.inputSpec;\n            this.inputSpec = fullInputSpec;\n            const output = super.apply(fullInput, kwargs);\n            this.inputSpec = originalInputSpec;\n            return output;\n        }\n        else {\n            return super.apply(inputs, kwargs);\n        }\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const initialState = kwargs['initialState'];\n            let y;\n            let yRev;\n            if (initialState == null) {\n                y = this.forwardLayer.call(inputs, kwargs);\n                yRev = this.backwardLayer.call(inputs, kwargs);\n            }\n            else {\n                const forwardState = initialState.slice(0, initialState.length / 2);\n                const backwardState = initialState.slice(initialState.length / 2);\n                y = this.forwardLayer.call(inputs, Object.assign(kwargs, { initialState: forwardState }));\n                yRev = this.backwardLayer.call(inputs, Object.assign(kwargs, { initialState: backwardState }));\n            }\n            let states;\n            if (this.returnState) {\n                if (Array.isArray(y)) {\n                    states = y.slice(1).concat(yRev.slice(1));\n                }\n                else {\n                }\n                y = y[0];\n                yRev = yRev[0];\n            }\n            if (this.returnSequences) {\n                yRev = tfc.reverse(yRev, 1);\n            }\n            let output;\n            if (this.mergeMode === 'concat') {\n                output = K.concatenate([y, yRev]);\n            }\n            else if (this.mergeMode === 'sum') {\n                output = tfc.add(y, yRev);\n            }\n            else if (this.mergeMode === 'ave') {\n                output = tfc.mul(.5, tfc.add(y, yRev));\n            }\n            else if (this.mergeMode === 'mul') {\n                output = tfc.mul(y, yRev);\n            }\n            else if (this.mergeMode == null) {\n                output = [y, yRev];\n            }\n            // TODO(cais): Properly set learning phase.\n            if (this.returnState) {\n                if (this.mergeMode == null) {\n                    return output.concat(states);\n                }\n                return [output].concat(states);\n            }\n            return output;\n        });\n    }\n    resetStates(states) {\n        this.forwardLayer.resetStates();\n        this.backwardLayer.resetStates();\n    }\n    build(inputShape) {\n        nameScope(this.forwardLayer.name, () => {\n            this.forwardLayer.build(inputShape);\n        });\n        nameScope(this.backwardLayer.name, () => {\n            this.backwardLayer.build(inputShape);\n        });\n        this.built = true;\n    }\n    computeMask(inputs, mask) {\n        if (Array.isArray(mask)) {\n            mask = mask[0];\n        }\n        let outputMask;\n        if (this.returnSequences) {\n            if (this.mergeMode == null) {\n                outputMask = [mask, mask];\n            }\n            else {\n                outputMask = mask;\n            }\n        }\n        else {\n            if (this.mergeMode == null) {\n                outputMask = [null, null];\n            }\n            else {\n                outputMask = null;\n            }\n        }\n        if (this.returnState) {\n            const states = this.forwardLayer.states;\n            const stateMask = states.map(state => null);\n            if (Array.isArray(outputMask)) {\n                return outputMask.concat(stateMask).concat(stateMask);\n            }\n            else {\n                return [outputMask].concat(stateMask).concat(stateMask);\n            }\n        }\n        else {\n            return outputMask;\n        }\n    }\n    get trainableWeights() {\n        return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n    }\n    get nonTrainableWeights() {\n        return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n    }\n    // TODO(cais): Implement constraints().\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.forwardLayer != null) {\n            this.forwardLayer.setFastWeightInitDuringBuild(value);\n        }\n        if (this.backwardLayer != null) {\n            this.backwardLayer.setFastWeightInitDuringBuild(value);\n        }\n    }\n    getConfig() {\n        const config = {\n            'mergeMode': this.mergeMode,\n        };\n        // TODO(cais): Add logic for `numConstants` once the property is added.\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        const rnnLayer = deserialize(config['layer']);\n        delete config['layer'];\n        // TODO(cais): Add logic for `numConstants` once the property is added.\n        if (config['numConstants'] != null) {\n            throw new NotImplementedError(`Deserialization of a Bidirectional layer with numConstants ` +\n                `present is not supported yet.`);\n        }\n        // tslint:disable-next-line:no-any\n        const newConfig = config;\n        newConfig['layer'] = rnnLayer;\n        return new cls(newConfig);\n    }\n}\n/** @nocollapse */\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);\n//# sourceMappingURL=wrappers.js.map"]},"metadata":{},"sourceType":"module"}